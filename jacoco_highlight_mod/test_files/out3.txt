diff --git a/h2o-algos/src/main/java/hex/DataInfo.java b/h2o-algos/src/main/java/hex/DataInfo.java
index c88785b..8f9456a 100644
--- a/h2o-algos/src/main/java/hex/DataInfo.java
+++ b/h2o-algos/src/main/java/hex/DataInfo.java
@@ -86,7 +86,7 @@ public class DataInfo extends Keyed<DataInfo> {
   public int _nums;
   public int _cats;
   public int [] _catOffsets;
-  public int [] _catMissing;  // bucket for missing categoricals
+  public boolean [] _catMissing;  // bucket for missing categoricals
   public int [] _catModes;    // majority class of each categorical col
   public int [] _permutation; // permutation matrix mapping input col indices to adaptedFrame
   public double [] _normMul;
@@ -179,15 +179,16 @@ public class DataInfo extends Keyed<DataInfo> {
     // Compute the cardinality of each cat
     _catModes = new int[_cats];
     _catOffsets = MemoryManager.malloc4(ncats+1);
-    _catMissing = new int[ncats];
+    _catMissing = new boolean[ncats];
     int len = _catOffsets[0] = 0;
+
     for(int i = 0; i < ncats; ++i) {
       _catModes[i] = imputeCat(train.vec(cats[i]));
       _permutation[i] = cats[i];
       names[i]  =   train._names[cats[i]];
       Vec v = (tvecs2[i] = tvecs[cats[i]]);
-      _catMissing[i] = missingBucket ? 1 : 0; //needed for test time
-      _catOffsets[i+1] = (len += v.domain().length - (useAllFactorLevels?0:1) + (missingBucket ? 1 : 0)); //missing values turn into a new factor level
+      _catMissing[i] = missingBucket; //needed for test time
+      _catOffsets[i+1] = (len += v.domain().length - (useAllFactorLevels?0:1) + (missingBucket? 1 : 0)); //missing values turn into a new factor level
     }
     _numMeans = new double[_nums];
     for(int i = 0; i < _nums; ++i){
@@ -217,14 +218,14 @@ public class DataInfo extends Keyed<DataInfo> {
   }
 
   public DataInfo validDinfo(Frame valid) {
-    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,false,_weights,_offset,_fold);
+    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,!(_skipMissing || _imputeMissing),_weights,_offset,_fold);
     res._adaptedFrame = new Frame(_adaptedFrame.names(),valid.vecs(_adaptedFrame.names()));
     res._valid = true;
     return res;
   }
 
   public DataInfo scoringInfo(){
-    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,false,_weights,_offset,_fold);
+    DataInfo res = new DataInfo(_adaptedFrame,null,1,_useAllFactorLevels,TransformType.NONE,TransformType.NONE,_skipMissing,_imputeMissing,!_skipMissing,_weights,_offset,_fold);
     res._adaptedFrame = null;
     res._weights = false;
     res._offset = false;
@@ -270,7 +271,8 @@ public class DataInfo extends Keyed<DataInfo> {
     _imputeMissing = imputeMissing;
     _adaptedFrame = fr;
     _catOffsets = MemoryManager.malloc4(catLevels.length + 1);
-    _catMissing = new int[catLevels.length];
+    _catMissing = new boolean[catLevels.length];
+    Arrays.fill(_catMissing,!(imputeMissing || skipMissing));
     int s = 0;
     for(int i = 0; i < catLevels.length; ++i){
       _catOffsets[i] = s;
@@ -374,10 +376,23 @@ public class DataInfo extends Keyed<DataInfo> {
     }
     if(_predictor_transform.isMeanAdjusted()) {
       if(mean.length != _normSub.length)
-        throw new IllegalArgumentException("Length of sigmas does not match number of scaled columns.");
+        throw new IllegalArgumentException("Length of means does not match number of scaled columns.");
       System.arraycopy(mean,0,_normSub,0,mean.length);
     }
   }
+  public void updateWeightedSigmaAndMeanForResponse(double [] sigmas, double [] mean) {
+    if(_response_transform.isSigmaScaled()) {
+      if(sigmas.length != _normRespMul.length)
+        throw new IllegalArgumentException("Length of sigmas does not match number of scaled columns.");
+      for(int i = 0; i < sigmas.length; ++i)
+        _normRespMul[i] = sigmas[i] != 0?1.0/sigmas[i]:1;
+    }
+    if(_response_transform.isMeanAdjusted()) {
+      if(mean.length != _normRespSub.length)
+        throw new IllegalArgumentException("Length of means does not match number of scaled columns.");
+      System.arraycopy(mean,0,_normRespSub,0,mean.length);
+    }
+  }
 
   private void setTransform(TransformType t, double [] normMul, double [] normSub, int vecStart, int n) {
     for (int i = 0; i < n; ++i) {
@@ -445,7 +460,7 @@ public class DataInfo extends Keyed<DataInfo> {
           continue;
         res[k++] = _adaptedFrame._names[i] + "." + vecs[i].domain()[j];
       }
-      if (_catMissing[i] > 0) res[k++] = _adaptedFrame._names[i] + ".missing(NA)";
+      if (_catMissing[i]) res[k++] = _adaptedFrame._names[i] + ".missing(NA)";
     }
     final int nums = n-k;
     System.arraycopy(_adaptedFrame._names, _cats, res, k, nums);
@@ -618,17 +633,15 @@ public class DataInfo extends Keyed<DataInfo> {
     int v = c + _catOffsets[cid];
     if(v >= _catOffsets[cid+1]) { // previously unseen level
       assert _valid:"categorical value out of bounds, got " + v + ", next cat starts at " + _catOffsets[cid+1];
-      return -2;
+      return _catMissing[cid]?_catOffsets[cid+1]-1:-2;// if we have NA bucket, treat previously unseen as NA.
     }
     return v;
   }
 
-  public final Row extractDenseRow(double [] vals, Row row, double w, double o) {
+  public final Row extractDenseRow(double [] vals, Row row) {
     row.bad = false;
     row.rid = 0;
     row.cid = 0;
-    row.offset = o;
-    row.weight = w;
     if(row.weight == 0) return row;
 
     if (_skipMissing)
@@ -644,7 +657,7 @@ public class DataInfo extends Keyed<DataInfo> {
           int c = getCategoricalId(i,_catModes[i]);
           if(c >= 0)
             row.binIds[nbins++] = c;
-        } else   // TODO: What if missingBucket = false?
+        } else if(_catMissing[i])  // TODO: What if missingBucket = false?
           row.binIds[nbins++] = _catOffsets[i + 1] - 1; // missing value turns into extra (last) factor
       } else {
         int c = getCategoricalId(i,(int)vals[i]);
@@ -697,8 +710,9 @@ public class DataInfo extends Keyed<DataInfo> {
             int c = getCategoricalId(i,_catModes[i]);
             if(c >= 0)
               row.binIds[nbins++] = c;
-          } else   // TODO: What if missingBucket = false?
+          } else if(_catMissing[i])  // TODO: What if missingBucket = false?
             row.binIds[nbins++] = _catOffsets[i + 1] - 1; // missing value turns into extra (last) factor
+          // else skip
       } else {
         int c = getCategoricalId(i,(int)chunks[i].at8(rid));
         if(c >= 0)
diff --git a/h2o-algos/src/main/java/hex/FrameTask.java b/h2o-algos/src/main/java/hex/FrameTask.java
index d0c60ed..5921a26 100644
--- a/h2o-algos/src/main/java/hex/FrameTask.java
+++ b/h2o-algos/src/main/java/hex/FrameTask.java
@@ -100,8 +100,10 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
     final long offset = chunks[0].start();
     boolean doWork = chunkInit();
     if (!doWork) return;
-    final boolean obs_weights = _dinfo._weights && !_fr.vecs()[_dinfo.weightChunkId()].isConst();
-    final double global_weight_sum = obs_weights ? _fr.vecs()[_dinfo.weightChunkId()].mean() * _fr.numRows() : 0;
+    final boolean obs_weights = _dinfo._weights
+            && !_fr.vecs()[_dinfo.weightChunkId()].isConst() //if all constant weights (such as 1) -> doesn't count as obs weights
+            && !(_fr.vecs()[_dinfo.weightChunkId()].isBinary()); //special case for cross-val      -> doesn't count as obs weights
+    final double global_weight_sum = obs_weights ? Math.round(_fr.vecs()[_dinfo.weightChunkId()].mean() * _fr.numRows()) : 0;
 
     DataInfo.Row row = null;
     DataInfo.Row[] rows = null;
@@ -148,6 +150,7 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
 
     final int miniBatchSize = getMiniBatchSize();
     long num_processed_rows = 0;
+    long num_skipped_rows = 0;
     int miniBatchCounter = 0;
     for(int rep = 0; rep < repeats; ++rep) {
       for(int row_idx = 0; row_idx < nrows; ++row_idx){
@@ -175,7 +178,10 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
         assert(r >= 0 && r<=nrows);
 
         row = _sparse ? rows[r] : _dinfo.extractDenseRow(chunks, r, row);
-        if(!row.bad) {
+        if(row.bad || row.weight == 0) {
+          num_skipped_rows++;
+          continue;
+        } else {
           assert(row.weight > 0); //check that we never process a row that was held out via row.weight = 0
           long seed = offset + rep * nrows + r;
           miniBatchCounter++;
@@ -194,7 +200,7 @@ public abstract class FrameTask<T extends FrameTask<T>> extends MRTask<T>{
     if (miniBatchCounter>0)
       applyMiniBatchUpdate(miniBatchCounter); //finish up the last piece
 
-    assert(fraction != 1 || num_processed_rows == repeats * nrows);
+    assert(fraction != 1 || num_processed_rows + num_skipped_rows == repeats * nrows);
     chunkDone(num_processed_rows);
   }
 
diff --git a/h2o-algos/src/main/java/hex/coxph/CoxPH.java b/h2o-algos/src/main/java/hex/coxph/CoxPH.java
index 4dd25d9..425af31 100644
--- a/h2o-algos/src/main/java/hex/coxph/CoxPH.java
+++ b/h2o-algos/src/main/java/hex/coxph/CoxPH.java
@@ -20,7 +20,6 @@ public class CoxPH extends ModelBuilder<CoxPHModel,CoxPHModel.CoxPHParameters,Co
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; }
   public CoxPH( CoxPHModel.CoxPHParameters parms ) { super(parms); init(false); }
   @Override protected CoxPHDriver trainModelImpl() { return new CoxPHDriver(); }
-  @Override public long progressUnits() { return _parms.iter_max; }
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
diff --git a/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java b/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java
index d6c2883..96c54a0 100644
--- a/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java
+++ b/h2o-algos/src/main/java/hex/coxph/CoxPHModel.java
@@ -24,6 +24,7 @@ public class CoxPHModel extends Model<CoxPHModel,CoxPHParameters,CoxPHOutput> {
     public String algoName() { return "CoxPH"; }
     public String fullName() { return "Cox Proportional Hazards"; }
     public String javaName() { return CoxPHModel.class.getName(); }
+    @Override public long progressUnits() { return iter_max; }
     // get destination_key  from SupervisedModel.SupervisedParameters from Model.Parameters
     // get training_frame   from SupervisedModel.SupervisedParameters from Model.Parameters
     // get validation_frame from SupervisedModel.SupervisedParameters from Model.Parameters
diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
index 1215534..3daf313 100755
--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearning.java
@@ -2,6 +2,8 @@ package hex.deeplearning;
 
 import hex.*;
 import hex.deeplearning.DeepLearningModel.DeepLearningParameters;
+import hex.deeplearning.DeepLearningModel.DeepLearningParameters.MissingValuesHandling;
+import hex.glm.GLMTask;
 import water.*;
 import water.exceptions.H2OIllegalArgumentException;
 import water.exceptions.H2OModelBuilderIllegalArgumentException;
@@ -47,14 +49,6 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
 
   @Override protected DeepLearningDriver trainModelImpl() { return new DeepLearningDriver(); }
 
-  @Override
-  public long progressUnits() {
-    long work = 1;
-    if (null != _train)
-      work = (long)(_parms._epochs * _train.numRows());
-    return Math.max(1, work);
-  }
-
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
    *  and each subclass will start with "super.init();".  This call is made
@@ -73,12 +67,13 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
    * @param train Training frame
    * @param valid Validation frame
    * @param parms Model parameters
+   * @param nClasses Number of response levels (1: regression, >=2: classification)
    * @return DataInfo
    */
-  static DataInfo makeDataInfo(Frame train, Frame valid, DeepLearningParameters parms) {
+  static DataInfo makeDataInfo(Frame train, Frame valid, DeepLearningParameters parms, int nClasses) {
     double x = 0.782347234;
-    boolean identityLink = new Distribution(parms._distribution, parms._tweedie_power).link(x) == x;
-    return new DataInfo(
+    boolean identityLink = new Distribution(parms).link(x) == x;
+    DataInfo dinfo = new DataInfo(
             train,
             valid,
             parms._autoencoder ? 0 : 1, //nResponses
@@ -91,7 +86,22 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
             parms._weights_column != null, // observation weights
             parms._offset_column != null,
             parms._fold_column != null
-      );
+    );
+    // Checks and adjustments:
+    // 1) observation weights (adjust mean/sigmas for predictors and response)
+    // 2) NAs (check that there's enough rows left)
+    GLMTask.YMUTask ymt = new GLMTask.YMUTask(dinfo, nClasses, true, !parms._autoencoder && nClasses == 1, parms._missing_values_handling == MissingValuesHandling.Skip, !parms._autoencoder).doAll(dinfo._adaptedFrame);
+    if (ymt._wsum == 0 && parms._missing_values_handling == DeepLearningParameters.MissingValuesHandling.Skip)
+      throw new H2OIllegalArgumentException("No rows left in the dataset after filtering out rows with missing values. Ignore columns with many NAs or set missing_values_handling to 'MeanImputation'.");
+    if (parms._weights_column != null && parms._offset_column != null) {
+      Log.warn("Combination of offset and weights can lead to slight differences because Rollupstats aren't weighted - need to re-calculate weighted mean/sigma of the response including offset terms.");
+    }
+    if (parms._weights_column != null && parms._offset_column == null /*FIXME: offset not yet implemented*/) {
+      dinfo.updateWeightedSigmaAndMean(ymt._basicStats.sigma(), ymt._basicStats.mean());
+      if (nClasses == 1)
+        dinfo.updateWeightedSigmaAndMeanForResponse(ymt._basicStatsResponse.sigma(), ymt._basicStatsResponse.mean());
+    }
+    return dinfo;
   }
 
   @Override protected void checkMemoryFootPrint() {
@@ -237,8 +247,7 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
           // This can add or remove dummy columns (can happen if the dataset is sparse and datasets have different non-const columns)
           for (String st : previous.adaptTestForTrain(_train,true,false)) Log.warn(st);
           for (String st : previous.adaptTestForTrain(_valid,true,false)) Log.warn(st);
-
-          dinfo = makeDataInfo(_train, _valid, _parms);
+          dinfo = makeDataInfo(_train, _valid, _parms, nclasses());
           DKV.put(dinfo);
           cp = new DeepLearningModel(dest(), _parms, previous, false, dinfo);
           cp.write_lock(_job);
@@ -343,6 +352,11 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
           model._output._modelClassDist = _weights != null ? cd.doAll(l, w).rel_dist() : cd.doAll(l).rel_dist();
         }
         model.training_rows = train.numRows();
+        if (_weights != null && _weights.min()==0 && _weights.max()==1 && _weights.isInt()) {
+          model.training_rows = Math.round(train.numRows()*_weights.mean());
+          Log.warn("Not counting " + (train.numRows() - model.training_rows) + " rows with weight=0 towards an epoch.");
+        }
+        Log.info("One epoch corresponds to " + model.training_rows + " training data rows.");
         trainScoreFrame = sampleFrame(train, mp._score_training_samples, mp._seed); //training scoring dataset is always sampled uniformly from the training dataset
         if( trainScoreFrame != train ) Scope.track(trainScoreFrame);
 
@@ -363,14 +377,14 @@ public class DeepLearning extends ModelBuilder<DeepLearningModel,DeepLearningMod
         }
 
         // Set train_samples_per_iteration size (cannot be done earlier since this depends on whether stratified sampling is done)
-        model.actual_train_samples_per_iteration = computeTrainSamplesPerIteration(mp, train.numRows(), model);
+        model.actual_train_samples_per_iteration = computeTrainSamplesPerIteration(mp, model.training_rows, model);
         // Determine whether shuffling is enforced
-        if(mp._replicate_training_data && (model.actual_train_samples_per_iteration == train.numRows()*(mp._single_node_mode ?1:H2O.CLOUD.size())) && !mp._shuffle_training_data && H2O.CLOUD.size() > 1 && !mp._reproducible) {
+        if(mp._replicate_training_data && (model.actual_train_samples_per_iteration == model.training_rows*(mp._single_node_mode ?1:H2O.CLOUD.size())) && !mp._shuffle_training_data && H2O.CLOUD.size() > 1 && !mp._reproducible) {
           if (!mp._quiet_mode)
             Log.info("Enabling training data shuffling, because all nodes train on the full dataset (replicated training data).");
           mp._shuffle_training_data = true;
         }
-        if(!mp._shuffle_training_data && model.actual_train_samples_per_iteration == train.numRows() && train.anyVec().nChunks()==1) {
+        if(!mp._shuffle_training_data && model.actual_train_samples_per_iteration == model.training_rows && train.anyVec().nChunks()==1) {
           if (!mp._quiet_mode)
             Log.info("Enabling training data shuffling to avoid training rows in the same order over and over (no Hogwild since there's only 1 chunk).");
           mp._shuffle_training_data = true;
diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java
index 5f1341f..36cc971 100755
--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModel.java
@@ -80,7 +80,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
   public double deviance(double w, double y, double f) {
     // Note: Must use sanitized parameters via get_params() as this._params can still have defaults AUTO, etc.)
     assert(get_params()._distribution != Distribution.Family.AUTO);
-    return new Distribution(get_params()._distribution, get_params()._tweedie_power).deviance(w,y,f);
+    return new Distribution(get_params()).deviance(w,y,f);
   }
 
   // Default publicly visible Schema is V2
@@ -437,7 +437,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
    */
   public DeepLearningModel(final Key destKey, final DeepLearningParameters parms, final DeepLearningModelOutput output, Frame train, Frame valid, int nClasses) {
     super(destKey, parms, output);
-    final DataInfo dinfo = makeDataInfo(train, valid, _parms);
+    final DataInfo dinfo = makeDataInfo(train, valid, _parms, nClasses);
     _output._names  = train._names   ; // Since changed by DataInfo, need to be reflected in the Model output as well
     _output._domains= train.domains();
     _output._names = dinfo._adaptedFrame.names();
@@ -788,7 +788,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
     double loss = 0;
     Neurons[] neurons = DeepLearningTask.makeNeuronsForTraining(model_info());
     //for absolute error, gradient -1/1 matches the derivative of abs(x) without correction term
-    final double prefactor = _parms._distribution == Distribution.Family.laplace ? 1 : 0.5;
+    final double prefactor = _parms._distribution == Distribution.Family.laplace || _parms._distribution == Distribution.Family.quantile ? 1 : 0.5;
     for (DataInfo.Row myRow : myRows) {
       if (myRow == null) continue;
       long seed = -1; //ignored
@@ -826,7 +826,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
 //        pred = (pred / di._normRespMul[0] + di._normRespSub[0]);
 //        actual = (actual / di._normRespMul[0] + di._normRespSub[0]);
 //      }
-        Distribution dist = new Distribution(model_info.get_params()._distribution, model_info.get_params()._tweedie_power);
+        Distribution dist = new Distribution(model_info.get_params());
         pred = dist.linkInv(pred);
         loss += prefactor * dist.deviance(1 /*weight*/, actual, pred);
       }
@@ -879,8 +879,9 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
       else
         preds[0] = out[0];
       // transform prediction to response space
-      preds[0] = new Distribution(model_info.get_params()._distribution, model_info.get_params()._tweedie_power).linkInv(preds[0]);
-      if (Double.isNaN(preds[0])) throw new RuntimeException("Predicted regression target NaN!");
+      preds[0] = new Distribution(model_info.get_params()).linkInv(preds[0]);
+      if (Double.isNaN(preds[0]))
+        throw new RuntimeException("Predicted regression target NaN!");
     }
     return preds;
   }
@@ -1396,7 +1397,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
       else {
         bodySb.i(2).p("preds[1] = ACTIVATION[i][0];").nl();
       }
-      bodySb.i(2).p("preds[1] = " + new Distribution(model_info.get_params()._distribution, model_info.get_params()._tweedie_power).linkInvString("preds[1]")+";").nl();
+      bodySb.i(2).p("preds[1] = " + new Distribution(model_info.get_params()).linkInvString("preds[1]")+";").nl();
       bodySb.i(2).p("if (Double.isNaN(preds[1])) throw new RuntimeException(\"Predicted regression target NaN!\");").nl();
       bodySb.i(1).p("}").nl();
       bodySb.i().p("}").nl();
@@ -1455,7 +1456,11 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
       super();
       _stopping_rounds = 5;
     }
-  
+    @Override
+    public long progressUnits() {
+      if (train()==null) return 1;
+      return (long)Math.ceil(_epochs*train().numRows());
+    }
     @Override
     public double missingColumnsType() {
       return _sparse ? 0 : Double.NaN;
@@ -1860,7 +1865,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
      * Absolute, Quadratic, Huber or CrossEntropy for classification
      */
     public enum Loss {
-      Automatic, Quadratic, CrossEntropy, Huber, Absolute
+      Automatic, Quadratic, CrossEntropy, Huber, Absolute, Quantile
     }
   
     /**
@@ -1966,6 +1971,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
           case gaussian:
           case huber:
           case laplace:
+          case quantile:
           case tweedie:
           case gamma:
           case poisson:
@@ -1994,6 +2000,10 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
             if (_loss != Loss.Absolute && _loss != Loss.Automatic)
               dl.error("_distribution", "Only Automatic or Absolute loss is allowed for " + _distribution + " distribution.");
             break;
+          case quantile:
+            if (_loss != Loss.Quantile && _loss != Loss.Automatic)
+              dl.error("_distribution", "Only Automatic or Quantile loss is allowed for " + _distribution + " distribution.");
+            break;
           case huber:
             if (_loss != Loss.Huber && _loss != Loss.Automatic)
               dl.error("_distribution", "Only Automatic or Huber loss is allowed for " + _distribution + " distribution.");
@@ -2136,6 +2146,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
               "_max_categorical_features",
               "_nfolds",
               "_distribution",
+              "_quantile_alpha",
               "_tweedie_power"
       };
   
@@ -2147,6 +2158,7 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
                   ) {
             if (f.getName().equals("_hidden")) continue;
             if (f.getName().equals("_ignored_columns")) continue;
+	    if (f.getName().equals("$jacocoData")) continue; // If code coverage is enabled
             throw H2O.unimpl("Please add " + f.getName() + " to either cp_modifiable or cp_not_modifiable");
           }
       }
@@ -2319,6 +2331,9 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
               case Absolute:
                 toParms._distribution = Distribution.Family.laplace;
                 break;
+              case Quantile:
+                toParms._distribution = Distribution.Family.quantile;
+                break;
               case Huber:
                 toParms._distribution = Distribution.Family.huber;
                 break;
@@ -2333,6 +2348,9 @@ public class DeepLearningModel extends Model<DeepLearningModel,DeepLearningModel
             case gaussian:
               toParms._loss = Loss.Quadratic;
               break;
+            case quantile:
+              toParms._loss = Loss.Quantile;
+              break;
             case laplace:
               toParms._loss = Loss.Absolute;
               break;
diff --git a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java
index e160495..4bcb765 100644
--- a/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/DeepLearningModelInfo.java
@@ -60,7 +60,7 @@ final public class DeepLearningModelInfo extends Iced {
     if (cats == null) return;
     if (_saw_missing_cats == null) return;
     for (int i=0; i<cats.length; ++i) {
-      assert(data_info._catMissing[i] == 1); //have a missing bucket for each categorical
+      assert(data_info._catMissing[i]); //have a missing bucket for each categorical
       if (_saw_missing_cats[i]) continue;
       _saw_missing_cats[i] = (cats[i] == data_info._catOffsets[i+1]-1);
     }
@@ -589,7 +589,7 @@ final public class DeepLearningModelInfo extends Iced {
     // zero out missing categorical variables if they were never seen
     if (_saw_missing_cats != null) {
       for (int i = 0; i < _saw_missing_cats.length; ++i) {
-        assert (data_info._catMissing[i] == 1); //have a missing bucket for each categorical
+        assert (data_info._catMissing[i]); //have a missing bucket for each categorical
         if (!_saw_missing_cats[i]) vi[data_info._catOffsets[i + 1] - 1] = 0;
       }
     }
diff --git a/h2o-algos/src/main/java/hex/deeplearning/Neurons.java b/h2o-algos/src/main/java/hex/deeplearning/Neurons.java
index abda351..2a72037 100644
--- a/h2o-algos/src/main/java/hex/deeplearning/Neurons.java
+++ b/h2o-algos/src/main/java/hex/deeplearning/Neurons.java
@@ -134,7 +134,7 @@ public abstract class Neurons {
     params._hidden_dropout_ratios = minfo.get_params()._hidden_dropout_ratios;
     params._rate *= Math.pow(params._rate_decay, index-1);
     params._distribution = minfo.get_params()._distribution;
-    _dist = new Distribution(params._distribution, params._tweedie_power);
+    _dist = new Distribution(params);
     _a = new Storage.DenseVector(units);
     if (!(this instanceof Input)) {
       _e = new Storage.DenseVector(units);
@@ -481,7 +481,7 @@ public abstract class Neurons {
       int    [] cats = MemoryManager.malloc4(_dinfo._cats); // a bit wasteful - reallocated each time
       int i = 0, ncats = 0;
       for(; i < _dinfo._cats; ++i){
-        assert(_dinfo._catMissing[i] != 0); //we now *always* have a categorical level for NAs, just in case.
+        assert(_dinfo._catMissing[i]); //we now *always* have a categorical level for NAs, just in case.
         if (Double.isNaN(data[i])) {
           cats[ncats] = (_dinfo._catOffsets[i+1]-1); //use the extra level for NAs made during training
         } else {
@@ -499,8 +499,7 @@ public abstract class Neurons {
         }
         ncats++;
       }
-      final int n = data.length - (_dinfo._weights ? 1 : 0) - (_dinfo._offset ? 1 : 0);
-      for(;i < n;++i){
+      for(;i < data.length;++i){
         double d = data[i];
         if(_dinfo._normMul != null) d = (d - _dinfo._normSub[i-_dinfo._cats])*_dinfo._normMul[i-_dinfo._cats];
         nums[i-_dinfo._cats] = d; //can be NaN for missing numerical data
diff --git a/h2o-algos/src/main/java/hex/example/Example.java b/h2o-algos/src/main/java/hex/example/Example.java
index baa8762..57d0801 100644
--- a/h2o-algos/src/main/java/hex/example/Example.java
+++ b/h2o-algos/src/main/java/hex/example/Example.java
@@ -20,7 +20,6 @@ public class Example extends ModelBuilder<ExampleModel,ExampleParameters,Example
   // Called from Nano thread; start the Example Job on a F/J thread
   public Example( ExampleModel.ExampleParameters parms ) { super(parms); init(false); }
   @Override protected ExampleDriver trainModelImpl() { return new ExampleDriver(); }
-  @Override public long progressUnits() { return _parms._max_iterations; }
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
diff --git a/h2o-algos/src/main/java/hex/example/ExampleModel.java b/h2o-algos/src/main/java/hex/example/ExampleModel.java
index f066ef4..b1d40e3 100644
--- a/h2o-algos/src/main/java/hex/example/ExampleModel.java
+++ b/h2o-algos/src/main/java/hex/example/ExampleModel.java
@@ -12,6 +12,7 @@ public class ExampleModel extends Model<ExampleModel,ExampleModel.ExampleParamet
     public String algoName() { return "Example"; }
     public String fullName() { return "Example"; }
     public String javaName() { return ExampleModel.class.getName(); }
+    @Override public long progressUnits() { return _max_iterations; }
     public int _max_iterations = 1000; // Max iterations
   }
 
diff --git a/h2o-algos/src/main/java/hex/glm/ComputationState.java b/h2o-algos/src/main/java/hex/glm/ComputationState.java
index 34a49bd..49cfac1 100644
--- a/h2o-algos/src/main/java/hex/glm/ComputationState.java
+++ b/h2o-algos/src/main/java/hex/glm/ComputationState.java
@@ -13,6 +13,7 @@ import water.H2O;
 import water.Key;
 import water.MemoryManager;
 import water.util.ArrayUtils;
+import water.util.Log;
 import water.util.MathUtils;
 
 import java.text.DecimalFormat;
@@ -119,8 +120,8 @@ public final class ComputationState {
     int selected = 0;
     _activeBC = _bc;
     _activeData = _activeData != null?_activeData:_dinfo;
-    if (!_allIn && _alpha > 0) {
-      final double rhs = _alpha * (2 * _lambda - _previousLambda);
+    if (!_allIn) {
+      final double rhs = Math.abs(_alpha * (2 * _lambda - _previousLambda));
       int [] cols = MemoryManager.malloc4(P);
       int j = 0;
       int[] oldActiveCols = _activeData._activeCols == null ? new int[0] : _activeData.activeCols();
@@ -128,25 +129,27 @@ public final class ComputationState {
         if (j < oldActiveCols.length && i == oldActiveCols[j]) {
           cols[selected++] = i;
           ++j;
-        } else if (_ginfo._gradient[i] > rhs || _ginfo._gradient[i] < -rhs) {
+        } else if (_ginfo._gradient[i] > rhs || -_ginfo._gradient[i] > rhs) {
           cols[selected++] = i;
         }
       }
-      _allIn = _alpha == 0 || selected == P;
+      _allIn = selected == P;
       if(!_allIn) {
-        if (_intercept) cols[selected++] = P;
+        cols[selected++] = P; // intercept is always selected, even if it is false (it's gonna be dropped later, it is needed for other stuff too)
         cols = Arrays.copyOf(cols, selected);
         double [] b = ArrayUtils.select(_beta, cols);
         assert Arrays.equals(_beta,ArrayUtils.expandAndScatter(b,_dinfo.fullN()+1,cols));
         _beta = b;
-        _ginfo = new GLMGradientInfo(_ginfo._likelihood, _ginfo._objVal, ArrayUtils.select(_ginfo._gradient, cols));
         _activeData = _dinfo.filterExpandedColumns(Arrays.copyOf(cols, selected));
+        _ginfo = new GLMGradientInfo(_ginfo._likelihood, _ginfo._objVal, ArrayUtils.select(_ginfo._gradient, cols));
         _activeBC = _bc.filterExpandedColumns(_activeData.activeCols());
         _gslvr = new GLMGradientSolver(_jobKey,_parms,_activeData,(1-_alpha)*_lambda,_bc);
         assert _beta.length == selected;
-      } else _activeData = _dinfo;
+        return selected;
+      }
     }
-    return selected;
+    _activeData = _dinfo;
+    return _dinfo.fullN();
   }
 
   private DataInfo [] _activeDataMultinomial;
@@ -222,7 +225,7 @@ public final class ComputationState {
     int selected = 0;
     _activeBC = _bc;
     _activeData = _dinfo;
-    if (!_allIn && _alpha > 0) {
+    if (!_allIn) {
       if(_activeDataMultinomial == null)
         _activeDataMultinomial = new DataInfo[_nclasses];
       final double rhs = _alpha * (2 * _lambda - _previousLambda);
@@ -245,7 +248,7 @@ public final class ComputationState {
         for(int i = start; i < selected; ++i)
           cols[i] += c*N;
       }
-      _allIn = _alpha == 0 || selected == cols.length;
+      _allIn = selected == cols.length;
     }
     return selected;
   }
@@ -260,7 +263,7 @@ public final class ComputationState {
       return checkKKTsMultinomial();
     double [] beta = _beta;
     if(_activeData._activeCols != null)
-      beta = ArrayUtils.expandAndScatter(beta,_dinfo.fullN() + (_intercept?1:0),_activeData._activeCols);
+      beta = ArrayUtils.expandAndScatter(beta,_dinfo.fullN() + 1,_activeData._activeCols);
     int [] activeCols = _activeData.activeCols();
     _gslvr = new GLMGradientSolver(_jobKey,_parms,_dinfo,(1-_alpha)*_lambda,_bc);
     GLMGradientInfo ginfo = _gslvr.getGradient(beta);
@@ -274,7 +277,7 @@ public final class ComputationState {
     _beta = beta;
     _ginfo = ginfo;
     _activeBC = null;
-    if(!_allIn && _lambda*_alpha > 0) {
+    if(!_allIn) {
       int[] failedCols = new int[64];
       int fcnt = 0;
       for (int i = 0; i < grad.length - 1; ++i) {
@@ -286,6 +289,7 @@ public final class ComputationState {
         }
       }
       if (fcnt > 0) {
+        Log.info(fcnt + " variables failed KKT conditions, adding them to the model and recomputing.");
         final int n = activeCols.length;
         int[] newCols = Arrays.copyOf(activeCols, activeCols.length + fcnt);
         for (int i = 0; i < fcnt; ++i)
@@ -302,12 +306,13 @@ public final class ComputationState {
     return true;
   }
   public int []  removeCols(int [] cols) {
-    int [] activeCols = ArrayUtils.removeSorted(_activeData.activeCols(),cols);
+    int [] activeCols = ArrayUtils.removeIds(_activeData.activeCols(),cols);
     if(_beta != null)
-      _beta = ArrayUtils.select(_beta,activeCols);
+      _beta = ArrayUtils.removeIds(_beta,cols);
     if(_ginfo != null)
-      _ginfo._gradient = ArrayUtils.select(_ginfo._gradient,activeCols);
-    _activeData = _activeData.filterExpandedColumns(activeCols);
+      _ginfo._gradient = ArrayUtils.removeIds(_ginfo._gradient,cols);
+    _activeData = _dinfo.filterExpandedColumns(activeCols);
+    _activeBC = _bc.filterExpandedColumns(activeCols);
     _gslvr = new GLMGradientSolver(_jobKey, _parms, _activeData, (1 - _alpha) * _lambda, _activeBC);
     return activeCols;
   }
diff --git a/h2o-algos/src/main/java/hex/glm/GLM.java b/h2o-algos/src/main/java/hex/glm/GLM.java
index dd358b6..80b5ebb 100644
--- a/h2o-algos/src/main/java/hex/glm/GLM.java
+++ b/h2o-algos/src/main/java/hex/glm/GLM.java
@@ -302,7 +302,7 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         _parms._max_active_predictors = _parms._solver == Solver.IRLSM ? 7000 : 100000000;
       if (_parms._link == Link.family_default)
         _parms._link = _parms._family.defaultLink;
-      _dinfo = new DataInfo(_train.clone(), _valid, 1, _parms._use_all_factor_levels || _parms._lambda_search, _parms._standardize ? DataInfo.TransformType.STANDARDIZE : DataInfo.TransformType.NONE, DataInfo.TransformType.NONE, _parms._missing_values_handling == MissingValuesHandling.Skip, _parms._missing_values_handling == MissingValuesHandling.MeanImputation, false, hasWeightCol(), hasOffsetCol(), hasFoldCol());
+      _dinfo = new DataInfo(_train.clone(), _valid, 1, _parms._use_all_factor_levels || _parms._lambda_search, _parms._standardize ? DataInfo.TransformType.STANDARDIZE : DataInfo.TransformType.NONE, DataInfo.TransformType.NONE, _parms._missing_values_handling == MissingValuesHandling.Skip, false ,_parms._missing_values_handling == MissingValuesHandling.MeanImputation, hasWeightCol(), hasOffsetCol(), hasFoldCol());
       checkMemoryFootPrint(_dinfo);
       if (_parms._max_iterations == -1) { // fill in default max iterations
         int numclasses = _parms._family == Family.multinomial?nclasses():1;
@@ -315,6 +315,9 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         }
       }
       BetaConstraint bc = (_parms._beta_constraints != null)?new BetaConstraint(_parms._beta_constraints.get()):new BetaConstraint();
+      if((bc.hasBounds() || bc.hasProximalPenalty()) && _parms._compute_p_values)
+        error("_compute_p_values","P-values can not be computed for constrained problems");
+
       if (_valid != null)
         _validDinfo = _dinfo.validDinfo(_valid);
       _state = new ComputationState(_job._key, _parms, _dinfo, bc, nclasses());
@@ -326,7 +329,7 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
           Vec wc = _weights == null ? _dinfo._adaptedFrame.anyVec().makeCon(1) : _weights.makeCopy();
           _dinfo.setWeights(_generatedWeights = "__glm_gen_weights", wc);
         }
-        YMUTask ymt = new YMUTask(_dinfo, _parms._family == Family.multinomial?nclasses():1, !_parms._stdOverride, setWeights, skippingRows).doAll(_dinfo._adaptedFrame);
+        YMUTask ymt = new YMUTask(_dinfo, _parms._family == Family.multinomial?nclasses():1, !_parms._stdOverride, setWeights, skippingRows,true).doAll(_dinfo._adaptedFrame);
         if (ymt._wsum == 0)
           throw new IllegalArgumentException("No rows left in the dataset after filtering out rows with missing values. Ignore columns with many NAs or impute your missing values prior to calling glm.");
         Log.info(LogMsg("using " + ymt._nobs + " nobs out of " + _dinfo._adaptedFrame.numRows() + " total"));
@@ -396,12 +399,10 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
     }
   }
 
-  private static final long WORK_TOTAL = 1000000;
+  protected static final long WORK_TOTAL = 1000000;
 
   @Override protected GLMDriver trainModelImpl() { return new GLMDriver(); }
 
-  @Override public long progressUnits() { return WORK_TOTAL; }
-
   private final double lmax(double[] grad) {
     return Math.max(ArrayUtils.maxValue(grad), -ArrayUtils.minValue(grad)) / Math.max(1e-2, _parms._alpha[0]);
   }
@@ -435,11 +436,20 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         gram.dropIntercept();
         xy = Arrays.copyOf(xy, xy.length - 1);
       }
+      int [] zeros = gram.dropZeroCols();
+
+      assert zeros.length == 0:"zero column(s) in gram matrix";
       gram.mul(_parms._obj_reg);
       ArrayUtils.mult(xy, _parms._obj_reg);
       if(_parms._remove_collinear_columns || _parms._compute_p_values) {
         ArrayList<Integer> ignoredCols = new ArrayList<>();
-        Cholesky chol = ((_state._iter == 0 && _parms._remove_collinear_columns)?gram.qrCholesky(ignoredCols):gram.cholesky(null));
+        Cholesky chol = ((_state._iter == 0)?gram.qrCholesky(ignoredCols):gram.cholesky(null));
+        if(!ignoredCols.isEmpty() && !_parms._remove_collinear_columns) {
+          int [] collinear_cols = new int[ignoredCols.size()];
+          for(int i = 0; i < collinear_cols.length; ++i)
+            collinear_cols[i] = ignoredCols.get(i);
+          throw new NonSPDMatrixException("Found collinear columns in the dataset. Can not compute compute p-values without removing them, set remove_collinear_columns flag to true. Found collinear columns " + Arrays.toString(ArrayUtils.select(_dinfo.coefNames(),collinear_cols)));
+        }
         if(!chol.isSPD()) throw new NonSPDMatrixException();
         _chol = chol;
         if(!ignoredCols.isEmpty()) { // got some redundant cols
@@ -450,7 +460,8 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
           // need to drop the cols from everywhere
           _model.addWarning("Removed collinear columns " + Arrays.toString(collinear_col_names));
           Log.warn("Removed collinear columns " + Arrays.toString(collinear_col_names));
-          xy = ArrayUtils.select(xy,_state.removeCols(collinear_cols));
+          _state.removeCols(collinear_cols);
+          xy = ArrayUtils.removeIds(xy,collinear_cols);
         }
         chol.solve(xy);
       } else { // todo add switch between COD and ADMM
@@ -835,6 +846,9 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
       _keys2Keep.put("dest",dest());
       _parms.read_lock_frames(_job);
       init(true);
+      if (error_count() > 0) {
+        throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(GLM.this);
+      }
       double nullDevTrain = Double.NaN;
       double nullDevTest = Double.NaN;
       if(_parms._lambda_search) {
@@ -848,9 +862,7 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
         _workPerIteration = WORK_TOTAL/_parms._nlambdas;
       } else
         _workPerIteration = 1 + (WORK_TOTAL/_parms._max_iterations);
-      if (error_count() > 0) {
-        throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(GLM.this);
-      }
+
       if(_parms._family == Family.multinomial && _parms._solver == Solver.IRLSM) {
         double [] nb = getNullBeta();
         double maxRow = ArrayUtils.maxValue(nb);
@@ -866,12 +878,14 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
       for (int i = 0; i < _parms._lambda.length; ++i) { // lambda search
         _model.addSubmodel(_state.beta(),_parms._lambda[i],_state._iter);
         _state.setLambda(_parms._lambda[i]);
-        if(_parms._family == Family.multinomial)
-          for(int c = 0; c < _nclass; ++c)
-            Log.info(LogMsg("Class " + c + " got " + _state.activeDataMultinomial(c).fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
-        else
-          Log.info(LogMsg("Got " + _state.activeData().fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
-        do { fitModel(); } while(!_state.checkKKTs());
+        do {
+          if(_parms._family == Family.multinomial)
+            for(int c = 0; c < _nclass; ++c)
+              Log.info(LogMsg("Class " + c + " got " + _state.activeDataMultinomial(c).fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
+          else
+            Log.info(LogMsg("Got " + _state.activeData().fullN() + " active columns out of " + _state._dinfo.fullN() + " total"));
+          fitModel();
+        } while(!_state.checkKKTs());
         Log.info(LogMsg("solution has " + ArrayUtils.countNonzeros(_state.beta()) + " nonzeros"));
         if(_parms._lambda_search) {  // compute train and test dev
           double trainDev = _parms._family == Family.multinomial
@@ -1552,6 +1566,10 @@ public class GLM extends ModelBuilder<GLMModel,GLMParameters,GLMOutput> {
       return false;
     }
 
+    public boolean hasProximalPenalty() {
+      return _betaGiven != null && _rho != null && ArrayUtils.countNonzeros(_rho) > 0;
+    }
+
     public void adjustGradient(double[] beta, double[] grad) {
       if (_betaGiven != null && _rho != null) {
         for (int i = 0; i < _betaGiven.length; ++i) {
diff --git a/h2o-algos/src/main/java/hex/glm/GLMModel.java b/h2o-algos/src/main/java/hex/glm/GLMModel.java
index a82fb23..8b9578a 100755
--- a/h2o-algos/src/main/java/hex/glm/GLMModel.java
+++ b/h2o-algos/src/main/java/hex/glm/GLMModel.java
@@ -13,7 +13,6 @@ import hex.glm.GLMModel.GLMParameters.Link;
 import org.apache.commons.math3.distribution.NormalDistribution;
 import org.apache.commons.math3.distribution.RealDistribution;
 import org.apache.commons.math3.distribution.TDistribution;
-import water.DKV;
 import water.H2O;
 import water.Iced;
 import water.Key;
@@ -111,6 +110,7 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
     public String algoName() { return "GLM"; }
     public String fullName() { return "Generalized Linear Modeling"; }
     public String javaName() { return GLMModel.class.getName(); }
+    @Override public long progressUnits() { return GLM.WORK_TOTAL; }
     // public int _response; // TODO: the standard is now _response_column in SupervisedModel.SupervisedParameters
     public boolean _standardize = true;
     public Family _family;
@@ -164,7 +164,7 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
       if(_obj_reg != -1 && _obj_reg <= 0)
         glm.error("obj_reg","Must be positive or -1 for default");
       if(_prior != -1 && _prior <= 0 || _prior >= 1)
-        glm.error("_prior","Prior must be in (exlusive) range (0,1)");
+        glm.error("_prior","Prior must be in (exclusive) range (0,1)");
       if(_family != Family.tweedie) {
         glm.hide("_tweedie_variance_power","Only applicable with Tweedie family");
         glm.hide("_tweedie_link_power","Only applicable with Tweedie family");
@@ -945,125 +945,56 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
     return super.checksum_impl();
   }
 
-  private double [] scoreMultinomial(Chunk[] chks, int row_in_chunk, double[] tmp, double[] preds) {
-    double[] eta = MemoryManager.malloc8d(_output.nclasses());
-    final double[][] b = _output._global_beta_multinomial;
-    final int P = b[0].length;
-    int[] catOffs = dinfo()._catOffsets;
-    for (int i = 0; i < catOffs.length - 1; ++i) {
-      if (chks[i].isNA(row_in_chunk)) {
-        Arrays.fill(eta, Double.NaN);
-        break;
+  private double [] scoreRow(Row r, double o, double [] preds) {
+    if(_parms._family == Family.multinomial) {
+      if(_eta == null) _eta = new ThreadLocal<>();
+      double[] eta = _eta.get();
+      if(eta == null) _eta.set(eta = MemoryManager.malloc8d(_output.nclasses()));
+      final double[][] bm = _output._global_beta_multinomial;
+      double sumExp = 0;
+      double maxRow = 0;
+      for (int c = 0; c < bm.length; ++c) {
+        eta[c] = r.innerProduct(bm[c]) + o;
+        if(eta[c] > maxRow)
+          maxRow = eta[c];
       }
-      long lval = chks[i].at8(row_in_chunk);
-      int ival = (int) lval;
-      if (ival != lval) throw new IllegalArgumentException("categorical value out of range");
-      if (!_parms._use_all_factor_levels) --ival;
-      int from = catOffs[i];
-      int to = catOffs[i + 1];
-      // can get values out of bounds for cat levels not seen in training
-      if (ival >= 0 && (ival + from) < catOffs[i + 1])
-        for (int j = 0; j < _output.nclasses(); ++j)
-          eta[j] += b[j][ival + from];
-    }
-    final int noff = dinfo().numStart() - dinfo()._cats;
-    for (int i = dinfo()._cats; i < b.length - 1 - noff; ++i) {
-      double d = chks[i].atd(row_in_chunk);
-      for (int j = 0; j < _output.nclasses(); ++j)
-        eta[j] += b[j][noff + i] * d;
-    }
-    double sumExp = 0;
-    double max_row = 0;
-    for (int j = 0; j < _output.nclasses(); ++j) {
-      eta[j] += b[j][P - 1];
-      if(eta[j] > max_row)
-        max_row = eta[j];
+      for (int c = 0; c < bm.length; ++c)
+        sumExp += eta[c] = Math.exp(eta[c]-maxRow); // intercept
+      sumExp = 1.0 / sumExp;
+      for (int c = 0; c < bm.length; ++c)
+        preds[c + 1] = eta[c] * sumExp;
+      preds[0] = ArrayUtils.maxIndex(eta);
+    } else {
+      double mu = _parms.linkInv(r.innerProduct(beta()) + o);
+      if (_parms._family == Family.binomial) { // threshold for prediction
+        preds[0] = mu >= defaultThreshold()?1:0;
+        preds[1] = 1.0 - mu; // class 0
+        preds[2] = mu; // class 1
+      } else
+        preds[0] = mu;
     }
-    for (int j = 0; j < _output.nclasses(); ++j)
-      sumExp += eta[j] = Math.exp(eta[j]-max_row); // intercept
-    sumExp = 1.0 / sumExp;
-    for (int i = 0; i < eta.length; ++i)
-      preds[i + 1] = eta[i] * sumExp;
-    preds[0] = ArrayUtils.maxIndex(eta);
     return preds;
   }
 
   private transient ThreadLocal<Row> _row;
+  private transient ThreadLocal<double[]> _eta;
 
-  @Override
-  // public double[] score0( Chunk chks[], double weight, double offset, int row_in_chunk, double[] tmp, double[] preds )
-  public double[] score0(Chunk[] chks, double weight, double offset, int row_in_chunk, double[] tmp, double[] preds) {
-    if(_parms._family == Family.multinomial)
-      return scoreMultinomial(chks,row_in_chunk,tmp,preds);
+  private final Row getRow(){
     if(_row == null) _row = new ThreadLocal<>();
     Row r = _row.get();
     if(r == null) _row.set(r = _output._scoringDinfo.newDenseRow());
-    _output._scoringDinfo.extractDenseRow(chks,row_in_chunk,r);
-    double mu = preds[0] = _parms.linkInv(r.innerProduct(beta()) + offset);
-    if( _parms._family == Family.binomial ) { // threshold for prediction
-      preds[1] = 1.0 - mu; // class 0
-      preds[2] =       mu; // class 1
-    }
-    return preds;
+    return r;
   }
 
-  @Override protected double[] score0(double[] data, double[] preds){return score0(data,preds,1,0);}
 
-  private double [] scoreMultinomial(double[] data, double[] preds, double w, double o) {
-    double [] eta = MemoryManager.malloc8d(_output.nclasses());
-    final double [][] b = _output._global_beta_multinomial;
-    final int P = b[0].length;
-    final DataInfo dinfo = _output._dinfo;
-    for(int i = 0; i < dinfo._cats; ++i) {
-      if(Double.isNaN(data[i])) {
-        Arrays.fill(eta,Double.NaN);
-        break;
-      }
-      int ival = (int) data[i];
-      if (ival != data[i]) throw new IllegalArgumentException("categorical value out of range");
-      ival += dinfo._catOffsets[i];
-      if (!_parms._use_all_factor_levels)
-        --ival;
-      // can get values out of bounds for cat levels not seen in training
-      if (ival >= dinfo._catOffsets[i] && ival < dinfo._catOffsets[i + 1])
-        for(int j = 0; j < eta.length; ++j)
-          eta[j] += b[j][ival];
-    }
-    int noff = dinfo.numStart();
-    for(int i = 0; i < dinfo._nums; ++i) {
-      double d = data[dinfo._cats + i];
-      for (int j = 0; j < eta.length; ++j)
-        eta[j] += b[j][noff + i] * d;
-    }
-    double sumExp = 0;
-    double max_row = 0;
-    for (int j = 0; j < eta.length; ++j) {
-      eta[j] += b[j][P - 1];
-      if(eta[j] > max_row)
-        max_row = eta[j];
-    }
-    for (int j = 0; j < eta.length; ++j)
-      sumExp += (eta[j] = Math.exp(eta[j]-max_row));
-    sumExp = 1.0/sumExp;
-    preds[0] = ArrayUtils.maxIndex(eta);
-    for(int i = 0; i < eta.length; ++i)
-      preds[1+i] = eta[i]*sumExp;
-    return preds;
+  @Override
+  // public double[] score0( Chunk chks[], double weight, double offset, int row_in_chunk, double[] tmp, double[] preds )
+  public double[] score0(Chunk[] chks, double weight, double offset, int row_in_chunk, double[] tmp, double[] preds) {
+    return scoreRow(_output._scoringDinfo.extractDenseRow(chks,row_in_chunk,getRow()),offset,preds);
   }
-
+  @Override protected double[] score0(double[] data, double[] preds){return score0(data,preds,1,0);}
   @Override protected double[] score0(double[] data, double[] preds, double w, double o) {
-    if(_parms._family == Family.multinomial)
-      return scoreMultinomial(data, preds, w, o);
-    Row r = _row.get();
-    if(r == null) _row.set(r = _output._scoringDinfo.newDenseRow());
-    _output._scoringDinfo.extractDenseRow(data,r,w,0);
-    double eta = r.innerProduct(beta());
-    double mu = preds[0] = _parms.linkInv(eta + o);
-    if( _parms._family == Family.binomial ) { // threshold for prediction
-      preds[1] = 1.0 - mu; // class 0
-      preds[2] =       mu; // class 1
-    }
-    return preds;
+    return scoreRow(_output._scoringDinfo.extractDenseRow(data,getRow()),o,preds);
   }
 
   @Override protected void toJavaPredictBody(SBPrintStream body,
@@ -1075,12 +1006,16 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
       @Override
       public void generate(JCodeSB out) {
         JCodeGen.toClassWithArray(out, "static", "BETA", beta_internal()); // "The Coefficients"
+        JCodeGen.toClassWithArray(out, "static", "NUM_MEANS", _output._dinfo._numMeans,"Imputed numeric values");
+        JCodeGen.toClassWithArray(out, "static", "CAT_MODES", _output._dinfo._catModes,"Imputed categorical values.");
         JCodeGen.toStaticVar(out, "CATOFFS", dinfo()._catOffsets, "Categorical Offsets");
       }
     });
-
     body.ip("final double [] b = BETA.VALUES;").nl();
-
+    if(_parms._missing_values_handling == MissingValuesHandling.MeanImputation){
+      body.ip("for(int i = 0; i < " + _output._dinfo._cats + "; ++i) if(Double.isNaN(data[i])) data[i] = CAT_MODES.VALUES[i];").nl();
+      body.ip("for(int i = 0; i < " + _output._dinfo._nums + "; ++i) if(Double.isNaN(data[i + " + _output._dinfo._cats + "])) data[i+" + _output._dinfo._cats + "] = NUM_MEANS.VALUES[i];").nl();
+    }
     if(_parms._family != Family.multinomial) {
       body.ip("double eta = 0.0;").nl();
       if (!_parms._use_all_factor_levels) { // skip level 0 of all factors
@@ -1107,7 +1042,6 @@ public class GLMModel extends Model<GLMModel,GLMModel.GLMParameters,GLMModel.GLM
         body.ip("double mu = hex.genmodel.GenModel.GLM_").p(_parms._link.toString()).p("Inv(eta");
       else
         body.ip("double mu = hex.genmodel.GenModel.GLM_tweedieInv(eta," + _parms._tweedie_link_power);
-//    if( _parms._link == hex.glm.GLMModel.GLMParameters.Link.tweedie ) body.p(",").p(_parms._tweedie_link_power);
       body.p(");").nl();
       if (_parms._family == Family.binomial) {
         body.ip("preds[0] = (mu > ").p(defaultThreshold()).p(") ? 1 : 0").p("; // threshold given by ROC").nl();
diff --git a/h2o-algos/src/main/java/hex/glm/GLMTask.java b/h2o-algos/src/main/java/hex/glm/GLMTask.java
index 6071e2e..d166934 100644
--- a/h2o-algos/src/main/java/hex/glm/GLMTask.java
+++ b/h2o-algos/src/main/java/hex/glm/GLMTask.java
@@ -3,8 +3,6 @@ package hex.glm;
 import hex.DataInfo;
 import hex.DataInfo.Row;
 
-import hex.DataInfo.Rows;
-import hex.FrameTask;
 import hex.FrameTask2;
 import hex.glm.GLMModel.GLMParameters;
 import hex.glm.GLMModel.GLMParameters.Link;
@@ -12,7 +10,6 @@ import hex.glm.GLMModel.GLMWeightsFun;
 import hex.glm.GLMModel.GLMWeights;
 import hex.gram.Gram;
 import hex.glm.GLMModel.GLMParameters.Family;
-import jsr166y.CountedCompleter;
 import water.H2O.H2OCountedCompleter;
 import water.*;
 import water.fvec.*;
@@ -123,34 +120,34 @@ public abstract class GLMTask  {
     @Override public void reduce(GLMResDevTaskMultinomial gt) {_likelihood += gt._likelihood;}
   }
 
- static class YMUTask extends MRTask<YMUTask> {
+ static public class YMUTask extends MRTask<YMUTask> {
    double _yMin = Double.POSITIVE_INFINITY, _yMax = Double.NEGATIVE_INFINITY;
    long _nobs;
-   double _wsum;
+   public double _wsum;
    final int _responseId;
    final int _weightId;
    final int _offsetId;
    final int _nums; // number of numeric columns
    final int _numOff;
-   final boolean _setIgnores;
-   final boolean _comupteWeightedSigma;
+   final boolean _computeWeightedSigma;
    final boolean _skipNAs;
+   final boolean _computeWeightedMeanSigmaResponse;
 
-   BasicStats _basicStats;
+   public BasicStats _basicStats;
+   public BasicStats _basicStatsResponse;
    double [] _yMu;
    double [] _means;
    final int _nClasses;
 
-
-   public YMUTask(DataInfo dinfo, int nclasses, boolean computeWeightedSigma, boolean setIgnores, boolean skipNAs){
+   public YMUTask(DataInfo dinfo, int nclasses, boolean computeWeightedSigma, boolean computeWeightedMeanSigmaResponse, boolean skipNAs, boolean haveResponse){
      _nums = dinfo._nums;
      _numOff = dinfo._cats;
-     _responseId = dinfo.responseChunkId(0);
+     _responseId = haveResponse ? dinfo.responseChunkId(0) : -1;
      _weightId = dinfo._weights?dinfo.weightChunkId():-1;
      _offsetId = dinfo._offset?dinfo.offsetChunkId():-1;
      _nClasses = nclasses;
-     _comupteWeightedSigma = computeWeightedSigma;
-     _setIgnores = setIgnores;
+     _computeWeightedSigma = computeWeightedSigma;
+     _computeWeightedMeanSigmaResponse = computeWeightedMeanSigmaResponse;
      _skipNAs = skipNAs;
      _means = dinfo._numMeans;
    }
@@ -163,21 +160,27 @@ public abstract class GLMTask  {
      for(int i = 0; i < chunks.length; ++i) {
        for (int r = chunks[i].nextNZ(-1); r < chunks[i]._len; r = chunks[i].nextNZ(r)) {
          if(skip[r])continue;
-         if((skip[r] = _skipNAs && chunks[i].isNA(r)) && _setIgnores)
+         if((skip[r] = _skipNAs && chunks[i].isNA(r)) && _weightId != -1)
           weight.set(r,0);
        }
      }
-     Chunk response = chunks[_responseId];
+     Chunk response = _responseId < 0 ? null : chunks[_responseId];
      double [] nums = null;
-     if(_comupteWeightedSigma) {
+     double [] numsResponse = null;
+     if(_computeWeightedSigma) {
        _basicStats = new BasicStats(_nums);
        nums = MemoryManager.malloc8d(_nums);
      }
+     if(_computeWeightedMeanSigmaResponse) {
+       _basicStatsResponse = new BasicStats(_nClasses);
+       numsResponse = MemoryManager.malloc8d(_nClasses);
+     }
      double w;
+     if (response == null) return;
      for(int r = 0; r < response._len; ++r) {
        if(skip[r] || (w = weight.atd(r)) == 0)
          continue;
-       if(_comupteWeightedSigma) {
+       if(_computeWeightedSigma) {
          for(int i = 0; i < _nums; ++i) {
            nums[i] = chunks[i + _numOff].atd(r);
            if(Double.isNaN(nums[i]))
@@ -185,13 +188,19 @@ public abstract class GLMTask  {
          }
          _basicStats.add(nums,w);
        }
-       double d = w*response.atd(r);
+       if(_computeWeightedMeanSigmaResponse) {
+         //FIXME: Add support for subtracting offset from response
+         for(int i = 0; i < _nClasses; ++i)
+           numsResponse[i] = chunks[chunks.length-_nClasses+i].atd(r);
+         _basicStatsResponse.add(numsResponse,w);
+       }
+       double d = response.atd(r);
        if(!Double.isNaN(d)) {
          assert !Double.isNaN(d);
          if (_nClasses > 2)
-           _yMu[(int) d] += 1;
+           _yMu[(int) d] += w;
          else
-           _yMu[0] += d;
+           _yMu[0] += w*d;
          if (d < _yMin)
            _yMin = d;
          if (d > _yMax)
@@ -203,9 +212,8 @@ public abstract class GLMTask  {
    }
    @Override public void postGlobal() {
      ArrayUtils.mult(_yMu,1.0/_wsum);
-     Futures fs = new Futures();
-     fs.blockForPending();
    }
+
    @Override public void reduce(YMUTask ymt) {
      if(_nobs > 0 && ymt._nobs > 0) {
        ArrayUtils.add(_yMu,ymt._yMu);
@@ -215,15 +223,18 @@ public abstract class GLMTask  {
          _yMin = ymt._yMin;
        if(_yMax < ymt._yMax)
          _yMax = ymt._yMax;
-       if(_comupteWeightedSigma) {
+       if(_computeWeightedSigma)
          _basicStats.reduce(ymt._basicStats);
-       }
+       if(_computeWeightedMeanSigmaResponse)
+         _basicStatsResponse.reduce(ymt._basicStatsResponse);
      } else if (_nobs == 0) {
        _yMu = ymt._yMu;
        _nobs = ymt._nobs;
+       _wsum = ymt._wsum;
        _yMin = ymt._yMin;
        _yMax = ymt._yMax;
        _basicStats = ymt._basicStats;
+       _basicStatsResponse = ymt._basicStatsResponse;
      }
    }
  }
diff --git a/h2o-algos/src/main/java/hex/glrm/GLRM.java b/h2o-algos/src/main/java/hex/glrm/GLRM.java
index ba96db8..e668f26 100644
--- a/h2o-algos/src/main/java/hex/glrm/GLRM.java
+++ b/h2o-algos/src/main/java/hex/glrm/GLRM.java
@@ -54,7 +54,6 @@ public class GLRM extends ModelBuilder<GLRMModel,GLRMModel.GLRMParameters,GLRMMo
   private transient GLRMParameters.Loss[] _lossFunc;
 
   @Override protected GLRMDriver trainModelImpl() { return new GLRMDriver(); }
-  @Override public long progressUnits() { return 2 + _parms._max_iterations; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ModelCategory.Clustering}; }
   public enum Initialization { Random, SVD, PlusPlus, User }
 
diff --git a/h2o-algos/src/main/java/hex/glrm/GLRMModel.java b/h2o-algos/src/main/java/hex/glrm/GLRMModel.java
index 07034d6..796ef6a 100644
--- a/h2o-algos/src/main/java/hex/glrm/GLRMModel.java
+++ b/h2o-algos/src/main/java/hex/glrm/GLRMModel.java
@@ -19,6 +19,7 @@ public class GLRMModel extends Model<GLRMModel,GLRMModel.GLRMParameters,GLRMMode
     public String algoName() { return "GLRM"; }
     public String fullName() { return "Generalized Low Rank Modeling"; }
     public String javaName() { return GLRMModel.class.getName(); }
+    @Override public long progressUnits() { return 2 + _max_iterations; }
     public DataInfo.TransformType _transform = DataInfo.TransformType.NONE; // Data transformation (demean to compare with PCA)
     public int _k = 1;                            // Rank of resulting XY matrix
     public GLRM.Initialization _init = GLRM.Initialization.PlusPlus;  // Initialization of Y matrix
diff --git a/h2o-algos/src/main/java/hex/gram/Gram.java b/h2o-algos/src/main/java/hex/gram/Gram.java
index 9422198..d267824 100644
--- a/h2o-algos/src/main/java/hex/gram/Gram.java
+++ b/h2o-algos/src/main/java/hex/gram/Gram.java
@@ -19,7 +19,7 @@ public final class Gram extends Iced<Gram> {
   boolean _hasIntercept;
   public double[][] _xx;
   double[] _diag;
-  public final int _diagN;
+  public int _diagN;
   final int _denseN;
   int _fullN;
   final static int MIN_TSKSZ=10000;
@@ -142,7 +142,7 @@ public final class Gram extends Iced<Gram> {
     return res;
   }
 
-  private static double f_eps = 1e-8;
+  private static double f_eps = 1e-7;
   private static final int MIN_PAR = 1000;
 
   private final void updateZij(int i, int j, double [][] Z, double [] gamma) {
@@ -215,18 +215,7 @@ public final class Gram extends Iced<Gram> {
    * @return Cholesky - cholesky decomposition fo the gram
    */
   public Cholesky qrCholesky(ArrayList<Integer> dropped_cols) {
-    final double [][] Z = getXX(true);
-    // put intercept first
-    int icpt_id = Z.length-1;
-    double d = Z[0][0];
-    Z[0][0] = Z[icpt_id][icpt_id];
-    Z[icpt_id][icpt_id] = d;
-    for(int i = 1; i < Z.length-1; ++i) {
-      d = Z[i][0];
-      Z[i][0] = Z[icpt_id][i];
-      Z[icpt_id][i] = d;
-    }
-    // todo add diagonal hack to save on the largest categorical variable
+    final double [][] Z = getXX(true,true);
     final double [][] R = new double[Z.length][];
     final double [] ZdiagInv = new double[Z.length];
     for(int i = 0; i < Z.length; ++i)
@@ -239,9 +228,9 @@ public final class Gram extends Iced<Gram> {
       for(int k = 0; k < j; ++k) // only need the diagonal, the rest is 0 (dot product of orthogonal vectors)
         zjj += gamma[k] * (gamma[k] * Z[k][k] - 2*Z[j][k]);
       ZdiagInv[j] = 1./zjj;
-      if(-f_eps < zjj && zjj < f_eps) { // collinear column, drop it!
+      if(zjj < f_eps) { // collinear column, drop it!
         zjj = 0;
-        dropped_cols.add(j);
+        dropped_cols.add(j-1);
         ZdiagInv[j] = 0;
       }
       Z[j][j] = zjj;
@@ -302,7 +291,7 @@ public final class Gram extends Iced<Gram> {
       if(Z[i][i] == 0) continue;
       int k = 0;
       for(int l = 0; l <= i; ++l) {
-        if(k < dropped_cols.size() && l == dropped_cols.get(k)) {
+        if(k < dropped_cols.size() && l == (dropped_cols.get(k)+1)) {
           ++k;
           continue;
         }
@@ -310,14 +299,59 @@ public final class Gram extends Iced<Gram> {
       }
       ++j;
     }
-    if((dropped_cols.get(dropped_cols.size()-1)) == ZdiagInv.length-1){
-      dropped_cols.remove(dropped_cols.size()-1);
-      dropped_cols.add(0,0); // first and last columns are switched so that the intercept is the first drugin the QR decomp
-    }
     return new Cholesky(Rnew,new double[0], true);
   }
 
 
+  public int [] dropZeroCols(){
+    ArrayList<Integer> zeros = new ArrayList<>();
+    if(_diag != null)
+      for(int i = 0; i < _diag.length; ++i)
+        if(_diag[i] == 0)zeros.add(i);
+    int diagZeros = zeros.size();
+    for(int i = 0; i < _xx.length; ++i)
+      if(_xx[i][_xx[i].length-1] == 0)
+        zeros.add(_xx[i].length-1);
+    if(zeros.size() == 0) return new int[0];
+    int [] ary = new int[zeros.size() + 1];
+    ary[ary.length-1] = -1;
+    for(int i = 0; i < zeros.size(); ++i)
+      ary[i] = zeros.get(i);
+    int j = 0;
+    if(diagZeros > 0) {
+      double [] diag = MemoryManager.malloc8d(_diagN - diagZeros);
+      int k = 0;
+      for(int i = 0; i < _diagN; ++i)
+        if (ary[j] == i) {
+          ++j;
+        } else  diag[k++] = _diag[i];
+      _diag = diag;
+    }
+    double [][] xxNew = new double[_xx.length-ary.length+diagZeros+1][];
+    int iNew = 0;
+    for(int i = 0; i < _xx.length; ++i) {
+      if((_diagN + i) == ary[j]){
+        ++j; continue;
+      }
+      if(j == 0) {
+        xxNew[iNew++] = _xx[i];
+        continue;
+      }
+      int l = 0,m = 0;
+      double [] x = MemoryManager.malloc8d(_xx[i].length-j);
+      for(int k = 0; k < _xx[i].length; ++k)
+        if(k == ary[l]) {
+          ++l;
+        } else
+          x[m++] = _xx[i][k];
+      xxNew[iNew++] = x;
+    }
+    _xx = xxNew;
+    _diagN = _diag.length;
+    _fullN = _xx[_xx.length-1].length;
+    return Arrays.copyOf(ary,ary.length-1);
+  }
+
   public String toString(){
     if(_fullN >= 1000){
       if(_denseN >= 1000) return "Gram(" + _fullN + ")";
@@ -484,19 +518,27 @@ public final class Gram extends Iced<Gram> {
     return chol;
   }
 
-  public double[][] getXX(){return getXX(false);}
-  public double[][] getXX(boolean lowerDiag) {
+  public double[][] getXX(){return getXX(false, false);}
+  public double[][] getXX(boolean lowerDiag, boolean icptFist) {
     final int N = _fullN;
     double[][] xx = new double[N][];
     for( int i = 0; i < N; ++i )
       xx[i] = MemoryManager.malloc8d(lowerDiag?i+1:N);
+    int off = 0;
+    if(icptFist) {
+      double [] icptRow = _xx[_xx.length-1];
+      xx[0][0] = icptRow[icptRow.length-1];
+      for(int i = 0; i < icptRow.length-1; ++i)
+        xx[i+1][0] = icptRow[i];
+      off = 1;
+    }
     for( int i = 0; i < _diag.length; ++i )
-      xx[i][i] = _diag[i];
-    for( int i = 0; i < _xx.length; ++i ) {
+      xx[i+off][i+off] = _diag[i];
+    for( int i = 0; i < _xx.length - off; ++i ) {
       for( int j = 0; j < _xx[i].length; ++j ) {
-        xx[i + _diag.length][j] = _xx[i][j];
+        xx[i + _diag.length + off][j + off] = _xx[i][j];
         if(!lowerDiag)
-          xx[j][i + _diag.length] = _xx[i][j];
+          xx[j + off][i + _diag.length + off] = _xx[i][j];
       }
     }
     return xx;
@@ -906,9 +948,10 @@ public final class Gram extends Iced<Gram> {
     public final void   solve(double[] y) {
       if( !isSPD() ) throw new NonSPDMatrixException();
       if(_icptFirst) {
-        double d = y[y.length-1];
-        y[y.length-1] = y[0];
-        y[0] = d;
+        double icpt = y[y.length-1];
+        for(int i = y.length-1; i > 0; --i)
+          y[i] = y[i-1];
+        y[0] = icpt;
       }
       // diagonal
       for( int k = 0; k < _diag.length; ++k )
@@ -932,9 +975,10 @@ public final class Gram extends Iced<Gram> {
       for( int k = _diag.length - 1; k >= 0; --k )
         y[k] /= _diag[k];
       if(_icptFirst) {
-        double d = y[y.length-1];
-        y[y.length-1] = y[0];
-        y[0] = d;
+        double icpt = y[0];
+        for(int i = 1; i < y.length; ++i)
+          y[i-1] = y[i];
+        y[y.length-1] = icpt;
       }
     }
     public final boolean isSPD() {return _isSPD;}
@@ -1039,7 +1083,7 @@ public final class Gram extends Iced<Gram> {
 
   public void mul(double [] x, double [] res){
     Arrays.fill(res,0);
-    if(XX == null) XX = getXX(false);
+    if(XX == null) XX = getXX(false,false);
     for(int i = 0; i < XX.length; ++i){
       double d  = 0;
       double [] xi = XX[i];
@@ -1101,6 +1145,9 @@ public final class Gram extends Iced<Gram> {
       _nobs += gt._nobs;
     }
   }
-  public static class NonSPDMatrixException extends RuntimeException {}
+  public static class NonSPDMatrixException extends RuntimeException {
+    public NonSPDMatrixException(){}
+    public NonSPDMatrixException(String msg){super(msg);}
+  }
 }
 
diff --git a/h2o-algos/src/main/java/hex/grep/Grep.java b/h2o-algos/src/main/java/hex/grep/Grep.java
index 145a826..69d21fe 100644
--- a/h2o-algos/src/main/java/hex/grep/Grep.java
+++ b/h2o-algos/src/main/java/hex/grep/Grep.java
@@ -19,7 +19,6 @@ import java.util.regex.PatternSyntaxException;
 public class Grep extends ModelBuilder<GrepModel,GrepModel.GrepParameters,GrepModel.GrepOutput> {
   public Grep( GrepModel.GrepParameters parms ) { super(parms); init(false); }
   @Override protected GrepDriver trainModelImpl() { return new GrepDriver(); }
-  @Override public long progressUnits() { return _parms.train().numRows(); }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ModelCategory.Unknown}; }
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; };
 
diff --git a/h2o-algos/src/main/java/hex/grep/GrepModel.java b/h2o-algos/src/main/java/hex/grep/GrepModel.java
index 8689a27..7ee2ac0 100644
--- a/h2o-algos/src/main/java/hex/grep/GrepModel.java
+++ b/h2o-algos/src/main/java/hex/grep/GrepModel.java
@@ -12,6 +12,7 @@ public class GrepModel extends Model<GrepModel,GrepModel.GrepParameters,GrepMode
     public String algoName() { return "Grep"; }
     public String fullName() { return "Grep"; }
     public String javaName() { return GrepModel.class.getName(); }
+    @Override public long progressUnits() { return train() != null ? train().numRows() : 1; }
     public String _regex;       // The regex
   }
 
diff --git a/h2o-algos/src/main/java/hex/kmeans/KMeans.java b/h2o-algos/src/main/java/hex/kmeans/KMeans.java
index 5bc4719..d470478 100755
--- a/h2o-algos/src/main/java/hex/kmeans/KMeans.java
+++ b/h2o-algos/src/main/java/hex/kmeans/KMeans.java
@@ -28,7 +28,6 @@ public class KMeans extends ClusteringModelBuilder<KMeansModel,KMeansModel.KMean
   public enum Initialization { Random, PlusPlus, Furthest, User }
   /** Start the KMeans training Job on an F/J thread. */
   @Override protected KMeansDriver trainModelImpl() { return new KMeansDriver();  }
-  @Override public long progressUnits() { return _parms._max_iterations; }
 
   // Called from an http request
   public KMeans( KMeansModel.KMeansParameters parms         ) { super(parms    ); init(false); }
diff --git a/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java b/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java
index 6f54549..ec4a77d 100755
--- a/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java
+++ b/h2o-algos/src/main/java/hex/kmeans/KMeansModel.java
@@ -21,6 +21,7 @@ public class KMeansModel extends ClusteringModel<KMeansModel,KMeansModel.KMeansP
     public String algoName() { return "KMeans"; }
     public String fullName() { return "K-means"; }
     public String javaName() { return KMeansModel.class.getName(); }
+    @Override public long progressUnits() { return _max_iterations; }
     public int _max_iterations = 1000;     // Max iterations
     public boolean _standardize = true;    // Standardize columns
     public long _seed = System.nanoTime(); // RNG seed
diff --git a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java
index 911958c..e1db60c 100644
--- a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java
+++ b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayes.java
@@ -26,7 +26,6 @@ import java.util.List;
 public class NaiveBayes extends ModelBuilder<NaiveBayesModel,NaiveBayesParameters,NaiveBayesOutput> {
   public boolean isSupervised(){return true;}
   @Override protected NaiveBayesDriver trainModelImpl() { return new NaiveBayesDriver(); }
-  @Override public long progressUnits() { return 6; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ ModelCategory.Unknown }; }
 
   @Override
diff --git a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java
index 961d0b2..9f24b79 100644
--- a/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java
+++ b/h2o-algos/src/main/java/hex/naivebayes/NaiveBayesModel.java
@@ -27,6 +27,7 @@ public class NaiveBayesModel extends Model<NaiveBayesModel,NaiveBayesModel.Naive
     public String algoName() { return "NaiveBayes"; }
     public String fullName() { return "Naive Bayes"; }
     public String javaName() { return NaiveBayesModel.class.getName(); }
+    @Override public long progressUnits() { return 6; }
   }
 
   public static class NaiveBayesOutput extends Model.Output {
diff --git a/h2o-algos/src/main/java/hex/pca/PCA.java b/h2o-algos/src/main/java/hex/pca/PCA.java
index 33f398c..c76c7eb 100755
--- a/h2o-algos/src/main/java/hex/pca/PCA.java
+++ b/h2o-algos/src/main/java/hex/pca/PCA.java
@@ -33,7 +33,6 @@ public class PCA extends ModelBuilder<PCAModel,PCAModel.PCAParameters,PCAModel.P
   // Number of columns in training set (p)
   private transient int _ncolExp;    // With categoricals expanded into 0/1 indicator cols
   @Override protected PCADriver trainModelImpl() { return new PCADriver(); }
-  @Override public long progressUnits() { return _parms._pca_method == PCAParameters.Method.GramSVD ? 5 : 3; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ ModelCategory.Clustering }; }
 
   @Override protected void checkMemoryFootPrint() {
diff --git a/h2o-algos/src/main/java/hex/pca/PCAModel.java b/h2o-algos/src/main/java/hex/pca/PCAModel.java
index 42fc03c..e338d03 100755
--- a/h2o-algos/src/main/java/hex/pca/PCAModel.java
+++ b/h2o-algos/src/main/java/hex/pca/PCAModel.java
@@ -22,6 +22,8 @@ public class PCAModel extends Model<PCAModel,PCAModel.PCAParameters,PCAModel.PCA
     public String algoName() { return "PCA"; }
     public String fullName() { return "Principle Components Analysis"; }
     public String javaName() { return PCAModel.class.getName(); }
+    @Override public long progressUnits() { return _pca_method == PCAParameters.Method.GramSVD ? 5 : 3; }
+
     public DataInfo.TransformType _transform = DataInfo.TransformType.NONE; // Data transformation
     public Method _pca_method = Method.GramSVD;   // Method for computing PCA
     public int _k = 1;                     // Number of principal components
diff --git a/h2o-algos/src/main/java/hex/schemas/DRFV3.java b/h2o-algos/src/main/java/hex/schemas/DRFV3.java
index 2b27d02..110d8b9 100644
--- a/h2o-algos/src/main/java/hex/schemas/DRFV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/DRFV3.java
@@ -14,6 +14,7 @@ public class DRFV3 extends SharedTreeV3<DRF,DRFV3, DRFV3.DRFParametersV3> {
         "nfolds",
         "keep_cross_validation_predictions",
         "score_each_iteration",
+        "score_tree_interval",
         "fold_assignment",
         "fold_column",
 				"response_column",
diff --git a/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java b/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java
index 6b4d03e..b8d6715 100755
--- a/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/DeepLearningV3.java
@@ -59,6 +59,7 @@ public class DeepLearningV3 extends ModelBuilderSchema<DeepLearning,DeepLearning
         "initial_weight_scale",
         "loss",
         "distribution",
+        "quantile_alpha",
         "tweedie_power",
         "score_interval",
         "score_training_samples",
@@ -401,15 +402,18 @@ public class DeepLearningV3 extends ModelBuilderSchema<DeepLearning,DeepLearning
      * be used for classification as well (where it emphasizes the error on all
      * output classes, not just for the actual class).
      */
-    @API(help = "Loss function", values = { "Automatic", "CrossEntropy", "Quadratic", "Huber", "Absolute" }, required = false, level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
+    @API(help = "Loss function", values = { "Automatic", "CrossEntropy", "Quadratic", "Huber", "Absolute", "Quantile" }, required = false, level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
     public DeepLearningParameters.Loss loss;
 
-    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "huber" }, level = API.Level.secondary, gridable = true)
+    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "huber", "quantile" }, level = API.Level.secondary, gridable = true)
     public Distribution.Family distribution;
 
-    @API(help = "Tweedie Power", level = API.Level.secondary)
+    @API(help = "Tweedie Power", level = API.Level.secondary, gridable = true)
     public double tweedie_power;
 
+    @API(help="Desired quantile for quantile regression (from 0.0 to 1.0)", level = API.Level.secondary, gridable = true)
+    public double quantile_alpha;
+
     /*Scoring*/
     /**
      * The minimum time (in seconds) to elapse between model scoring. The actual
diff --git a/h2o-algos/src/main/java/hex/schemas/GBMV3.java b/h2o-algos/src/main/java/hex/schemas/GBMV3.java
index 28c8abe..8ec5b44 100755
--- a/h2o-algos/src/main/java/hex/schemas/GBMV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/GBMV3.java
@@ -16,6 +16,7 @@ public class GBMV3 extends SharedTreeV3<GBM,GBMV3,GBMV3.GBMParametersV3> {
         "nfolds",
         "keep_cross_validation_predictions",
         "score_each_iteration",
+        "score_tree_interval",
         "fold_assignment",
         "fold_column",
 				"response_column",
@@ -43,6 +44,7 @@ public class GBMV3 extends SharedTreeV3<GBM,GBMV3,GBMV3.GBMParametersV3> {
 				"build_tree_one_node",
         "learn_rate",
         "distribution",
+        "quantile_alpha",
         "tweedie_power",
         "checkpoint",
         "sample_rate",
@@ -54,9 +56,12 @@ public class GBMV3 extends SharedTreeV3<GBM,GBMV3,GBMV3.GBMParametersV3> {
     @API(help="Learning rate (from 0.0 to 1.0)", gridable = true)
     public float learn_rate;
 
-    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace" }, gridable = true)
+    @API(help = "Distribution function", values = { "AUTO", "bernoulli", "multinomial", "gaussian", "poisson", "gamma", "tweedie", "laplace", "quantile" }, gridable = true)
     public Distribution.Family distribution;
 
+    @API(help="Desired quantile for quantile regression (from 0.0 to 1.0)", level = API.Level.secondary, gridable = true)
+    public double quantile_alpha;
+
     @API(help = "Tweedie Power (between 1 and 2)", level = API.Level.secondary, gridable = true)
     public double tweedie_power;
 
diff --git a/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java b/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java
index 4be7885..d42e640 100755
--- a/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java
+++ b/h2o-algos/src/main/java/hex/schemas/SharedTreeV3.java
@@ -75,5 +75,8 @@ public class SharedTreeV3<B extends SharedTree, S extends SharedTreeV3<B,S,P>, P
 
     @API(help = "Column sample rate per tree (from 0.0 to 1.0)", gridable = true)
     public float col_sample_rate_per_tree;
+
+    @API(help="Score the model after every so many trees. Disabled if set to 0.", level = API.Level.secondary, gridable = false)
+    public int score_tree_interval;
   }
 }
diff --git a/h2o-algos/src/main/java/hex/svd/SVD.java b/h2o-algos/src/main/java/hex/svd/SVD.java
index a053fbe..51647db 100644
--- a/h2o-algos/src/main/java/hex/svd/SVD.java
+++ b/h2o-algos/src/main/java/hex/svd/SVD.java
@@ -41,14 +41,6 @@ public class SVD extends ModelBuilder<SVDModel,SVDModel.SVDParameters,SVDModel.S
   private transient int _ncolExp;    // With categoricals expanded into 0/1 indicator cols
 
   @Override protected SVDDriver trainModelImpl() { return new SVDDriver(); }
-  @Override public long progressUnits() {
-    switch(_parms._svd_method) {
-    case GramSVD:    return 2;
-    case Power:      return 1 + _parms._nv;
-    case Randomized: return 5 + _parms._max_iterations;
-    default:         return _parms._nv;
-    }
-  }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ ModelCategory.DimReduction }; }
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; }
 
diff --git a/h2o-algos/src/main/java/hex/svd/SVDModel.java b/h2o-algos/src/main/java/hex/svd/SVDModel.java
index e58ae0a..c40f4b4 100644
--- a/h2o-algos/src/main/java/hex/svd/SVDModel.java
+++ b/h2o-algos/src/main/java/hex/svd/SVDModel.java
@@ -17,6 +17,14 @@ public class SVDModel extends Model<SVDModel,SVDModel.SVDParameters,SVDModel.SVD
     public String algoName() { return "SVD"; }
     public String fullName() { return "Singular Value Decomposition"; }
     public String javaName() { return SVDModel.class.getName(); }
+    @Override public long progressUnits() {
+      switch(_svd_method) {
+        case GramSVD:    return 2;
+        case Power:      return 1 + _nv;
+        case Randomized: return 5 + _max_iterations;
+        default:         return _nv;
+      }
+    }
     public DataInfo.TransformType _transform = DataInfo.TransformType.NONE; // Data transformation (demean to compare with PCA)
     public Method _svd_method = Method.GramSVD;   // Method for computing SVD
     public int _nv = 1;    // Number of right singular vectors to calculate
diff --git a/h2o-algos/src/main/java/hex/tree/SharedTree.java b/h2o-algos/src/main/java/hex/tree/SharedTree.java
index 4ac95f8..677dafe 100755
--- a/h2o-algos/src/main/java/hex/tree/SharedTree.java
+++ b/h2o-algos/src/main/java/hex/tree/SharedTree.java
@@ -52,8 +52,6 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
 
   public boolean isSupervised(){return true;}
 
-  @Override public long progressUnits() { return _parms._ntrees; }
-
   @Override protected boolean computePriorClassDistribution(){ return true;}
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
@@ -108,6 +106,7 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
     if (_parms._nbins_top_level >= 1<<16) error ("_nbins_top_level", "nbins_top_level must be < " + (1<<16));
     if (_parms._max_depth <= 0) error ("_max_depth", "_max_depth must be > 0.");
     if (_parms._min_rows <=0) error ("_min_rows", "_min_rows must be > 0.");
+    if (_parms._score_tree_interval < 0 || _parms._score_tree_interval > _parms._ntrees) error ("_score_tree_interval", "_score_tree_interval must be >= 0 and <= _ntrees.");
     if (!(0.0 < _parms._sample_rate && _parms._sample_rate <= 1.0))
       error("_sample_rate", "sample_rate should be in interval ]0,1] but it is " + _parms._sample_rate);
     if (!(0.0 < _parms._col_sample_rate_per_tree && _parms._col_sample_rate_per_tree <= 1.0))
@@ -467,14 +466,18 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
     long sinceLastScore = now-_timeLastScoreStart;
     boolean updated = false;
     _job.update(0,"Built " + _model._output._ntrees + " trees so far (out of " + _parms._ntrees + ").");
-    // Now model already contains tid-trees in serialized form
-    if( _parms._score_each_iteration ||
-            finalScoring ||
-            (now-_firstScore < _parms._initial_score_interval) || // Score every time for 4 secs
-            // Throttle scoring to keep the cost sane; limit to a 10% duty cycle & every 4 secs
-            (sinceLastScore > _parms._score_interval && // Limit scoring updates to every 4sec
-                    (double)(_timeLastScoreEnd-_timeLastScoreStart)/sinceLastScore < 0.1) ) { // 10% duty cycle
 
+    boolean timeToScore = (now-_firstScore < _parms._initial_score_interval) || // Score every time for 4 secs
+        // Throttle scoring to keep the cost sane; limit to a 10% duty cycle & every 4 secs
+        (sinceLastScore > _parms._score_interval && // Limit scoring updates to every 4sec
+            (double)(_timeLastScoreEnd-_timeLastScoreStart)/sinceLastScore < 0.1); //10% duty cycle
+
+    boolean manualInterval = _parms._score_tree_interval > 0 && _model._output._ntrees % _parms._score_tree_interval == 0;
+
+    // Now model already contains tid-trees in serialized form
+    if( _parms._score_each_iteration || finalScoring || // always score under these circumstances
+        (timeToScore && _parms._score_tree_interval == 0) || // use time-based duty-cycle heuristic only if the user didn't specify _score_tree_interval
+        manualInterval) {
       checkMemoryFootPrint();
 
       // If validation is specified we use a model for scoring, so we need to
@@ -738,7 +741,7 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
    * @return initial value
    */
   protected double getInitialValue() {
-    return new InitialValue(_parms._distribution, _parms._tweedie_power).doAll(
+    return new InitialValue(_parms).doAll(
             _response,
             hasWeightCol() ? _weights : _response.makeCon(1),
             hasOffsetCol() ? _offset : _response.makeCon(0)
@@ -747,7 +750,7 @@ public abstract class SharedTree<M extends SharedTreeModel<M,P,O>, P extends Sha
 
   // Helper MRTask to compute the initial value
   private static class InitialValue extends MRTask<InitialValue> {
-    public  InitialValue(Distribution.Family family, double power) { _dist = new Distribution(family, power); }
+    public  InitialValue(Model.Parameters parms) { _dist = new Distribution(parms); }
     final private Distribution _dist;
     private double _num;
     private double _denom;
diff --git a/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java b/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java
index baf2d37..6e0bc17 100755
--- a/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java
+++ b/h2o-algos/src/main/java/hex/tree/SharedTreeModel.java
@@ -34,13 +34,17 @@ public abstract class SharedTreeModel<M extends SharedTreeModel<M,P,O>, P extend
 
     public boolean _build_tree_one_node = false;
 
+    public int _score_tree_interval = 0; // score every so many trees (no matter what)
+
     public int _initial_score_interval = 4000; //Adding this parameter to take away the hard coded value of 4000 for scoring the first  4 secs
 
     public int _score_interval = 4000; //Adding this parameter to take away the hard coded value of 4000 for scoring each iteration every 4 secs
 
     public float _sample_rate = 0.632f; //fraction of rows to sample for each tree
 
-    @Override protected long nFoldSeed() { 
+    @Override public long progressUnits() { return _ntrees; }
+
+    @Override protected long nFoldSeed() {
       return _seed == -1 ? (_seed = RandomUtils.getRNG(System.nanoTime()).nextLong()) : _seed;
     }
 
@@ -82,7 +86,7 @@ public abstract class SharedTreeModel<M extends SharedTreeModel<M,P,O>, P extend
 
   @Override
   public double deviance(double w, double y, double f) {
-    return new Distribution(_parms._distribution, _parms._tweedie_power).deviance(w, y, f);
+    return new Distribution(_parms).deviance(w, y, f);
   }
 
   @Override public ModelMetrics.MetricBuilder makeMetricBuilder(String[] domain) {
diff --git a/h2o-algos/src/main/java/hex/tree/gbm/GBM.java b/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
index 8ab3c4d..1c9386d 100755
--- a/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
+++ b/h2o-algos/src/main/java/hex/tree/gbm/GBM.java
@@ -4,7 +4,6 @@ import hex.Distribution;
 import hex.ModelCategory;
 import hex.quantile.Quantile;
 import hex.quantile.QuantileModel;
-import hex.schemas.GBMV3;
 import hex.tree.*;
 import hex.tree.DTree.DecidedNode;
 import hex.tree.DTree.LeafNode;
@@ -120,6 +119,9 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
     case laplace:
       if (isClassifier()) error("_distribution", H2O.technote(2, "Laplace requires the response to be numeric."));
       break;
+    case quantile:
+      if (isClassifier()) error("_distribution", H2O.technote(2, "Quantile requires the response to be numeric."));
+      break;
     case AUTO:
       break;
     default:
@@ -142,11 +144,13 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
       if (!(1 <= _mtry && _mtry <= _ncols)) throw new IllegalArgumentException("Computed mtry should be in interval <1,"+_ncols+"> but it is " + _mtry);
 
       // for Bernoulli, we compute the initial value with Newton-Raphson iteration, otherwise it might be NaN here
-      _initialPrediction = _nclass > 2 || _parms._distribution == Distribution.Family.laplace ? 0 : getInitialValue();
+      _initialPrediction = _nclass > 2 || _parms._distribution == Distribution.Family.laplace || _parms._distribution == Distribution.Family.quantile ? 0 : getInitialValue();
       if (_parms._distribution == Distribution.Family.bernoulli) {
         if (hasOffsetCol()) _initialPrediction = getInitialValueBernoulliOffset(_train);
       } else if (_parms._distribution == Distribution.Family.laplace) {
         _initialPrediction = getInitialValueQuantile(0.5);
+      } else if (_parms._distribution == Distribution.Family.quantile) {
+        _initialPrediction = getInitialValueQuantile(_parms._quantile_alpha);
       }
       _model._output._init_f = _initialPrediction; //always write the initial value here (not just for Bernoulli)
 
@@ -243,7 +247,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
         Chunk ys = chk_resp(chks);
         Chunk offset = chk_offset(chks);
         Chunk weight = hasWeightCol() ? chk_weight(chks) : new C0DChunk(1, chks[0]._len);
-        Distribution dist = new Distribution(Distribution.Family.bernoulli);
+        Distribution dist = new Distribution(_parms);
         for( int row = 0; row < ys._len; row++) {
           double w = weight.atd(row);
           if (w == 0) continue;
@@ -273,7 +277,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
         Chunk preds = chk_tree(chks, 0); // Prior tree sums
         Chunk wk = chk_work(chks, 0); // Place to store residuals
         double fs[] = _nclass > 1 ? new double[_nclass+1] : null;
-        Distribution dist = new Distribution(_parms._distribution, _parms._tweedie_power);
+        Distribution dist = new Distribution(_parms);
         for( int row = 0; row < wk._len; row++) {
           if( ys.isNA(row) ) continue;
           double f = preds.atd(row) + offset.atd(row);
@@ -412,6 +416,8 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
       GammaPass gp = new GammaPass(ktrees, leafs, _parms._distribution).doAll(_train);
       if (_parms._distribution == Distribution.Family.laplace) {
         fitBestConstantsQuantile(ktrees, leafs, 0.5); //special case for Laplace: compute the median for each leaf node and store that as prediction
+      } else if (_parms._distribution == Distribution.Family.quantile) {
+        fitBestConstantsQuantile(ktrees, leafs, _parms._quantile_alpha); //compute the alpha-quantile for each leaf node and store that as prediction
       } else {
         fitBestConstants(ktrees, leafs, gp);
       }
@@ -521,11 +527,11 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
       H2O.submitTask(sqt);
       sqt.join();
 
-      for (int i = 0; i < ktrees[0]._len - leafs[0]; i++) {
+      for (int i = 0; i < sqt._quantiles.length; i++) {
         float val = (float) (_parms._learn_rate * sqt._quantiles[i]);
         assert !Float.isNaN(val) && !Float.isInfinite(val);
-        ((LeafNode) ktrees[0].node(leafs[0] + i))._pred = val;
-//        Log.info("Leaf " + (leafs[0]+i) + " has median: " + sqt._quantiles[i]);
+        ((LeafNode) ktrees[0].node((int)strata.min() + i))._pred = val;
+//        Log.info("Leaf " + ((int)strata.min()+i) + " has median: " + sqt._quantiles[i]);
       }
     }
 
@@ -574,7 +580,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
                 || _dist == Distribution.Family.gamma
                 || _dist == Distribution.Family.tweedie)
         {
-          return new Distribution(_dist, _parms._tweedie_power).link(g);
+          return new Distribution(_parms).link(g);
         } else {
           return g;
         }
@@ -610,7 +616,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
           // If we have all constant responses, then we do not split even the
           // root and the residuals should be zero.
           if( tree.root() instanceof LeafNode ) continue;
-          Distribution dist = new Distribution(_parms._distribution, _parms._tweedie_power);
+          Distribution dist = new Distribution(_parms);
           for( int row=0; row<nids._len; row++ ) { // For all rows
             int nid = (int)nids.at8(row);          // Get Node to decide from
 
@@ -635,8 +641,8 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
             assert !ress.isNA(row);
 
             // OOB rows get placed properly (above), but they don't affect the computed Gamma (below)
-            // For Laplace distribution, we need to compute the median of (y-offset-preds == y-f), will be done outside of here
-            if (wasOOBRow || _parms._distribution == Distribution.Family.laplace) continue;
+            // For Laplace/Quantile distribution, we need to compute the median of (y-offset-preds == y-f), will be done outside of here
+            if (wasOOBRow || _parms._distribution == Distribution.Family.laplace || _parms._distribution == Distribution.Family.quantile) continue;
 
             // Compute numerator and denominator of terminal node estimate (gamma)
             double w = hasWeightCol() ? chk_weight(chks).atd(row) : 1; //weight
@@ -688,7 +694,7 @@ public class GBM extends SharedTree<GBMModel,GBMModel.GBMParameters,GBMModel.GBM
   // turns the results into a probability distribution.
   @Override protected double score1( Chunk chks[], double weight, double offset, double fs[/*nclass*/], int row ) {
     double f = chk_tree(chks,0).atd(row) + offset;
-    double p = new Distribution(_parms._distribution, _parms._tweedie_power).linkInv(f);
+    double p = new Distribution(_parms).linkInv(f);
     if( _parms._distribution == Distribution.Family.bernoulli ) {
       fs[2] = p;
       fs[1] = 1.0-p;
diff --git a/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java b/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java
index b61bdcb..88007f4 100755
--- a/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java
+++ b/h2o-algos/src/main/java/hex/tree/gbm/GBMModel.java
@@ -39,7 +39,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
     super.score0(data, preds, weight, offset);    // These are f_k(x) in Algorithm 10.4
     if (_parms._distribution == Distribution.Family.bernoulli) {
       double f = preds[1] + _output._init_f + offset; //Note: class 1 probability stored in preds[1] (since we have only one tree)
-      preds[2] = new Distribution(Distribution.Family.bernoulli).linkInv(f);
+      preds[2] = new Distribution(_parms).linkInv(f);
       preds[1] = 1.0 - preds[2];
     } else if (_parms._distribution == Distribution.Family.multinomial) { // Kept the initial prediction for binomial
       if (_output.nclasses() == 2) { //1-tree optimization for binomial
@@ -49,7 +49,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
       hex.genmodel.GenModel.GBM_rescale(preds);
     } else { //Regression
       double f = preds[0] + _output._init_f + offset;
-      preds[0] = new Distribution(_parms._distribution, _parms._tweedie_power).linkInv(f);
+      preds[0] = new Distribution(_parms).linkInv(f);
     }
     return preds;
   }
@@ -60,7 +60,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
     // the loss function.
     if( _parms._distribution == Distribution.Family.bernoulli ) {
       body.ip("preds[2] = preds[1] + ").p(_output._init_f).p(";").nl();
-      body.ip("preds[2] = " + new Distribution(_parms._distribution).linkInvString("preds[2]") + ";").nl();
+      body.ip("preds[2] = " + new Distribution(_parms).linkInvString("preds[2]") + ";").nl();
       body.ip("preds[1] = 1.0-preds[2];").nl();
       if (_parms._balance_classes)
         body.ip("hex.genmodel.GenModel.correctProbabilities(preds, PRIOR_CLASS_DISTRIB, MODEL_CLASS_DISTRIB);").nl();
@@ -69,7 +69,7 @@ public class GBMModel extends SharedTreeModel<GBMModel,GBMModel.GBMParameters,GB
     }
     if( _output.nclasses() == 1 ) { // Regression
       body.ip("preds[0] += ").p(_output._init_f).p(";").nl();
-      body.ip("preds[0] = " + new Distribution(_parms._distribution, _parms._tweedie_power).linkInvString("preds[0]") + ";").nl();
+      body.ip("preds[0] = " + new Distribution(_parms).linkInvString("preds[0]") + ";").nl();
       return;
     }
     if( _output.nclasses()==2 ) { // Kept the initial prediction for binomial
diff --git a/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java b/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java
index 334f46b..92c698f 100644
--- a/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java
+++ b/h2o-algos/src/main/java/hex/util/LinearAlgebraUtils.java
@@ -52,7 +52,7 @@ public class LinearAlgebraUtils {
       if (Double.isNaN(row[col])) {
         if (dinfo._imputeMissing)
           cidx = dinfo._catModes[col];
-        else if (dinfo._catMissing[col] == 0)
+        else if (!dinfo._catMissing[col])
           continue;   // Skip if entry missing and no NA bucket. All indicators will be zero.
         else
           cidx = dinfo._catOffsets[col+1]-1;  // Otherwise, missing value turns into extra (last) factor
@@ -171,7 +171,7 @@ public class LinearAlgebraUtils {
             if (Double.isNaN(a)) {
               if (_ainfo._imputeMissing)
                 cidx = _ainfo._catModes[p];
-              else if (_ainfo._catMissing[p] == 0)
+              else if (!_ainfo._catMissing[p])
                 continue;   // Skip if entry missing and no NA bucket. All indicators will be zero.
               else
                 cidx = _ainfo._catOffsets[p+1]-1;     // Otherwise, missing value turns into extra (last) factor
diff --git a/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java b/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java
index 8a62b33..be2d9b3 100644
--- a/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java
+++ b/h2o-algos/src/main/java/hex/word2vec/Word2Vec.java
@@ -14,7 +14,6 @@ public class Word2Vec extends ModelBuilder<Word2VecModel,Word2VecModel.Word2VecP
   public enum NormModel { HSM, NegSampling }
   public Word2Vec(Word2VecModel.Word2VecParameters parms) { super(parms); }
   @Override protected Word2VecDriver trainModelImpl() { return new Word2VecDriver(); }
-  @Override public long progressUnits() { return _parms._epochs; }
 
   /** Initialize the ModelBuilder, validating all arguments and preparing the
    *  training frame.  This call is expected to be overridden in the subclasses
diff --git a/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java b/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java
index acc497b..3e96ee6 100644
--- a/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java
+++ b/h2o-algos/src/main/java/hex/word2vec/Word2VecModel.java
@@ -223,6 +223,7 @@ public class Word2VecModel extends Model<Word2VecModel, Word2VecParameters, Word
     public String algoName() { return "Word2Vec"; }
     public String fullName() { return "Word2Vec"; }
     public String javaName() { return Word2VecModel.class.getName(); }
+    @Override public long progressUnits() { return _epochs; }
     static final int MAX_VEC_SIZE = 10000;
 
     public Word2Vec.WordModel _wordModel = Word2Vec.WordModel.SkipGram;
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java
index b244efd..063b31d 100644
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningGradientCheck.java
@@ -47,6 +47,7 @@ public class DeepLearningGradientCheck extends TestUtil {
       for (Distribution.Family dist : new Distribution.Family[]{
               Distribution.Family.gaussian,
               Distribution.Family.laplace,
+              Distribution.Family.quantile,
               Distribution.Family.huber,
               Distribution.Family.gamma,
               Distribution.Family.poisson,
@@ -90,6 +91,7 @@ public class DeepLearningGradientCheck extends TestUtil {
                 parms._activation = act;
                 parms._adaptive_rate = adaptive;
                 parms._rate = 1e-4;
+                parms._quantile_alpha = 0.2;
                 parms._momentum_start = 0.9;
                 parms._momentum_stable = 0.99;
                 DeepLearningModelInfo.gradientCheck = null;
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java
index 1b6c72c..eab9149 100644
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningMissingTest.java
@@ -37,13 +37,15 @@ public class DeepLearningMissingTest extends TestUtil {
     StringBuilder sb = new StringBuilder();
     for (DeepLearningParameters.MissingValuesHandling mvh :
             new DeepLearningParameters.MissingValuesHandling[]{
-            DeepLearningParameters.MissingValuesHandling.Skip,
-            DeepLearningParameters.MissingValuesHandling.MeanImputation })
+                    DeepLearningParameters.MissingValuesHandling.MeanImputation,
+                    DeepLearningParameters.MissingValuesHandling.Skip
+            })
     {
       double sumerr = 0;
       Map<Double,Double> map = new TreeMap<>();
-      for (double missing_fraction : new double[]{0, 0.1, 0.25, 0.5, 0.75, 0.99}) {
+      for (double missing_fraction : new double[]{0, 0.05, 0.1, 0.25, 0.5, 0.75, 0.9, 0.99}) {
 
+        double err=0;
         try {
           Scope.enter();
           NFSFileVec  nfs = NFSFileVec.make(find_test_file("smalldata/junit/weather.csv"));
@@ -75,8 +77,8 @@ public class DeepLearningMissingTest extends TestUtil {
           p._ignored_columns = new String[]{train._names[1],train._names[22]}; //only for weather data
           p._missing_values_handling = mvh;
           p._loss = DeepLearningParameters.Loss.Huber;
-          p._activation = DeepLearningParameters.Activation.Tanh;
-          p._hidden = new int[]{100,100};
+          p._activation = DeepLearningParameters.Activation.Rectifier;
+          p._hidden = new int[]{50,50};
           p._l1 = 1e-5;
           p._input_dropout_ratio = 0.2;
           p._epochs = 3;
@@ -97,16 +99,14 @@ public class DeepLearningMissingTest extends TestUtil {
           mymodel = dl.trainModel().get();
 
           // Extract the scoring on validation set from the model
-          double err = mymodel.loss();
+          err = mymodel.loss();
 
           Log.info("Missing " + missing_fraction * 100 + "% -> logloss: " + err);
-          map.put(missing_fraction, err);
-          sumerr += err;
-          Scope.exit();
         } catch(Throwable t) {
           t.printStackTrace();
-          throw new RuntimeException(t);
+          err = 100;
         } finally {
+          Scope.exit();
           // cleanup
           if (mymodel != null) {
             mymodel.delete();
@@ -115,6 +115,8 @@ public class DeepLearningMissingTest extends TestUtil {
           if (test != null) test.delete();
           if (data != null) data.delete();
         }
+        map.put(missing_fraction, err);
+        sumerr += err;
       }
       sb.append("\nMethod: ").append(mvh.toString()).append("\n");
       sb.append("missing fraction --> Error\n");
@@ -125,8 +127,8 @@ public class DeepLearningMissingTest extends TestUtil {
       sumErr.put(mvh, sumerr);
     }
     Log.info(sb.toString());
-    Assert.assertTrue(sumErr.get(DeepLearningParameters.MissingValuesHandling.Skip) > 4.9);
-    Assert.assertTrue(sumErr.get(DeepLearningParameters.MissingValuesHandling.MeanImputation) < 4.0);
+    Assert.assertEquals(501.37629982829094, sumErr.get(DeepLearningParameters.MissingValuesHandling.Skip), 1e-2);
+    Assert.assertEquals(sumErr.get(DeepLearningParameters.MissingValuesHandling.MeanImputation), 5.94659155607, 1e-7);
   }
 }
 
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java
index dce8b35..b686b84 100755
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningProstateTest.java
@@ -24,7 +24,7 @@ import java.util.Random;
 import static hex.ConfusionMatrix.buildCM;
 
 public class DeepLearningProstateTest extends TestUtil {
-  @BeforeClass() public static void setup() { stall_till_cloudsize(5); }
+  @BeforeClass() public static void setup() { stall_till_cloudsize(1); }
 
   @Test public void run() throws Exception { runFraction(0.00002f); }
 
diff --git a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java
index 618c6a6..7b61eab 100755
--- a/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java
+++ b/h2o-algos/src/test/java/hex/deeplearning/DeepLearningTest.java
@@ -705,10 +705,10 @@ public class DeepLearningTest extends TestUtil {
 
       pred = dl.score(parms.train());
       hex.ModelMetricsBinomial mm = hex.ModelMetricsBinomial.getFromDKV(dl, parms.train());
-      assertEquals(0.7222222222222222, mm.auc_obj()._auc, 1e-8);
+      assertEquals(0.7592592592592592, mm.auc_obj()._auc, 1e-8);
 
       double mse = dl._output._training_metrics.mse();
-      assertEquals(0.31599425403539766, mse, 1e-8); //Note: better results than non-shuffled
+      assertEquals(0.314813341867078, mse, 1e-8); //Note: better results than non-shuffled
 
 //      assertTrue(dl.testJavaScoring(tfr, fr2=dl.score(tfr, 1e-5)); //PUBDEV-1900
       dl.delete();
@@ -784,10 +784,10 @@ public class DeepLearningTest extends TestUtil {
 
       pred = dl.score(parms.train());
       hex.ModelMetricsBinomial mm = hex.ModelMetricsBinomial.getFromDKV(dl, parms.train());
-      assertEquals(0.7777777777777778, mm.auc_obj()._auc, 1e-8);
+      assertEquals(0.7592592592592592, mm.auc_obj()._auc, 1e-8);
 
       double mse = dl._output._training_metrics.mse();
-      assertEquals(0.32223485418125575, mse, 1e-8);
+      assertEquals(0.3116490253190556, mse, 1e-8);
 
 //      Assert.assertTrue(dl.testJavaScoring(tfr,fr2=dl.score(tfr),1e-5)); //PUBDEV-1900
       dl.delete();
@@ -1435,5 +1435,31 @@ public class DeepLearningTest extends TestUtil {
       if (dl2 != null) dl2.delete();
     }
   }
+  @Test
+  public void testCrossValidation() {
+    Frame tfr = null;
+    DeepLearningModel dl = null;
+
+    try {
+      tfr = parse_test_file("./smalldata/gbm_test/BostonHousing.csv");
+      DeepLearningParameters parms = new DeepLearningParameters();
+      parms._train = tfr._key;
+      parms._response_column = tfr.lastVecName();
+      parms._reproducible = true;
+      parms._hidden = new int[]{20,20};
+      parms._seed = 0xdecaf;
+      parms._nfolds = 4;
+
+      dl = new DeepLearning(parms).trainModel().get();
+
+      Assert.assertEquals(dl._output._training_metrics._MSE,12.892871729257042,1e-6);
+      Assert.assertEquals(dl._output._cross_validation_metrics._MSE,17.42844560821736,1e-6);
+
+    } finally {
+      if (tfr != null) tfr.delete();
+      if (dl != null) dl.deleteCrossValidationModels();
+      if (dl != null) dl.delete();
+    }
+  }
 }
 
diff --git a/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java b/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java
index c9989c3..42bee16 100644
--- a/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java
+++ b/h2o-algos/src/test/java/hex/glm/GLMBasicTestBinomial.java
@@ -13,6 +13,7 @@ import water.*;
 import water.exceptions.H2OModelBuilderIllegalArgumentException;
 import water.fvec.*;
 
+import java.util.Arrays;
 import java.util.HashMap;
 
 import static org.junit.Assert.assertEquals;
@@ -418,6 +419,7 @@ public class GLMBasicTestBinomial extends TestUtil {
     params._intercept = false;
     params._objective_epsilon = 0;
     params._gradient_epsilon = 1e-6;
+    params._missing_values_handling = MissingValuesHandling.Skip;
     params._max_iterations = 100; // not expected to reach max iterations here
     for(Solver s:new Solver[]{Solver.AUTO,Solver.IRLSM,Solver.L_BFGS /*, Solver.COORDINATE_DESCENT_NAIVE, Solver.COORDINATE_DESCENT*/}) {
       Frame scoreTrain = null, scoreTest = null;
@@ -440,7 +442,7 @@ public class GLMBasicTestBinomial extends TestUtil {
         // compare validation res dev matches R
         // sum(binomial()$dev.resids(y=test$CAPSULE,mu=p,wt=1))
         // [1]80.92923
-        assertEquals(80.92923, GLMTest.residualDevianceTest(model), CD? 1e-2:1e-4);
+        assertTrue(80.92923 >= GLMTest.residualDevianceTest(model) - 1e-2);
 //      compare validation null dev against R
 //      sum(binomial()$dev.resids(y=test$CAPSULE,mu=.5,wt=1))
 //      [1] 124.7665
@@ -1013,6 +1015,7 @@ public class GLMBasicTestBinomial extends TestUtil {
     params._standardize = false;
     params._train = _prostateTrain._key;
     params._compute_p_values = true;
+    params._missing_values_handling = MissingValuesHandling.Skip;
     params._lambda = new double[]{0};
     GLM job0 = null;
     try {
@@ -1062,6 +1065,7 @@ public class GLMBasicTestBinomial extends TestUtil {
       double[] zvals_expected = new double[]{-2.99223901, 1.24208800, -0.14610616, 0.04428674, -0.46826589, 2.24843259, 3.13779030, 1.44550154, 1.18227779, 2.71377864, -1.11887108, 4.67333842};
       double[] pvals_expected = new double[]{2.769394e-03, 2.142041e-01, 8.838376e-01, 9.646758e-01, 6.395945e-01, 2.454862e-02, 1.702266e-03, 1.483171e-01, 2.370955e-01, 6.652060e-03, 2.631951e-01, 2.963429e-06};
       String[] names_actual = model._output.coefficientNames();
+      System.out.println("names actual = " + Arrays.toString(names_actual));
       HashMap<String, Integer> coefMap = new HashMap<>();
       for (int i = 0; i < names_expected.length; ++i)
         coefMap.put(names_expected[i], i);
diff --git a/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java b/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java
index f123aa3..a1775e7 100644
--- a/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java
+++ b/h2o-algos/src/test/java/hex/glm/GLMBasicTestRegression.java
@@ -1,6 +1,7 @@
 package hex.glm;
 
 import hex.ModelMetricsRegressionGLM;
+import hex.deeplearning.DeepLearningModel;
 import hex.glm.GLMModel.GLMParameters;
 import hex.glm.GLMModel.GLMParameters.Family;
 import hex.glm.GLMModel.GLMParameters.Solver;
@@ -414,6 +415,7 @@ public class GLMBasicTestRegression extends TestUtil {
     parms._response_column = "Infections";
     parms._compute_p_values = true;
     parms._objective_epsilon = 0;
+    parms._missing_values_handling = DeepLearningModel.DeepLearningParameters.MissingValuesHandling.Skip;
 
     GLMModel model = null;
     try {
@@ -473,6 +475,7 @@ public class GLMBasicTestRegression extends TestUtil {
     parms._alpha = new double[]{0};
     parms._response_column = "Claims";
     parms._compute_p_values = true;
+    parms._missing_values_handling = DeepLearningModel.DeepLearningParameters.MissingValuesHandling.Skip;
 
     GLMModel model = null;
     try {
@@ -539,6 +542,7 @@ public class GLMBasicTestRegression extends TestUtil {
     params._train = _prostateTrain._key;
     params._compute_p_values = true;
     params._lambda = new double[]{0};
+    params._missing_values_handling = DeepLearningModel.DeepLearningParameters.MissingValuesHandling.Skip;
     try {
       params._solver = Solver.L_BFGS;
       new GLM(params).trainModel().get();
diff --git a/h2o-algos/src/test/java/hex/glm/GLMTest.java b/h2o-algos/src/test/java/hex/glm/GLMTest.java
index 1368412..deb1868 100644
--- a/h2o-algos/src/test/java/hex/glm/GLMTest.java
+++ b/h2o-algos/src/test/java/hex/glm/GLMTest.java
@@ -1397,9 +1397,11 @@ public class GLMTest  extends TestUtil {
       params._train = fr._key;
       params._lambda = new double[]{0};
       params._standardize = false;
+//      params._missing_values_handling = MissingValuesHandling.Skip;
       GLM glm = new GLM(params,glmkey("prostate_model"));
       model = glm.trainModel().get();
       HashMap<String, Double> coefs = model.coefficients();
+      System.out.println(coefs);
       for(int i = 0; i < cfs1.length; ++i)
         assertEquals(vals[i], coefs.get(cfs1[i]),1e-4);
       assertEquals(512.3, nullDeviance(model),1e-1);
diff --git a/h2o-algos/src/test/java/water/ModelSerializationTest.java b/h2o-algos/src/test/java/water/ModelSerializationTest.java
index 0729eba..8e8140c 100644
--- a/h2o-algos/src/test/java/water/ModelSerializationTest.java
+++ b/h2o-algos/src/test/java/water/ModelSerializationTest.java
@@ -185,6 +185,7 @@ public class ModelSerializationTest extends TestUtil {
       public String algoName() { return "Blah"; }
       public String fullName() { return "Blah"; }
       public String javaName() { return BlahModel.class.getName(); }
+      @Override public long progressUnits() { return 0; }
     }
     static class BlahOutput extends Model.Output {
       public BlahOutput(boolean hasWeights, boolean hasOffset, boolean hasFold) {
diff --git a/h2o-core/src/main/java/hex/Distribution.java b/h2o-core/src/main/java/hex/Distribution.java
index cd02b90..f4976a5 100644
--- a/h2o-core/src/main/java/hex/Distribution.java
+++ b/h2o-core/src/main/java/hex/Distribution.java
@@ -2,7 +2,6 @@ package hex;
 
 import water.H2O;
 import water.Iced;
-import water.util.Log;
 
 /**
  * Distribution functions to be used by ML Algos
@@ -13,34 +12,33 @@ public class Distribution extends Iced {
     AUTO,         //model-specific behavior
     bernoulli,    //binomial classification (nclasses == 2)
     multinomial,  //classification (nclasses >= 2)
-    gaussian, poisson, gamma, tweedie, huber, laplace //regression
+    gaussian, poisson, gamma, tweedie, huber, laplace, quantile //regression
   }
 
-  /**
-   * Short constructor for non-Tweedie distributions
-   * @param distribution
-   */
-  public Distribution(Family distribution) {
-    assert(distribution != Family.tweedie);
-    this.distribution = distribution;
-    this.tweediePower = 0;
+  // Default constructor for non-Tweedie and non-Quantile families
+  public Distribution(Family family) {
+    distribution = family;
+    assert(family != Family.tweedie);
+    assert(family != Family.quantile);
+    tweediePower = 1.5;
+    quantileAlpha = 0.5;
   }
 
   /**
-   * Constructor to be used for Tweedie (and if uncertain)
-   * @param distribution
-   * @param tweediePower Tweedie Power
+   * @param params
    */
-  public Distribution(Family distribution, double tweediePower) {
-    this.distribution = distribution;
+  public Distribution(Model.Parameters params) {
+    distribution = params._distribution;
+    tweediePower = params._tweedie_power;
+    quantileAlpha = params._quantile_alpha;
     assert(tweediePower >1 && tweediePower <2);
-    this.tweediePower = tweediePower;
   }
   static public double MIN_LOG = -19;
   static public double MAX = 1e19;
 
   public final Family distribution;
   public final double tweediePower; //tweedie power
+  public final double quantileAlpha; //for quantile regression
 
   // helper - sanitized exponential function
   public static double exp(double x) {
@@ -83,6 +81,8 @@ public class Distribution extends Iced {
         }
       case laplace:
         return w * Math.abs(y-f); // weighted absolute deviance == weighted absolute error
+      case quantile:
+        return y > f ? w*quantileAlpha*(y-f) : w*(1-quantileAlpha)*(f-y);
       case bernoulli:
         return -2 * w * (y * f - log(1 + exp(f)));
       case poisson:
@@ -123,6 +123,8 @@ public class Distribution extends Iced {
         }
       case laplace:
         return f > y ? -1 : 1;
+      case quantile:
+        return y > f ? quantileAlpha : quantileAlpha-1;
       default:
         throw H2O.unimpl();
     }
@@ -139,6 +141,7 @@ public class Distribution extends Iced {
       case gaussian:
       case huber:
       case laplace:
+      case quantile:
         return f;
       case bernoulli:
         return log(f/(1-f));
@@ -163,6 +166,7 @@ public class Distribution extends Iced {
       case gaussian:
       case huber:
       case laplace:
+      case quantile:
         return f;
       case bernoulli:
         return 1 / (1 + exp(-f));
@@ -187,6 +191,7 @@ public class Distribution extends Iced {
       case gaussian:
       case huber:
       case laplace:
+      case quantile:
         return f;
       case bernoulli:
         return "1/(1+" + expString("-" + f) + ")";
diff --git a/h2o-core/src/main/java/hex/Model.java b/h2o-core/src/main/java/hex/Model.java
index 42a6659..bed6a28 100755
--- a/h2o-core/src/main/java/hex/Model.java
+++ b/h2o-core/src/main/java/hex/Model.java
@@ -98,8 +98,10 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
     protected long nFoldSeed() { return new Random().nextLong(); }
     public FoldAssignmentScheme _fold_assignment = FoldAssignmentScheme.AUTO;
     public Distribution.Family _distribution = Distribution.Family.AUTO;
-    public double _tweedie_power = 1.5f;
+    public double _tweedie_power = 1.5;
+    public double _quantile_alpha = 0.5;
     protected double defaultStoppingTolerance() { return 1e-3; }
+    abstract public long progressUnits();
 
     // TODO: This field belongs in the front-end column-selection process and
     // NOT in the parameters - because this requires all model-builders to have
@@ -839,7 +841,6 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
       }
     }
     @Override public void reduce( BigScore bs ) { if(_mb != null)_mb.reduce(bs._mb); }
-
     @Override protected void postGlobal() { if(_mb != null)_mb.postGlobal(); }
   }
 
@@ -850,6 +851,7 @@ public abstract class Model<M extends Model<M,P,O>, P extends Model.Parameters,
   public double[] score0( Chunk chks[], int row_in_chunk, double[] tmp, double[] preds ) {
     return score0(chks, 1, 0, row_in_chunk, tmp, preds);
   }
+
   public double[] score0( Chunk chks[], double weight, double offset, int row_in_chunk, double[] tmp, double[] preds ) {
     assert(_output.nfeatures() == tmp.length);
     for( int i=0; i< tmp.length; i++ )
diff --git a/h2o-core/src/main/java/hex/ModelBuilder.java b/h2o-core/src/main/java/hex/ModelBuilder.java
index 2867124..ec1e686 100644
--- a/h2o-core/src/main/java/hex/ModelBuilder.java
+++ b/h2o-core/src/main/java/hex/ModelBuilder.java
@@ -161,7 +161,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
       throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(this);
     _start_time = System.currentTimeMillis();
     if( !nFoldCV() )
-      return _job.start(trainModelImpl(), progressUnits());
+      return _job.start(trainModelImpl(), _parms.progressUnits());
 
     // cross-validation needs to be forked off to allow continuous (non-blocking) progress bar
     return _job.start(new H2O.H2OCountedCompleter() {
@@ -170,7 +170,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
           computeCrossValidation();
           tryComplete();
         }
-      }, (1/*for all pre-fold work*/+nFoldWork()+1/*for all the post-fold work*/) * progressUnits());
+      }, (1/*for all pre-fold work*/+nFoldWork()+1/*for all the post-fold work*/) * _parms.progressUnits());
   }
 
   /** Train a model as part of a larger Job; the Job already exists and has started. */
@@ -186,7 +186,6 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
   /** Model-specific implementation of model training
    * @return A F/J Job, which, when executed, does the build.  F/J is NOT started.  */
   abstract protected Driver trainModelImpl();
-  abstract protected long progressUnits();
 
   /**
    * How many should be trained in parallel during N-fold cross-validation?
@@ -442,7 +441,7 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
     }
     mainModel._output._cross_validation_metrics = mbs[0].makeModelMetrics(mainModel, _parms.train(), null, preds);
     if (preds!=null) preds.remove();
-    mainModel._output._cross_validation_metrics._description = N + "-fold cross-validation on training data";
+    mainModel._output._cross_validation_metrics._description = N + "-fold cross-validation on training data (Metrics computed for combined holdout predictions)";
     Log.info(mainModel._output._cross_validation_metrics.toString());
 
     // Now, the main model is complete (has cv metrics)
@@ -587,11 +586,11 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
           error("_response_column", "Response column '" + _parms._response_column + "' not found in the training frame");
       } else {
         if(_response == _offset)
-          error("_response", "Response must be different from offset_column");
+          error("_response_column", "Response column must be different from offset_column");
         if(_response == _weights)
-          error("_response", "Response must be different from weights_column");
+          error("_response_column", "Response column must be different from weights_column");
         if(_response == _fold)
-          error("_response", "Response must be different from fold_column");
+          error("_response_column", "Response column must be different from fold_column");
         _train.add(_parms._response_column, _response);
         ++res;
       }
@@ -926,6 +925,12 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
   }
 
   public void checkDistributions() {
+    if (_parms._distribution != Distribution.Family.tweedie) {
+      hide("_tweedie_power", "Tweedie power is only used for Tweedie distribution.");
+    }
+    if (_parms._distribution != Distribution.Family.quantile) {
+      hide("_quantile_alpha", "Quantile (alpha) is only used for Quantile regression.");
+    }
     if (_parms._distribution == Distribution.Family.poisson) {
       if (_response.min() < 0)
         error("_response", "Response must be non-negative for Poisson distribution.");
@@ -937,6 +942,9 @@ abstract public class ModelBuilder<M extends Model<M,P,O>, P extends Model.Param
         error("_tweedie_power", "Tweedie power must be between 1 and 2.");
       if (_response.min() < 0)
         error("_response", "Response must be non-negative for Tweedie distribution.");
+    } else if (_parms._distribution == Distribution.Family.quantile) {
+      if (_parms._quantile_alpha > 1 || _parms._quantile_alpha < 0)
+        error("_quantile_alpha", "Quantile (alpha) must be between 0 and 1.");
     }
   }
 
diff --git a/h2o-core/src/main/java/hex/grid/GridSearch.java b/h2o-core/src/main/java/hex/grid/GridSearch.java
index 330b767..661c82c 100644
--- a/h2o-core/src/main/java/hex/grid/GridSearch.java
+++ b/h2o-core/src/main/java/hex/grid/GridSearch.java
@@ -68,13 +68,12 @@ import java.util.Map;
  * @see #startGridSearch(Key, HyperSpaceWalker)
  */
 public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSearch> {
-  public enum Strategy { Unknown, Cartesian, Random } // search strategy
   public final Key<Grid> _result;
   public final Job<Grid> _job;
 
   /** Walks hyper space and for each point produces model parameters. It is
    *  used only locally to fire new model builders.  */
-  private final transient HyperSpaceWalker<MP> _hyperSpaceWalker;
+  private final transient HyperSpaceWalker<MP, ?> _hyperSpaceWalker;
 
   /** For advanced search methods we can put a time limit on the overall grid search.  This doesn't make much sense
    * for strict Cartesian.
@@ -82,16 +81,12 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
   private int _max_time_ms = Integer.MAX_VALUE;
 
 
-  private GridSearch(Key<Grid> gkey, HyperSpaceWalker<MP> hyperSpaceWalker) {
+  private GridSearch(Key<Grid> gkey, HyperSpaceWalker<MP, ?> hyperSpaceWalker) {
     _result = gkey;
     String algoName = hyperSpaceWalker.getParams().algoName();
     _job = new Job<>(gkey, Grid.class.getName(), algoName + " Grid Search");
     assert hyperSpaceWalker != null : "Grid search needs to know to how walk around hyper space!";
     _hyperSpaceWalker = hyperSpaceWalker;
-    // TODO: hacky: when we have SearchCriteria classes pass an instance down through the startGridSearch chain into this constructor.
-    if (_hyperSpaceWalker instanceof HyperSpaceWalker.RandomDiscreteValueWalker) {
-      this._max_time_ms = ((HyperSpaceWalker.RandomDiscreteValueWalker)_hyperSpaceWalker).max_time_ms();
-    }
     // Note: do not validate parameters of created model builders here!
     // Leave it to launch time, and just mark the corresponding model builder job as failed.
   }
@@ -122,13 +117,31 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
                      _hyperSpaceWalker.getParametersBuilderFactory().getFieldNamingStrategy());
       grid.delete_and_lock(_job);
     }
+
+    Model model = null;
+    HyperSpaceWalker.HyperSpaceIterator<MP> it = _hyperSpaceWalker.iterator();
+    long gridWork=0;
+    if (gridSize > 0) {//if total grid space is known, walk it all and count up models to be built (not subject to time-based or converge-based early stopping)
+      while (it.hasNext(model)) {
+        try {
+          gridWork += it.nextModelParameters(model).progressUnits();
+        } catch(Throwable ex) {
+          //swallow invalid combinations
+        }
+      }
+    } else {
+      //TODO: Future totally unbounded search: need a time-based progress bar
+      gridWork = Long.MAX_VALUE;
+    }
+    it.reset();
+
     // Install this as job functions
     return _job.start(new H2O.H2OCountedCompleter() {
       @Override public void compute2() {
         gridSearch(grid);
         tryComplete();
       }
-    }, gridSize);
+    }, gridWork);
   }
 
   /**
@@ -335,22 +348,9 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
       final MP params,
       final Map<String, Object[]> hyperParams,
       final ModelParametersBuilderFactory<MP> paramsBuilderFactory,
-      final Strategy strategy,
-      final int max_models,
-      final int max_time_ms,
-      long seed) {
-
-    // Create a walker to traverse the hyper space of model parameters.
-    // TODO: encapsulate this switch in a factory to make it pluggable.
-    BaseWalker<MP> hyperSpaceWalker;
-    if (strategy == Strategy.Cartesian)
-      hyperSpaceWalker = new HyperSpaceWalker.CartesianWalker<>(params, hyperParams, paramsBuilderFactory);
-    else if (strategy == Strategy.Random)
-      hyperSpaceWalker = new HyperSpaceWalker.RandomDiscreteValueWalker<>(params, hyperParams, paramsBuilderFactory, max_models, max_time_ms, seed);
-    else
-      throw new H2OIllegalArgumentException("strategy", "GridSearch", strategy);
-
-    return startGridSearch(destKey, hyperSpaceWalker);
+      final HyperSpaceSearchCriteria search_criteria) {
+
+    return startGridSearch(destKey, BaseWalker.WalkerFactory.create(params, hyperParams, paramsBuilderFactory, search_criteria));
   }
 
 
@@ -369,13 +369,17 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
    * an expensive operation.  If the models in question are "in progress", a 2nd build will NOT be
    * kicked off.  This is a non-blocking call.
    *
-   * @see #startGridSearch(Key, Model.Parameters, Map, ModelParametersBuilderFactory, Strategy, int, int, long)
+   * @see #startGridSearch(Key, Model.Parameters, Map, ModelParametersBuilderFactory, HyperSpaceSearchCriteria)
    */
   public static <MP extends Model.Parameters> Job<Grid> startGridSearch(final Key<Grid> destKey,
                                                                         final MP params,
                                                                         final Map<String, Object[]> hyperParams) {
-    return startGridSearch(destKey, params, hyperParams, new SimpleParametersBuilderFactory<MP>(),
-            Strategy.Cartesian, -1, -1, -1L);
+    return startGridSearch(
+            destKey,
+            params,
+            hyperParams,
+            new SimpleParametersBuilderFactory<MP>(),
+            new HyperSpaceSearchCriteria.CartesianSearchCriteria());
   }
 
   /**
@@ -391,7 +395,7 @@ public final class GridSearch<MP extends Model.Parameters> extends Keyed<GridSea
    */
   public static <MP extends Model.Parameters> Job<Grid> startGridSearch(
       final Key<Grid> destKey,
-      final HyperSpaceWalker<MP> hyperSpaceWalker) {
+      final HyperSpaceWalker<MP, ?> hyperSpaceWalker) {
     // Compute key for destination object representing grid
     MP params = hyperSpaceWalker.getParams();
     Key<Grid> gridKey = destKey != null ? destKey
diff --git a/h2o-core/src/main/java/hex/grid/HyperSpaceSearchCriteria.java b/h2o-core/src/main/java/hex/grid/HyperSpaceSearchCriteria.java
new file mode 100644
index 0000000..812aa94
--- /dev/null
+++ b/h2o-core/src/main/java/hex/grid/HyperSpaceSearchCriteria.java
@@ -0,0 +1,49 @@
+package hex.grid;
+
+import water.Iced;
+
+/**
+ * Search criteria for a hyperparameter search including directives for how to search and
+ * when to stop the search.
+ */
+public class HyperSpaceSearchCriteria extends Iced {
+  public enum Strategy { Unknown, Cartesian, RandomDiscrete } // search strategy
+
+  public final Strategy _strategy;
+  public final Strategy strategy() { return _strategy; }
+
+// TODO: add a factory which accepts a Strategy and calls the right constructor
+
+  public HyperSpaceSearchCriteria(Strategy strategy) {
+    this._strategy = strategy;
+  }
+
+  /**
+   * Search criteria for an exhaustive Cartesian hyperparameter search.
+   */
+  public static final class CartesianSearchCriteria extends HyperSpaceSearchCriteria {
+    public CartesianSearchCriteria() {
+      super(Strategy.Cartesian);
+    }
+  }
+
+  /**
+   * Search criteria for a hyperparameter search including directives for how to search and
+   * when to stop the search.
+   */
+  public static final class RandomDiscreteValueSearchCriteria extends HyperSpaceSearchCriteria {
+    private long _seed = -1;
+
+    // stopping criteria:
+    private int _max_models = Integer.MAX_VALUE;
+    private int _max_time_ms = Integer.MAX_VALUE;
+
+    public long seed() { return _seed; }
+    public int max_models() { return _max_models; }
+    public int max_time_ms() { return _max_time_ms; }
+
+    public RandomDiscreteValueSearchCriteria() {
+      super(Strategy.RandomDiscrete);
+    }
+  }
+}
diff --git a/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java b/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
index 99c1055..01a88a5 100644
--- a/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
+++ b/h2o-core/src/main/java/hex/grid/HyperSpaceWalker.java
@@ -2,11 +2,12 @@ package hex.grid;
 
 import hex.Model;
 import hex.ModelParametersBuilderFactory;
+import water.exceptions.H2OIllegalArgumentException;
 import water.util.Log;
 
 import java.util.*;
 
-public interface HyperSpaceWalker<MP extends Model.Parameters> {
+public interface HyperSpaceWalker<MP extends Model.Parameters, C extends HyperSpaceSearchCriteria> {
 
   interface HyperSpaceIterator<MP extends Model.Parameters> {
     /**
@@ -35,6 +36,8 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
      */
     boolean hasNext(Model previousModel);
 
+    void reset();
+
     long timeRemaining();
 
     /**
@@ -51,7 +54,13 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
      * @return  array of "untyped" values representing configuration of grid parameters
      */
     Object[] getCurrentRawParameters();
-  }
+  } // interface HyperSpaceIterator
+
+  /**
+   * Search criteria for the hyperparameter search including directives for how to search and
+   * when to stop the search.
+   */
+  public C search_criteria();
 
   /**
    * Returns an iterator to traverse this hyper-space.
@@ -93,7 +102,18 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
    * values, where the String is a valid field name in the corresponding Model.Parameter, and the Object is
    * the field value (boxed as needed).
    */
-  abstract class BaseWalker<MP extends Model.Parameters> implements HyperSpaceWalker<MP> {
+  abstract class BaseWalker<MP extends Model.Parameters, C extends HyperSpaceSearchCriteria> implements HyperSpaceWalker<MP, C> {
+
+    /**
+     * @see #search_criteria()
+     */
+    final protected C _search_criteria;
+
+    /**
+     * Search criteria for the hyperparameter search including directives for how to search and
+     * when to stop the search.
+     */
+    public C search_criteria() { return _search_criteria; }
 
     /**
      * Parameters builder factory to create new instance of parameters.
@@ -123,18 +143,43 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
     final protected int _maxHyperSpaceSize;
 
     /**
+     * Java hackery so we can have a factory method on a class with type params.
+     */
+    public static class WalkerFactory<MP extends Model.Parameters, C extends HyperSpaceSearchCriteria> {
+      /**
+       * Factory method to create an instance based on the given HyperSpaceSearchCriteria instance.
+       */
+      public static <MP extends Model.Parameters, C extends HyperSpaceSearchCriteria>
+        HyperSpaceWalker create(MP params,
+                                              Map<String, Object[]> hyperParams,
+                                            ModelParametersBuilderFactory<MP> paramsBuilderFactory,
+                                            C search_criteria) {
+        HyperSpaceSearchCriteria.Strategy strategy = search_criteria.strategy();
+
+        if (strategy == HyperSpaceSearchCriteria.Strategy.Cartesian)
+          return new HyperSpaceWalker.CartesianWalker<>(params, hyperParams, paramsBuilderFactory, (HyperSpaceSearchCriteria.CartesianSearchCriteria) search_criteria);
+        else if (strategy == HyperSpaceSearchCriteria.Strategy.RandomDiscrete )
+          return new HyperSpaceWalker.RandomDiscreteValueWalker<>(params, hyperParams, paramsBuilderFactory, (HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria) search_criteria);
+        else
+          throw new H2OIllegalArgumentException("strategy", "GridSearch", strategy);
+      }
+    }
+
+    /**
      *
      * @param paramsBuilderFactory
      * @param hyperParams
      */
     public BaseWalker(MP params,
-                           Map<String, Object[]> hyperParams,
-                           ModelParametersBuilderFactory<MP> paramsBuilderFactory) {
+                      Map<String, Object[]> hyperParams,
+                      ModelParametersBuilderFactory<MP> paramsBuilderFactory,
+                      C search_criteria) {
       _params = params;
       _hyperParams = hyperParams;
       _paramsBuilderFactory = paramsBuilderFactory;
       _hyperParamNames = hyperParams.keySet().toArray(new String[0]);
       _maxHyperSpaceSize = computeMaxSizeOfHyperSpace();
+      _search_criteria = search_criteria;
     }
 
     @Override
@@ -198,12 +243,13 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
    * Hyperparameter space walker which visits each combination of hyperparameters in order.
    */
   public static class CartesianWalker<MP extends Model.Parameters>
-          extends BaseWalker<MP> {
+          extends BaseWalker<MP, HyperSpaceSearchCriteria.CartesianSearchCriteria> {
 
     public CartesianWalker(MP params,
-                      Map<String, Object[]> hyperParams,
-                      ModelParametersBuilderFactory<MP> paramsBuilderFactory) {
-      super(params, hyperParams, paramsBuilderFactory);
+                           Map<String, Object[]> hyperParams,
+                           ModelParametersBuilderFactory<MP> paramsBuilderFactory,
+                           HyperSpaceSearchCriteria.CartesianSearchCriteria search_criteria) {
+      super(params, hyperParams, paramsBuilderFactory, search_criteria);
     }
 
     @Override
@@ -245,6 +291,10 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
           return false;
         }
 
+        @Override public void reset() {
+          _currentHyperparamIndices = null;
+        }
+
         @Override
         public long timeRemaining() { return Long.MAX_VALUE; }
 
@@ -291,14 +341,9 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
    * given in explicit lists as they are with CartesianWalker.
    */
   public static class RandomDiscreteValueWalker<MP extends Model.Parameters>
-      extends BaseWalker<MP> {
+      extends BaseWalker<MP, HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria> {
     Random random;
 
-    // stopping criteria:
-    private int _max_models;
-    private int _max_time_ms;
-    public int max_time_ms() { return _max_time_ms; }
-
     /** All visited hyper params permutations, including the current one. */
     private List<int[]> _visitedPermutations = new ArrayList<>();
     private Set<Integer> _visitedPermutationHashes = new LinkedHashSet<>(); // for fast dupe lookup
@@ -306,13 +351,9 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
     public RandomDiscreteValueWalker(MP params,
                                      Map<String, Object[]> hyperParams,
                                      ModelParametersBuilderFactory<MP> paramsBuilderFactory,
-                                     int max_models,
-                                     int max_time_ms,
-                                     long seed) {
-      super(params, hyperParams, paramsBuilderFactory);
-      random = new Random(seed); // TODO: allow the user to set the seed
-      this._max_models = max_models;
-      this._max_time_ms = max_time_ms;
+                                     HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria search_criteria) {
+      super(params, hyperParams, paramsBuilderFactory, search_criteria);
+      random = new Random(search_criteria.seed());
     }
 
     @Override
@@ -346,8 +387,6 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
             MP commonModelParams = (MP) _params.clone();
             // Fill model parameters
             MP params = getModelParams(commonModelParams, hypers);
-            // We have another model parameters
-            Log.info("About to build model: " + _currentPermutationNum);
             return params;
           } else {
             throw new NoSuchElementException("No more elements to explore in hyper-space!");
@@ -357,12 +396,20 @@ public interface HyperSpaceWalker<MP extends Model.Parameters> {
         @Override
         public boolean hasNext(Model previousModel) {
           // _currentPermutationNum is 1-based
-          return _currentPermutationNum < _maxHyperSpaceSize && _currentPermutationNum < _max_models;
+          return _currentPermutationNum < _maxHyperSpaceSize && _currentPermutationNum < search_criteria().max_models();
+        }
+
+        @Override
+        public void reset() {
+          _currentPermutationNum = 0;
+          _currentHyperparamIndices = null;
+          _visitedPermutations.clear();
+          _visitedPermutationHashes.clear();
         }
 
         @Override
         public long timeRemaining() {
-          return _max_time_ms - (System.currentTimeMillis() - _start_time);
+          return search_criteria().max_time_ms() - (System.currentTimeMillis() - _start_time);
         }
 
         @Override
diff --git a/h2o-core/src/main/java/hex/quantile/Quantile.java b/h2o-core/src/main/java/hex/quantile/Quantile.java
index f49f39d..28b9c16 100755
--- a/h2o-core/src/main/java/hex/quantile/Quantile.java
+++ b/h2o-core/src/main/java/hex/quantile/Quantile.java
@@ -21,7 +21,6 @@ public class Quantile extends ModelBuilder<QuantileModel,QuantileModel.QuantileP
   public Quantile( QuantileModel.QuantileParameters parms ) { super(parms); init(false); }
   public Quantile( QuantileModel.QuantileParameters parms, Job job ) { super(parms, job); init(false); }
   @Override public Driver trainModelImpl() { return new QuantileDriver(); }
-  @Override public long progressUnits() { return train().numCols()*_parms._probs.length; }
   @Override public ModelCategory[] can_build() { return new ModelCategory[]{ModelCategory.Unknown}; }
   // any number of chunks is fine - don't rebalance - it's not worth it for a few passes over the data (at most)
   @Override protected int desiredChunks(final Frame original_fr, boolean local) { return 1;  }
diff --git a/h2o-core/src/main/java/hex/quantile/QuantileModel.java b/h2o-core/src/main/java/hex/quantile/QuantileModel.java
index 219d171..99c65b9 100644
--- a/h2o-core/src/main/java/hex/quantile/QuantileModel.java
+++ b/h2o-core/src/main/java/hex/quantile/QuantileModel.java
@@ -17,6 +17,7 @@ public class QuantileModel extends Model<QuantileModel,QuantileModel.QuantilePar
     public String algoName() { return "Quantiles"; }
     public String fullName() { return "Quantiles"; }
     public String javaName() { return QuantileModel.class.getName(); }
+    @Override public long progressUnits() { return train().numCols()*_probs.length; }
   }
 
   public static class QuantileOutput extends Model.Output {
diff --git a/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java b/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java
index 8101333..1729063 100644
--- a/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java
+++ b/h2o-core/src/main/java/hex/schemas/GridSearchSchema.java
@@ -2,8 +2,6 @@ package hex.schemas;
 
 import hex.Model;
 import hex.grid.Grid;
-import hex.grid.GridSearch;
-import hex.grid.GridSearch.Strategy;
 import water.H2O;
 import water.Key;
 import water.api.*;
@@ -13,7 +11,6 @@ import water.util.IcedHashMap;
 import java.util.List;
 import java.util.Map;
 import java.util.Properties;
-import java.util.Random;
 
 /**
  * This is a common grid search schema composed of two parameters: default parameters for a builder
@@ -44,17 +41,8 @@ public class GridSearchSchema<G extends Grid<MP>,
   @API(help = "Destination id for this grid; auto-generated if not specified.", required = false, direction = API.Direction.INOUT)
   public KeyV3.GridKeyV3 grid_id;
 
-  @API(help="Hyperparameter search strategy, either \"Cartesian\" or \"Random\".", required = false, values = { "Cartesian", "Random" }, direction = API.Direction.INOUT)
-  public Strategy strategy = Strategy.Cartesian;
-
-  @API(help="Maximum number of models to build.", required = false, direction = API.Direction.INOUT)
-  public int max_models = Integer.MAX_VALUE;
-
-  @API(help="Maximum time to spend building models, in mS.  The highest possible is ~24.855 days.", required = false, direction = API.Direction.INOUT)
-  public int max_time_ms = Integer.MAX_VALUE;
-
-  @API(help="Seed for randomized search criteria.", required = false, direction = API.Direction.INOUT)
-  public long seed = (new Random().nextLong());
+  @API(help="Hyperparameter search criteria, including strategy and early stopping directives.  If it is not given, exhaustive Cartesian is used.", required = false, direction = API.Direction.INOUT)
+  public HyperSpaceSearchCriteriaV99 search_criteria;
 
   //
   // Outputs
@@ -78,26 +66,31 @@ public class GridSearchSchema<G extends Grid<MP>,
       parms.remove("hyper_parameters");
     }
 
-    // Ugh:
-    if (parms.containsKey("strategy"))
-      try { strategy = GridSearch.Strategy.valueOf((String)parms.get("strategy")); }
-      catch (IllegalArgumentException iae) { throw new H2OIllegalArgumentException("strategy", (String)parms.get("strategy")); }
-      finally { parms.remove("strategy"); }
-
-    if (parms.containsKey("max_models"))
-      try { max_models = Integer.valueOf((String)parms.get("max_models")); }
-      catch (NumberFormatException nfe) { throw new H2OIllegalArgumentException("max_models", (String)parms.get("max_models")); }
-      finally { parms.remove("max_models"); }
-
-    if (parms.containsKey("max_time_ms"))
-      try { max_time_ms = Integer.valueOf((String)parms.get("max_time_ms")); }
-      catch (NumberFormatException nfe) { throw new H2OIllegalArgumentException("max_time_ms", (String)parms.get("max_time_ms")); }
-      finally { parms.remove("max_time_ms"); }
-
-    if (parms.containsKey("seed"))
-      try { seed = Long.valueOf((String)parms.get("seed")); }
-      catch (NumberFormatException nfe) { throw new H2OIllegalArgumentException("seed", (String)parms.get("seed")); }
-      finally { parms.remove("seed"); }
+    if( parms.containsKey("search_criteria") ) {
+      Properties p = water.util.JSONUtils.parseToProperties(parms.getProperty("search_criteria"));
+
+      if (! p.containsKey("strategy")) {
+        throw new H2OIllegalArgumentException("search_criteria.strategy", "null");
+      }
+
+      // TODO: move this into a factory method in HyperSpaceSearchCriteriaV99
+      String strategy = (String)p.get("strategy");
+      if ("Cartesian".equals(strategy)) {
+        search_criteria = new HyperSpaceSearchCriteriaV99.CartesianSearchCriteriaV99();
+      } else if ("RandomDiscrete".equals(strategy)) {
+        search_criteria = new HyperSpaceSearchCriteriaV99.RandomDiscreteValueSearchCriteriaV99();
+      } else {
+        throw new H2OIllegalArgumentException("search_criteria.strategy", strategy);
+      }
+
+      search_criteria.fillWithDefaults();
+      search_criteria.fillFromParms(p);
+      parms.remove("search_criteria");
+    } else {
+      // Fall back to Cartesian if there's no search_criteria specified.
+      search_criteria = new HyperSpaceSearchCriteriaV99.CartesianSearchCriteriaV99();
+    }
+
 
     if (parms.containsKey("grid_id")) { grid_id = new KeyV3.GridKeyV3(Key.<Grid>make(parms.getProperty("grid_id"))); parms.remove("grid_id"); }
 
diff --git a/h2o-core/src/main/java/hex/schemas/HyperSpaceSearchCriteriaV99.java b/h2o-core/src/main/java/hex/schemas/HyperSpaceSearchCriteriaV99.java
new file mode 100644
index 0000000..8912352
--- /dev/null
+++ b/h2o-core/src/main/java/hex/schemas/HyperSpaceSearchCriteriaV99.java
@@ -0,0 +1,73 @@
+package hex.schemas;
+
+import hex.grid.HyperSpaceSearchCriteria;
+import water.api.API;
+import water.api.Schema;
+import water.exceptions.H2OIllegalArgumentException;
+
+/**
+ * Search criteria for a hyperparameter search including directives for how to search and
+ * when to stop the search.
+ */
+public class HyperSpaceSearchCriteriaV99<I, S> extends Schema<HyperSpaceSearchCriteria, HyperSpaceSearchCriteriaV99.CartesianSearchCriteriaV99> {
+
+  @API(help = "Hyperparameter space search strategy.", required = true, values = { "Unknown", "Cartesian", "RandomDiscrete" }, direction = API.Direction.INOUT)
+  public HyperSpaceSearchCriteria.Strategy strategy;
+
+// TODO: add a factory which accepts a Strategy and calls the right constructor
+
+  /**
+   * Search criteria for an exhaustive Cartesian hyperparameter search.
+   */
+  public static class CartesianSearchCriteriaV99 extends HyperSpaceSearchCriteriaV99<HyperSpaceSearchCriteria.CartesianSearchCriteria, CartesianSearchCriteriaV99> {
+    public CartesianSearchCriteriaV99() {
+      strategy = HyperSpaceSearchCriteria.Strategy.Cartesian;
+    }
+  }
+
+  /**
+   * Search criteria for random hyperparameter search using hyperparameter values given by
+   * lists. Includes directives for how to search and when to stop the search.
+   */
+  public static class RandomDiscreteValueSearchCriteriaV99 extends HyperSpaceSearchCriteriaV99<HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria, RandomDiscreteValueSearchCriteriaV99> {
+    public RandomDiscreteValueSearchCriteriaV99() {
+      strategy = HyperSpaceSearchCriteria.Strategy.RandomDiscrete;
+    }
+
+    public RandomDiscreteValueSearchCriteriaV99(long seed, int max_models, int max_time_ms) {
+      strategy = HyperSpaceSearchCriteria.Strategy.RandomDiscrete;
+      this.seed = seed;
+      this.max_models = max_models;
+      this.max_time_ms = max_time_ms;
+    }
+
+    @API(help = "Seed for random number generator; used for reproducibility.", required = false, direction = API.Direction.INOUT)
+    public long seed;
+
+    @API(help = "Maximum number of models to build (optional).", required = false, direction = API.Direction.INOUT)
+    public int max_models;
+
+    @API(help = "Maximum time to spend building models (optional).", required = false, direction = API.Direction.INOUT)
+    public int max_time_ms;
+  }
+
+  /**
+   * Fill with the default values from the corresponding Iced object.
+   */
+  public S fillWithDefaults() {
+    HyperSpaceSearchCriteria defaults = null;
+
+    if (HyperSpaceSearchCriteria.Strategy.Cartesian == strategy) {
+      defaults = new HyperSpaceSearchCriteria.CartesianSearchCriteria();
+    } else if (HyperSpaceSearchCriteria.Strategy.RandomDiscrete == strategy) {
+      defaults = new HyperSpaceSearchCriteria.RandomDiscreteValueSearchCriteria();
+    } else {
+      throw new H2OIllegalArgumentException("search_criteria.strategy", strategy.toString());
+    }
+
+    fillFromImpl(defaults);
+
+    return (S) this;
+  }
+
+}
diff --git a/h2o-core/src/main/java/water/H2O.java b/h2o-core/src/main/java/water/H2O.java
index 22740c7..9236274 100644
--- a/h2o-core/src/main/java/water/H2O.java
+++ b/h2o-core/src/main/java/water/H2O.java
@@ -291,6 +291,13 @@ final public class H2O {
 
       return result.toString();
     }
+
+    /**
+     * Whether this H2O instance was launched on hadoop (using 'hadoop jar h2odriver.jar') or not.
+     */
+    public boolean launchedWithHadoopJar() {
+      return hdfs_skip;
+    }
   }
 
   public static void parseFailed(String message) {
diff --git a/h2o-core/src/main/java/water/MRTask.java b/h2o-core/src/main/java/water/MRTask.java
index 290f131..05f348e 100644
--- a/h2o-core/src/main/java/water/MRTask.java
+++ b/h2o-core/src/main/java/water/MRTask.java
@@ -227,7 +227,7 @@ public abstract class MRTask<T extends MRTask<T>> extends DTask<T> implements Fo
     if( _output_types == null ) return null;
     final int noutputs = _output_types.length;
     Vec[] vecs = new Vec[noutputs];
-    if( _appendables==null )  // Zero rows?
+    if( _appendables==null || _appendables.length == 0)  // Zero rows?
       for( int i = 0; i < noutputs; i++ )
         vecs[i] = _fr.anyVec().makeZero();
     else {
diff --git a/h2o-core/src/main/java/water/api/GridSearchHandler.java b/h2o-core/src/main/java/water/api/GridSearchHandler.java
index 18064e6..ba4f942 100644
--- a/h2o-core/src/main/java/water/api/GridSearchHandler.java
+++ b/h2o-core/src/main/java/water/api/GridSearchHandler.java
@@ -5,6 +5,7 @@ import hex.ModelBuilder;
 import hex.ModelParametersBuilderFactory;
 import hex.grid.Grid;
 import hex.grid.GridSearch;
+import hex.grid.HyperSpaceSearchCriteria;
 import hex.schemas.GridSearchSchema;
 import water.H2O;
 import water.Job;
@@ -62,8 +63,13 @@ public class GridSearchHandler<G extends Grid<MP>,
     gss.init_meta();
     gss.parameters = (P)TypeMap.newFreezable(paramSchemaName);
     gss.parameters.init_meta();
+
+    // Get default parameters, then overlay the passed-in values
     ModelBuilder builder = ModelBuilder.make(algoURLName,null,null); // Default parameter settings
     gss.parameters.fillFromImpl(builder._parms); // Defaults for this builder into schema
+
+
+
     gss.fillFromParms(parms);   // Override defaults from user parms
 
     // Verify list of hyper parameters
@@ -82,7 +88,7 @@ public class GridSearchHandler<G extends Grid<MP>,
                                                  params,
                                                  gss.hyper_parameters,
                                                  new DefaultModelParametersBuilderFactory<MP, P>(),
-            gss.strategy, gss.max_models, gss.max_time_ms, gss.seed);
+                                                 (HyperSpaceSearchCriteria)gss.search_criteria.createAndFillImpl());
 
     // Fill schema with job parameters
     // FIXME: right now we have to remove grid parameters which we sent back
diff --git a/h2o-core/src/main/java/water/api/ModelBuilderHandler.java b/h2o-core/src/main/java/water/api/ModelBuilderHandler.java
index 693fedf..37b0897 100644
--- a/h2o-core/src/main/java/water/api/ModelBuilderHandler.java
+++ b/h2o-core/src/main/java/water/api/ModelBuilderHandler.java
@@ -42,8 +42,6 @@ public class ModelBuilderHandler<B extends ModelBuilder, S extends ModelBuilderS
     schema.parameters.fillFromParms(parms);         // Overwrite with user parms
     schema.parameters.fillImpl(builder._parms);     // Merged parms back over Model.Parameter object
     builder.init(false);          // validate parameters
-    if (builder.error_count() > 0)// Check for any parameter errors and bail now
-      throw H2OModelBuilderIllegalArgumentException.makeFromBuilder(builder);
 
     _t_start = System.currentTimeMillis();
     if( doTrain ) builder.trainModel();
diff --git a/h2o-core/src/main/java/water/api/ModelParametersSchema.java b/h2o-core/src/main/java/water/api/ModelParametersSchema.java
index db06251..340eea9 100644
--- a/h2o-core/src/main/java/water/api/ModelParametersSchema.java
+++ b/h2o-core/src/main/java/water/api/ModelParametersSchema.java
@@ -89,7 +89,7 @@ public class ModelParametersSchema<P extends Model.Parameters, S extends ModelPa
   @API(help = "Early stopping based on convergence of stopping_metric. Stop if simple moving average of length k of the stopping_metric does not improve for k:=stopping_rounds scoring events (0 to disable)", level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
   public int stopping_rounds;
 
-  @API(help = "Maximum allowed runtime in seconds for model training. Use 0 to disable. For cross-validation or grid searches, this limit applies to all sub-models.", level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
+  @API(help = "Maximum allowed runtime in seconds for model training. Use 0 to disable.", level = API.Level.secondary, direction=API.Direction.INOUT, gridable = true)
   public double max_runtime_secs;
 
   /**
diff --git a/h2o-core/src/main/java/water/parser/ParseDataset.java b/h2o-core/src/main/java/water/parser/ParseDataset.java
index cbeefa9..f7a9b0d 100644
--- a/h2o-core/src/main/java/water/parser/ParseDataset.java
+++ b/h2o-core/src/main/java/water/parser/ParseDataset.java
@@ -975,6 +975,7 @@ public final class ParseDataset {
       String isConstantStr = isConstant ? "constant" : "";
       String numLevelsStr = isCategorical ? String.format("%d", v.domain().length) : "";
 
+      boolean launchedWithHadoopJar = H2O.ARGS.launchedWithHadoopJar();
       boolean printLogSeparatorToStdout = false;
       boolean printColumnToStdout;
       {
@@ -984,7 +985,9 @@ public final class ParseDataset {
         // Print information to stdout for this many trailing columns.
         final int MAX_TAIL_TO_PRINT_ON_STDOUT = 10;
 
-        if (vecArr.length <= (MAX_HEAD_TO_PRINT_ON_STDOUT + MAX_TAIL_TO_PRINT_ON_STDOUT)) {
+        if (launchedWithHadoopJar) {
+          printColumnToStdout = true;
+        } else if (vecArr.length <= (MAX_HEAD_TO_PRINT_ON_STDOUT + MAX_TAIL_TO_PRINT_ON_STDOUT)) {
           // For small numbers of columns, print them all.
           printColumnToStdout = true;
         } else if (i < MAX_HEAD_TO_PRINT_ON_STDOUT) {
diff --git a/h2o-core/src/main/java/water/persist/PersistFS.java b/h2o-core/src/main/java/water/persist/PersistFS.java
index 102fb72..db555e5 100644
--- a/h2o-core/src/main/java/water/persist/PersistFS.java
+++ b/h2o-core/src/main/java/water/persist/PersistFS.java
@@ -19,7 +19,7 @@ final class PersistFS extends Persist {
   PersistFS(File root) {
     _root = root;
     _dir = new File(root, "ice" + H2O.API_PORT);
-    //deleteRecursive(_dir);
+    deleteRecursive(_dir);
     // Make the directory as-needed
     root.mkdirs();
     if( !(root.isDirectory() && root.canRead() && root.canWrite()) )
diff --git a/h2o-core/src/main/java/water/rapids/ASTBinOp.java b/h2o-core/src/main/java/water/rapids/ASTBinOp.java
index a4ce8d4..529f0e3 100644
--- a/h2o-core/src/main/java/water/rapids/ASTBinOp.java
+++ b/h2o-core/src/main/java/water/rapids/ASTBinOp.java
@@ -349,10 +349,11 @@ class ASTRound extends ASTBinOp {
     if(Double.isNaN(x)) return x;
     double sgn = x < 0 ? -1 : 1;
     x = Math.abs(x);
+    if( (int) digits != digits) digits = Math.round(digits);
     double power_of_10 = (int)Math.pow(10, (int)digits);
     return sgn*(digits == 0
                 // go to the even digit
-                ? (x % 1 >= 0.5 && !(Math.floor(x)%2==0))
+                ?( x % 1 > 0.5 || (x % 1 == 0.5 && !(Math.floor(x)%2==0)))
                 ? Math.ceil(x)
                 : Math.floor(x)
                 : Math.floor(x * power_of_10 + 0.5) / power_of_10);
@@ -363,6 +364,8 @@ class ASTSignif extends ASTBinOp {
   public String str() { return "signif"; }
   double op(double x, double digits) {
     if(Double.isNaN(x)) return x;
+    if(digits < 1) digits = 1; //mimic R's base::signif
+    if( (int) digits != digits) digits = Math.round(digits);
     java.math.BigDecimal bd = new java.math.BigDecimal(x);
     bd = bd.round(new java.math.MathContext((int)digits, java.math.RoundingMode.HALF_EVEN));
     return bd.doubleValue();
diff --git a/h2o-core/src/main/java/water/rapids/ASTCut.java b/h2o-core/src/main/java/water/rapids/ASTCut.java
index 2003b9f..5821a92 100644
--- a/h2o-core/src/main/java/water/rapids/ASTCut.java
+++ b/h2o-core/src/main/java/water/rapids/ASTCut.java
@@ -5,6 +5,9 @@ import water.fvec.Chunk;
 import water.fvec.Frame;
 import water.fvec.NewChunk;
 import water.fvec.Vec;
+import water.util.MathUtils;
+
+import java.util.Arrays;
 
 public class ASTCut extends ASTPrim {
   @Override
@@ -15,10 +18,11 @@ public class ASTCut extends ASTPrim {
   @Override Val apply( Env env, Env.StackHelp stk, AST asts[] ) {
     Frame fr = stk.track(asts[1].exec(env)).getFrame();
     double[] cuts  = check(asts[2]);
+    Arrays.sort(cuts);
     String[] labels= check2(asts[3]);
     final boolean lowest = asts[4].exec(env).getNum()==1;
     final boolean rite   = asts[5].exec(env).getNum()==1;
-    final int     digits = Math.max((int)asts[6].exec(env).getNum(),12); // cap at 12
+    final int     digits = Math.min((int) asts[6].exec(env).getNum(), 12); // cap at 12
 
     if(fr.vecs().length != 1 || fr.vecs()[0].isCategorical())
       throw new IllegalArgumentException("First argument must be a numeric column vector");
@@ -59,9 +63,9 @@ public class ASTCut extends ASTPrim {
         for (int r = 0; r < rows; ++r) {
           double x = c.atd(r);
           if (Double.isNaN(x) || (lowest && x <  cutz[0])
-                  || (!lowest && x <= cutz[0])
+                  || (!lowest && (x < cutz[0] || MathUtils.equalsWithinOneSmallUlp(x,cutz[0])) )
                   || (rite    && x >  cutz[cutz.length-1])
-                  || (!rite   && x >= cutz[cutz.length-1])) nc.addNum(Double.NaN); //slightly faster than nc.addNA();
+                  || (!rite   && (x > cutz[cutz.length-1] || MathUtils.equalsWithinOneSmallUlp(x,cutz[cutz.length-1]) )) ) nc.addNum(Double.NaN);
           else {
             for (int i = 1; i < cutz.length; ++i) {
               if (rite) {
diff --git a/h2o-core/src/main/java/water/rapids/ASTMerge.java b/h2o-core/src/main/java/water/rapids/ASTMerge.java
index d541a66..e8fd8d3 100644
--- a/h2o-core/src/main/java/water/rapids/ASTMerge.java
+++ b/h2o-core/src/main/java/water/rapids/ASTMerge.java
@@ -75,10 +75,7 @@ public class ASTMerge extends ASTPrim {
     // columns.  The hashed dataframe is completely replicated per-node
     boolean walkLeft;
     if( allLeft == allRite ) {
-      long lsize = 0, rsize = 0;
-      for( int i=ncols; i<l.numCols(); i++ ) lsize += l.vecs()[i].byteSize();
-      for( int i=ncols; i<r.numCols(); i++ ) rsize += r.vecs()[i].byteSize();
-      walkLeft = lsize > rsize;
+      walkLeft = l.numRows() > r.numRows();
     } else {
       walkLeft = allLeft;
     }
@@ -379,7 +376,7 @@ public class ASTMerge extends ASTPrim {
     void addRow(NewChunk[] nchks, Chunk[] chks, Vec[] vecs, int relRow, long absRow, BufferedString bStr) {
       int c=0;
       for( ;c< chks.length;++c) addElem(nchks[c],chks[c],relRow);
-      for( ;c<nchks.length;++c) addElem(nchks[c],vecs[_ncols],absRow,bStr);
+      for( ;c<nchks.length;++c) addElem(nchks[c],vecs[c - chks.length + _ncols],absRow,bStr);
     }
   }
 }
diff --git a/h2o-core/src/main/java/water/rapids/ASTUniOp.java b/h2o-core/src/main/java/water/rapids/ASTUniOp.java
index 847f818..c6f2823 100644
--- a/h2o-core/src/main/java/water/rapids/ASTUniOp.java
+++ b/h2o-core/src/main/java/water/rapids/ASTUniOp.java
@@ -50,7 +50,7 @@ abstract class ASTUniOp extends ASTPrim {
 
 class ASTCeiling extends ASTUniOp{ public String str() { return "ceiling";}double op(double d) { return Math.ceil (d); } }
 class ASTFloor extends ASTUniOp { public String str() { return "floor"; } double op(double d) { return Math.floor(d); } }
-class ASTNot   extends ASTUniOp { public String str() { return "!!"   ; } double op(double d) { return d==0?1:0; } }
+class ASTNot   extends ASTUniOp { public String str() { return "!!"   ; } double op(double d) { return Double.isNaN(d)?Double.NaN:d==0?1:0; } }
 class ASTTrunc extends ASTUniOp { public String str() { return "trunc"; } double op(double d) { return d>=0?Math.floor(d):Math.ceil(d);}}
 class ASTCos  extends ASTUniOp { public String str(){ return "cos";  } double op(double d) { return Math.cos(d);}}
 class ASTSin  extends ASTUniOp { public String str(){ return "sin";  } double op(double d) { return Math.sin(d);}}
@@ -357,23 +357,25 @@ class ASTMatch extends ASTPrim {
     else if( asts[2] instanceof ASTStr    ) strsTable2 = new String[]{asts[2].exec(env).getStr()};
     else throw new IllegalArgumentException("Expected numbers/strings. Got: "+asts[2].getClass());
 
+    final double nomatch = asts[3].exec(env).getNum();
+
     final String[] strsTable = strsTable2;
     final double[] dblsTable = dblsTable2;
 
     Frame rez = new MRTask() {
       @Override public void map(Chunk c, NewChunk n) {
         String[] domain = c.vec().domain();
-        int x, rows = c._len;
+        double x; int rows = c._len;
         for( int r = 0; r < rows; ++r) {
-          x = c.isNA(r) ? 0 : (strsTable==null ? in(dblsTable, c.atd(r)) : in(strsTable, domain[(int)c.at8(r)]));
-          n.addNum(x,0);
+          x = c.isNA(r) ? nomatch : (strsTable==null ? in(dblsTable, c.atd(r), nomatch) : in(strsTable, domain[(int)c.at8(r)], nomatch));
+          n.addNum(x);
         }
       }
     }.doAll(new byte[]{Vec.T_NUM}, fr.anyVec()).outputFrame();
     return new ValFrame(rez);
   }
-  private static int in(String[] matches, String s) { return Arrays.binarySearch(matches, s) >=0 ? 1: 0; }
-  private static int in(double[] matches, double d) { return binarySearchDoublesUlp(matches, 0,matches.length,d) >=0 ? 1: 0; }
+  private static double in(String[] matches, String s, double nomatch) { return Arrays.binarySearch(matches, s) >=0 ? 1: nomatch; }
+  private static double in(double[] matches, double d, double nomatch) { return binarySearchDoublesUlp(matches, 0,matches.length,d) >=0 ? 1: nomatch; }
 
   private static int binarySearchDoublesUlp(double[] a, int from, int to, double key) {
     int lo = from;
diff --git a/h2o-core/src/main/java/water/rapids/ASTVariance.java b/h2o-core/src/main/java/water/rapids/ASTVariance.java
index 200b7a5..893fe6f 100644
--- a/h2o-core/src/main/java/water/rapids/ASTVariance.java
+++ b/h2o-core/src/main/java/water/rapids/ASTVariance.java
@@ -6,116 +6,370 @@ import water.fvec.Chunk;
 import water.fvec.Frame;
 import water.fvec.Vec;
 import water.util.ArrayUtils;
+import java.util.Arrays;
 
 /** Variance between columns of a frame */
 class ASTVariance extends ASTPrim {
   @Override
-  public String[] args() { return new String[]{"ary", "x","y","use"}; }
-  private enum Mode { Everything, AllObs, CompleteObs }
-  @Override int nargs() { return 1+3; /* (var X Y use) */}
+  public String[] args() { return new String[]{"ary", "x","y","use", "symmetric"}; }
+  private enum Mode { Everything, AllObs, CompleteObs, PairwiseCompleteObs }
+  @Override int nargs() { return 1+4; /* (var X Y use symmetric) */}
   @Override public String str() { return "var"; }
   @Override Val apply( Env env, Env.StackHelp stk, AST asts[] ) {
     Frame frx = stk.track(asts[1].exec(env)).getFrame();
     Frame fry = stk.track(asts[2].exec(env)).getFrame();
     if( frx.numRows() != fry.numRows() )
       throw new IllegalArgumentException("Frames must have the same number of rows, found "+frx.numRows()+" and "+fry.numRows());
-    if( frx.numCols() != fry.numCols() )
-      throw new IllegalArgumentException("Frames must have the same number of columns, found "+frx.numCols()+" and "+fry.numCols());
-
     String use = stk.track(asts[3].exec(env)).getStr();
+    boolean symmetric = asts[4].exec(env).getNum()==1;
     Mode mode;
+    //In R, if the use arg is set, the na.rm arg has no effect (same result whether it is T or F). The na.rm param only 
+    // comes into play when no use arg is set. Without a use arg, setting na.rm = T is equivalent to use = "complete.obs",
+    // while setting na.rm = F (default) is equivalent to use = "everything". 
     switch( use ) {
-    case "everything":            mode = Mode.Everything; break;
-    case "all.obs":               mode = Mode.AllObs; break;
-    case "complete.obs":          mode = Mode.CompleteObs; break;
-    default: throw new IllegalArgumentException("unknown use mode, found: "+use);
+      case "everything":            mode = Mode.Everything; break;
+      case "all.obs":               mode = Mode.AllObs; break;
+      case "complete.obs":          mode = Mode.CompleteObs; break;
+      case "pairwise.complete.obs": mode = Mode.PairwiseCompleteObs; break;
+      default: throw new IllegalArgumentException("unknown use mode, found: "+use);
     }
-
-    return frx.numRows() == 1 ? scalar(frx,fry,mode) : array(frx,fry,mode);
+    
+    return frx.numRows() == 1 ? scalar(frx,fry,mode) : array(frx,fry,mode, symmetric);
   }
 
   // Scalar covariance for 1 row
-  private ValNum scalar( Frame frx, Frame fry, Mode mode ) {
+  private ValNum scalar( Frame frx, Frame fry, Mode mode) {
+    if( frx.numCols() != fry.numCols())
+      throw new IllegalArgumentException("Single rows have the same number of columns, found "+frx.numCols()+" and "+fry.numCols());
     Vec vecxs[] = frx.vecs();
     Vec vecys[] = fry.vecs();
-    double xmean=0, ymean=0, ncols = frx.numCols();
-    for( Vec v : vecxs ) xmean += v.at(0);
-    for( Vec v : vecys ) ymean += v.at(0);
-    xmean /= ncols; ymean /= ncols;
-   
-    double ss=0;
-    for( int r = 0; r < ncols; ++r )
-      ss += (vecxs[r].at(0) - xmean) * (vecys[r].at(0) - ymean);
-    if( Double.isNaN(ss) && mode.equals(Mode.AllObs) ) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
-    return new ValNum(ss/(ncols-1));
+    double xsum=0, ysum=0, NACount=0, ncols = frx.numCols(), xval, yval, ss=0;
+    for( int i=0; i< vecxs.length; i++) {
+      xval = vecxs[i].at(0);
+      yval = vecys[i].at(0);
+      if (Double.isNaN(xval) || Double.isNaN(yval))
+        NACount++;
+      else {
+        xsum += xval;
+        ysum += yval;
+        ss += xval * yval;
+      }
+    }
+    
+    if (NACount>0) {
+      if (mode.equals(Mode.AllObs)) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+      if (mode.equals(Mode.Everything)) return new ValNum(Double.NaN);
+    }
+    return new ValNum((ss - xsum * ysum/(ncols - NACount)) / (ncols-1-NACount));
   }
 
   // Matrix covariance.  Compute covariance between all columns from each Frame
   // against each other.  Return a matrix of covariances which is frx.numCols
-  // wide and fry.numCols tall.
-  private Val array( Frame frx, Frame fry, Mode mode ) {
+  // tall and fry.numCols wide.
+  private Val array( Frame frx, Frame fry, Mode mode, boolean symmetric) {
     Vec[] vecxs = frx.vecs();  int ncolx = vecxs.length;
     Vec[] vecys = fry.vecs();  int ncoly = vecys.length;
-    double[] ymeans = new double[ncoly];
-    for( int y=0; y<ncoly; y++ ) // All the Y means
-      ymeans[y] = vecys[y].mean();
 
-    // Launch tasks; each does all Ys vs one X
-    CoVarTask[] cvts = new CoVarTask[ncolx];
-    for( int x=0; x<ncolx; x++ )
-      cvts[x] = new CoVarTask(vecxs[x].mean(),ymeans).dfork(new Frame(vecxs[x]).add(fry));
-    // Short cut for the 1-row-1-col result: return a scalar
-    if( ncolx==1 && ncoly==1 )
-      return new ValNum(cvts[0].getResult()._covs[0]/(frx.numRows()-1));
+    if (mode.equals(Mode.Everything) || mode.equals(Mode.AllObs)) {
+      CoVarTaskEverything[] cvts = new CoVarTaskEverything[ncoly];
+      if (symmetric) {
+        int[] idx  = new int[ncoly];
+        for (int y = 0; y < ncoly; y++) idx[y] = y;
+        int[] first_index = new int[]{0};
+        //compute covariances between column_i and and column_i, column_i+1, ... 
+        Frame reduced_fr = new Frame(frx);
+        for (int y = 0; y <ncoly; y++) {
+          cvts[y] = new CoVarTaskEverything().dfork(new Frame(vecys[y]).add(reduced_fr));
+          idx = ArrayUtils.removeIds(idx, first_index);
+          reduced_fr = new Frame(frx.vecs(idx));
+        }
+        //arrange the results into the bottom left of res_array. each successive cvts is 1 smaller in length
+        double[][] res_array = new double[ncoly][ncoly];
+        for (int y =0; y<ncoly; y++) {
+          double[] res_array_y = res_array[y];
+          CoVarTaskEverything cvtx = cvts[y].getResult();
+          if (mode.equals(Mode.AllObs))
+            for (double ss : cvtx._ss)
+              if (Double.isNaN(ss)) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+          double[] res = ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                  ArrayUtils.div(cvtx._ysum, frx.numRows()))), frx.numRows() - 1);
+          System.arraycopy(res, 0, res_array_y, y, ncoly - y);
+        }
+        //copy over the bottom left of res_array to its top right
+        for (int y = 0; y < ncoly -1; y++) {
+          for (int x = y+1; x < ncoly ; x++) {
+            res_array[x][y] = res_array[y][x];
+          }
+        }
+        //set Frame
+        Vec[] res = new Vec[ncoly];
+        Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+        for (int y = 0; y < ncoly; y++) {
+          res[y] = Vec.makeVec(res_array[y], keys[y]);
+        }
+        return new ValFrame(new Frame(fry._names, res));
+      }
+      // Launch tasks; each does all Xs vs one Y
+      for (int y = 0; y < ncoly; y++)
+        cvts[y] = new CoVarTaskEverything().dfork(new Frame(vecys[y]).add(frx));
+      // Short cut for the 1-row-1-col result: return a scalar
+      if (ncolx == 1 && ncoly == 1) {
+        CoVarTaskEverything res = cvts[0].getResult();
+        if (mode.equals(Mode.AllObs) && Double.isNaN(res._ss[0])) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+        return new ValNum((res._ss[0] - res._xsum[0] * res._ysum[0] / (frx.numRows())) / (frx.numRows() - 1));
+      }
+      // Gather all the Xs-vs-Y covariance arrays; divide by rows
+      Vec[] res = new Vec[ncoly];
+      Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+      for (int y = 0; y < ncoly; y++) {
+        CoVarTaskEverything cvtx = cvts[y].getResult();
+        if (mode.equals(Mode.AllObs))
+          for (double ss : cvtx._ss)
+            if (Double.isNaN(ss)) throw new IllegalArgumentException("Mode is 'all.obs' but NAs are present");
+        res[y] = Vec.makeVec(ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                ArrayUtils.div(cvtx._ysum, frx.numRows()))), frx.numRows() - 1), keys[y]);
+      }
 
-    // Gather all the Ys-vs-X covariance arrays; divide by rows
-    Vec[] res = new Vec[ncolx];
-    Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncolx);
-    for( int x=0; x<ncolx; x++ )
-      res[x] = Vec.makeVec(ArrayUtils.div(cvts[x].getResult()._covs, (frx.numRows()-1)), keys[x]);
+      // CNC - For fun, uncomment this code to scale all values by their
+      // respective std-devs, basically standardizing the results.  This gives
+      // something similar to a r^2 correlation where 1.0 (max possible value)
+      // indicates perfect linearity (maybe someting weaker: perfect equality?),
+      // and -1 perfectly anti-linear (90% twist), and zero is remains
+      // uncorrelated (with actual covariance, zero is also uncorrelated but
+      // non-zero values are scaled by the columns' numeric range).
+      //
+      //for( int x=0; x<ncolx; x++ ) {
+      //  double ds[] = ArrayUtils.div(cvts[x].getResult()._covs, (frx.numRows()-1));
+      //  ArrayUtils.div(cvts[x].getResult()._covs, vecxs[x].sigma());
+      //  for( int y=0; y<ncoly; y++ )
+      //    ds[y] /= vecys[y].sigma();
+      //  res[x] = Vec.makeVec(ds, keys[x]);
+      //}
+      return new ValFrame(new Frame(fry._names, res));
+    }
+    if (mode.equals(Mode.CompleteObs)) {
+      CoVarTaskComplete cvs = new CoVarTaskComplete(ncolx, ncoly).doAll(new Frame(fry).add(frx));
+      
+      if (ncolx == 1 && ncoly == 1)
+        return new ValNum((cvs._ss[0][0] - cvs._xsum[0] * cvs._ysum[0] / (frx.numRows() - cvs._NACount)) / (frx.numRows() - cvs._NACount - 1));
+      
+      //All Xs-and-Ys
+      Vec[] res = new Vec[ncoly];
+      Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);      
+      for (int y= 0; y < ncoly; y++) {
+        res[y] = Vec.makeVec(ArrayUtils.div(ArrayUtils.subtract(cvs._ss[y], ArrayUtils.mult(cvs._xsum.clone(),
+                (cvs._ysum[y] / (frx.numRows() - cvs._NACount)))), (frx.numRows() - 1 - cvs._NACount)), keys[y]);
+      }
+      return new ValFrame(new Frame(fry._names, res));
+    }
+    else { //if (mode.equals(Mode.PairwiseCompleteObs)) {
+      CoVarTaskPairwise[] cvts = new CoVarTaskPairwise[ncoly];
+      if (symmetric) {
+        int[] idx  = new int[ncoly];
+        for (int y = 0; y < ncoly; y++) idx[y] = y;
+        int[] first_index = new int[]{0};
+        //compute covariances between column_i and and column_i, column_i+1, ... 
+        Frame reduced_fr = new Frame(frx);
+        for (int y = 0; y <ncoly; y++) {
+          cvts[y] = new CoVarTaskPairwise().dfork(new Frame(vecys[y]).add(reduced_fr));
+          idx = ArrayUtils.removeIds(idx, first_index);
+          reduced_fr = new Frame(frx.vecs(idx));
+        }
+        //arrange the results into the bottom left of res_array. each successive cvts is 1 smaller in length
+        double[][] res_array = new double[ncoly][ncoly];
+        for (int y =0; y<ncoly; y++) {
+          double[] res_array_y = res_array[y];
+          CoVarTaskPairwise cvtx = cvts[y].getResult();
 
-    // CNC - For fun, uncomment this code to scale all values by their
-    // respective std-devs, basically standardizing the results.  This gives
-    // something similar to a r^2 correlation where 1.0 (max possible value)
-    // indicates perfect linearity (maybe someting weaker: perfect equality?),
-    // and -1 perfectly anti-linear (90% twist), and zero is remains
-    // uncorrelated (with actual covariance, zero is also uncorrelated but
-    // non-zero values are scaled by the columns' numeric range).
-    //
-    //for( int x=0; x<ncolx; x++ ) {
-    //  double ds[] = ArrayUtils.div(cvts[x].getResult()._covs, (frx.numRows()-1));
-    //  ArrayUtils.div(cvts[x].getResult()._covs, vecxs[x].sigma());
-    //  for( int y=0; y<ncoly; y++ )
-    //    ds[y] /= vecys[y].sigma();
-    //  res[x] = Vec.makeVec(ds, keys[x]);
-    //}
-    return new ValFrame(new Frame(frx._names,res));
+          double[] res = ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                  ArrayUtils.div(cvtx._ysum, ArrayUtils.subtract(frx.numRows(), cvtx._NACount.clone())))), 
+                  ArrayUtils.subtract(frx.numRows() - 1, cvtx._NACount.clone()));
+          System.arraycopy(res, 0, res_array_y, y, ncoly - y);
+        }
+        //copy over the bottom left of res_array to its top right
+        for (int y = 0; y < ncoly -1; y++) {
+          for (int x = y+1; x < ncoly ; x++) {
+            res_array[x][y] = res_array[y][x];
+          }
+        }
+        //set Frame
+        Vec[] res = new Vec[ncoly];
+        Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+        for (int y = 0; y < ncoly; y++) {
+          res[y] = Vec.makeVec(res_array[y], keys[y]);
+        }
+        return new ValFrame(new Frame(fry._names, res));
+      }
+      for (int y = 0; y < ncoly; y++)
+        cvts[y] = new CoVarTaskPairwise().dfork(new Frame(vecys[y]).add(frx));
+      // Short cut for the 1-row-1-col result: return a scalar
+      if (ncolx == 1 && ncoly == 1) {
+        CoVarTaskPairwise res = cvts[0].getResult();
+        return new ValNum((res._ss[0] - res._xsum[0] * res._ysum[0] / (frx.numRows() - res._NACount[0])) / (frx.numRows() - 1 - res._NACount[0]));
+      }
+      // Gather all the Xs-vs-Y covariance arrays; divide by rows
+      Vec[] res = new Vec[ncoly];
+      Key<Vec>[] keys = Vec.VectorGroup.VG_LEN1.addVecs(ncoly);
+      for (int y = 0; y < ncoly; y++) {
+        CoVarTaskPairwise cvtx = cvts[y].getResult();
+        res[y] = Vec.makeVec(ArrayUtils.div(ArrayUtils.subtract(cvtx._ss, ArrayUtils.mult(cvtx._xsum,
+                ArrayUtils.div(cvtx._ysum, ArrayUtils.subtract(frx.numRows(), cvtx._NACount.clone())))), 
+                ArrayUtils.subtract(frx.numRows() - 1, cvtx._NACount.clone())), keys[y]);
+      }
+      
+      return new ValFrame(new Frame(fry._names, res));
+    }
   }
 
-  private static class CoVarTask extends MRTask<CoVarTask> {
-    double[] _covs;
-    final double _xmean, _ymeans[];
-    CoVarTask( double xmean, double[] ymeans ) { _xmean = xmean; _ymeans = ymeans; }
+
+  private static class CoVarTaskEverything extends MRTask<CoVarTaskEverything> {
+    double[] _ss, _xsum, _ysum;
+    CoVarTaskEverything() {}
     @Override public void map( Chunk cs[] ) {
-      final int ncols = cs.length-1;
-      final Chunk cx = cs[0];
-      final int len = cx._len;
-      _covs = new double[ncols];
-      for( int y=0; y<ncols; y++ ) {
-        double sum = 0;
-        final Chunk cy = cs[y+1];
-        final double ymean = _ymeans[y];
-        for( int row=0; row<len; row++ ) 
-          sum += (cx.atd(row)-_xmean)*(cy.atd(row)-ymean);
-        _covs[y] = sum;
+      final int ncolsx = cs.length-1;
+      final Chunk cy = cs[0];
+      final int len = cy._len;
+      _ss = new double[ncolsx];
+      _xsum = new double[ncolsx];
+      _ysum = new double[ncolsx];
+      double xval, yval;
+      for( int x=0; x<ncolsx; x++ ) {
+        double ss = 0, xsum = 0, ysum = 0;
+        final Chunk cx = cs[x+1];
+        for( int row=0; row<len; row++ ) {
+          xval = cx.atd(row);
+          yval = cy.atd(row);
+          xsum += xval;
+          ysum += yval;
+          ss += xval * yval;
+        }
+        _ss[x] = ss;
+        _xsum[x] = xsum;
+        _ysum[x] = ysum;
       }
     }
-    @Override public void reduce( CoVarTask cvt ) { ArrayUtils.add(_covs,cvt._covs); }
+    @Override public void reduce( CoVarTaskEverything cvt ) {
+      ArrayUtils.add(_ss,cvt._ss);
+      ArrayUtils.add(_xsum, cvt._xsum);
+      ArrayUtils.add(_ysum, cvt._ysum);
+    }
+  }
+
+  private static class CoVarTaskComplete extends MRTask<CoVarTaskComplete> {
+    double[][] _ss;
+    double[] _xsum, _ysum;
+    long _NACount;
+    int _ncolx, _ncoly;
+    CoVarTaskComplete(int ncolx, int ncoly) { _ncolx = ncolx; _ncoly = ncoly;}
+    @Override public void map( Chunk cs[] ) {
+      
+      _ss = new double[_ncoly][_ncolx];
+      _xsum = new double[_ncolx];
+      _ysum = new double[_ncoly];
+
+      double[] xvals = new double[_ncolx];
+      double[] yvals = new double[_ncoly];
+
+      double xval, yval;
+      double[] _ss_y;
+      boolean add;
+      int len = cs[0]._len;
+      for (int row = 0; row < len; row++) {
+        add = true;
+        //reset existing arrays to 0 rather than initializing new ones to save on garbage collection
+        Arrays.fill(xvals, 0);
+        Arrays.fill(yvals, 0);
+        
+        for (int y = 0; y < _ncoly; y++) {
+          final Chunk cy = cs[y];
+          yval = cy.atd(row);
+          //if any yval along a row is NA, discard the entire row
+          if (Double.isNaN(yval)) {
+            _NACount++;
+            add = false;
+            break;
+          }
+          yvals[y] = yval;
+        }
+        if (add) {
+          for (int x = 0; x < _ncolx; x++) {
+            final Chunk cx = cs[x + _ncoly];
+            xval = cx.atd(row);
+            //if any xval along a row is NA, discard the entire row
+            if (Double.isNaN(xval)) {
+              _NACount++;
+              add = false;
+              break;
+            }
+            xvals[x] = xval;
+          }
+        }
+        //add is true iff row has been traversed and found no NAs among yvals and xvals  
+        if (add) {
+          for (int y=0; y < _ncoly; y++) {
+            _ss_y = _ss[y];
+            yval = yvals[y];
+            for (int x = 0; x < _ncolx; x++)
+              _ss_y[x] += xvals[x] * yval;
+          }
+          ArrayUtils.add(_xsum, xvals);
+          ArrayUtils.add(_ysum, yvals);
+        }
+      }
+    }
+    @Override public void reduce( CoVarTaskComplete cvt ) {
+      ArrayUtils.add(_ss,cvt._ss);
+      ArrayUtils.add(_xsum, cvt._xsum);
+      ArrayUtils.add(_ysum, cvt._ysum);
+      _NACount += cvt._NACount;
+    }
+  }
+
+  private static class CoVarTaskPairwise extends MRTask<CoVarTaskPairwise> {
+    double[] _ss, _xsum, _ysum;
+    long[] _NACount;
+    CoVarTaskPairwise() {}
+    @Override public void map( Chunk cs[] ) {
+      final int ncolsx = cs.length-1;
+      final Chunk cy = cs[0];
+      final int len = cy._len;
+      _ss = new double[ncolsx];
+      _xsum = new double[ncolsx];
+      _ysum = new double[ncolsx];
+      _NACount = new long[ncolsx];
+      double xval, yval;
+      for( int x=0; x<ncolsx; x++ ) {
+        double ss = 0, xsum = 0, ysum = 0;
+        long na = 0;
+        final Chunk cx = cs[x+1];
+        for( int row=0; row<len; row++ ) {
+          xval = cx.atd(row);
+          yval = cy.atd(row);
+          if (Double.isNaN(xval) || Double.isNaN(yval))
+            na++;
+          else {
+            xsum += xval;
+            ysum += yval; 
+            ss += xval * yval;
+          }
+        }
+        _ss[x] = ss;
+        _xsum[x] = xsum;
+        _ysum[x] = ysum;
+        _NACount[x] = na;
+      }
+    }
+    @Override public void reduce( CoVarTaskPairwise cvt ) { 
+      ArrayUtils.add(_ss,cvt._ss);
+      ArrayUtils.add(_xsum, cvt._xsum);
+      ArrayUtils.add(_ysum, cvt._ysum);
+      ArrayUtils.add(_NACount, cvt._NACount);
+    }
   }
 
   static double getVar(Vec v) {
-    double m = v.mean();
-    CoVarTask t = new CoVarTask(m,new double[]{m}).doAll(new Frame(v, v));
-    return t._covs[0] / (v.length() - 1);
+    CoVarTaskEverything res = new CoVarTaskEverything().doAll(new Frame(v, v));
+    return (res._ss[0] - res._xsum[0] * res._ysum[0] / (v.length())) / (v.length() - 1);
+    
   }
 }
diff --git a/h2o-core/src/main/java/water/util/ArrayUtils.java b/h2o-core/src/main/java/water/util/ArrayUtils.java
index 081ed66..7262b7b 100644
--- a/h2o-core/src/main/java/water/util/ArrayUtils.java
+++ b/h2o-core/src/main/java/water/util/ArrayUtils.java
@@ -253,6 +253,12 @@ public class ArrayUtils {
     for (int i=0; i<ary.length; i++) mult(ary[i], n);
     return ary;
   }
+  
+  public static double[] mult(double[] nums, double[] nums2) {
+    for (int i=0; i<nums.length; i++) nums[i] *= nums2[i];
+    return nums;
+  }
+  
   public static double[] invert(double[] ary) {
     if(ary == null) return null;
     for(int i=0;i<ary.length;i++) ary[i] = 1. / ary[i];
@@ -1242,7 +1248,7 @@ public class ArrayUtils {
     double [] res = new double[x.length-ids.length];
     int j = 0;
     for(int i = 0; i < x.length; ++i)
-      if(i != ids[j]) res[i-j] = x[i]; else ++j;
+      if(j == ids.length || i != ids[j]) res[i-j] = x[i]; else ++j;
     return res;
   }
 
@@ -1260,4 +1266,9 @@ public class ArrayUtils {
       if(d != 0)++res;
     return res;
   }
+
+  public static long[] subtract(long n, long[] nums) {
+    for (int i=0; i<nums.length; i++) nums[i] = n - nums[i];
+    return nums;
+  }
 }
diff --git a/h2o-core/src/main/java/water/util/JSONUtils.java b/h2o-core/src/main/java/water/util/JSONUtils.java
index 37f90fa..3e0c049 100644
--- a/h2o-core/src/main/java/water/util/JSONUtils.java
+++ b/h2o-core/src/main/java/water/util/JSONUtils.java
@@ -3,9 +3,14 @@ package water.util;
 import com.google.gson.Gson;
 import water.nbhm.NonBlockingHashMap;
 
+import java.util.Properties;
+
 public class JSONUtils {
 
   public static NonBlockingHashMap<String, Object> parse(String json) {
     return new Gson().fromJson(json, NonBlockingHashMap.class);
   }
+  public static Properties parseToProperties(String json) {
+    return new Gson().fromJson(json, Properties.class);
+  }
 }
diff --git a/h2o-core/src/main/java/water/util/Log.java b/h2o-core/src/main/java/water/util/Log.java
index 331ed2a..00fc123 100644
--- a/h2o-core/src/main/java/water/util/Log.java
+++ b/h2o-core/src/main/java/water/util/Log.java
@@ -303,7 +303,7 @@ abstract public class Log {
   private static synchronized org.apache.log4j.Logger createLog4j() {
     if( _logger != null ) return _logger; // Test again under lock
 
-    boolean launchedWithHadoopJar = H2O.ARGS.hdfs_skip;
+    boolean launchedWithHadoopJar = H2O.ARGS.launchedWithHadoopJar();
     String log4jConfiguration = System.getProperty ("h2o.log4j.configuration");
     boolean log4jConfigurationProvided = log4jConfiguration != null;
 
diff --git a/h2o-core/src/test/java/hex/ModelAdaptTest.java b/h2o-core/src/test/java/hex/ModelAdaptTest.java
index 796751d..590b5a0 100644
--- a/h2o-core/src/test/java/hex/ModelAdaptTest.java
+++ b/h2o-core/src/test/java/hex/ModelAdaptTest.java
@@ -20,6 +20,7 @@ public class ModelAdaptTest extends TestUtil {
       public String algoName() { return "A"; }
       public String fullName() { return "A"; }
       public String javaName() { return AModel.class.getName(); }
+      @Override public long progressUnits() { return 0; }
     }
     static class AOutput extends Model.Output { }
   }
diff --git a/h2o-core/src/test/java/water/TestUtil.java b/h2o-core/src/test/java/water/TestUtil.java
index 760a18d..9fdc52a 100644
--- a/h2o-core/src/test/java/water/TestUtil.java
+++ b/h2o-core/src/test/java/water/TestUtil.java
@@ -26,6 +26,7 @@ import static org.junit.Assert.assertTrue;
 
 @Ignore("Support for tests, but no actual tests here")
 public class TestUtil extends Iced {
+  public final static boolean JACOCO_ENABLED = Boolean.parseBoolean(System.getProperty("test.jacocoEnabled", "false"));
   private static boolean _stall_called_before = false;
   protected static int _initial_keycnt = 0;
   protected static int MINCLOUDSIZE;
@@ -41,7 +42,11 @@ public class TestUtil extends Iced {
       H2O.registerRestApis(System.getProperty("user.dir"));
       _stall_called_before = true;
     }
-    H2O.waitForCloudSize(x, 30000);
+    if (JACOCO_ENABLED) {
+      H2O.waitForCloudSize(x, 120000);
+    } else {
+      H2O.waitForCloudSize(x, 30000);
+    }
     _initial_keycnt = H2O.store_size();
   }
 
diff --git a/h2o-core/src/test/java/water/api/APIThrPriorTest.java b/h2o-core/src/test/java/water/api/APIThrPriorTest.java
index 737b3e5..d566294 100644
--- a/h2o-core/src/test/java/water/api/APIThrPriorTest.java
+++ b/h2o-core/src/test/java/water/api/APIThrPriorTest.java
@@ -158,6 +158,7 @@ class BogusModel extends Model<BogusModel,BogusModel.BogusParameters,BogusModel.
     public String algoName() { return "Bogus"; }
     public String fullName() { return "Bogus"; }
     public String javaName() { return BogusModel.class.getName(); }
+    @Override public long progressUnits() { return 0; }
   }
   public static class BogusOutput extends Model.Output { }
   BogusModel( Key selfKey, BogusParameters parms, BogusOutput output) { super(selfKey,parms,output); }
@@ -177,7 +178,6 @@ class Bogus extends ModelBuilder<BogusModel,BogusModel.BogusParameters,BogusMode
   @Override public BuilderVisibility builderVisibility() { return BuilderVisibility.Experimental; }
   public Bogus( BogusModel.BogusParameters parms ) { super(parms); init(false); }
   @Override protected Driver trainModelImpl() { return new BogusDriver(); }
-  @Override public long progressUnits() { return 0; }
   @Override public void init(boolean expensive) { super.init(expensive); }
 
   private class BogusDriver extends Driver {
diff --git a/h2o-core/src/test/java/water/junit/H2OTestRunner.java b/h2o-core/src/test/java/water/junit/H2OTestRunner.java
index 3eff5ba..9ce7aba 100644
--- a/h2o-core/src/test/java/water/junit/H2OTestRunner.java
+++ b/h2o-core/src/test/java/water/junit/H2OTestRunner.java
@@ -20,7 +20,6 @@ import java.util.List;
 public class H2OTestRunner {
 
   public Result run(String[] args) throws Exception {
-
     // List all classes - adapted from JUnitCore code
     List<Class<?>> classes = new ArrayList<Class<?>>();
     List<Failure> missingClasses = new ArrayList<Failure>();
@@ -51,7 +50,8 @@ public class H2OTestRunner {
 
   public static void main(String[] args) throws Exception {
     H2OTestRunner testRunner = new H2OTestRunner();
-    Result result = testRunner.run(args);
+    Result result = null;
+    result = testRunner.run(args);
     System.exit(result.wasSuccessful() ? 0 : 1);
   }
 }
diff --git a/h2o-core/src/test/java/water/rapids/RapidsTest.java b/h2o-core/src/test/java/water/rapids/RapidsTest.java
index da9884e..f4abfa8 100644
--- a/h2o-core/src/test/java/water/rapids/RapidsTest.java
+++ b/h2o-core/src/test/java/water/rapids/RapidsTest.java
@@ -205,10 +205,10 @@ public class RapidsTest extends TestUtil {
 
   @Test public void testVariance() {
     // Checking variance: scalar
-    String tree = "({x . (var x x \"everything\")} (rows a.hex [0]))";
+    String tree = "({x . (var x x \"everything\" FALSE)} (rows a.hex [0]))";
     checkTree(tree);
 
-    tree = "({x . (var x x \"everything\")} a.hex)";
+    tree = "({x . (var x x \"everything\" FALSE)} a.hex)";
     checkTree(tree);
 
     tree = "(table (trunc (cols a.hex 1)) FALSE)";
diff --git a/jacoco/coverage_tool/DiffScanner.java b/jacoco/coverage_tool/DiffScanner.java
new file mode 100644
index 0000000..a416e1f
--- /dev/null
+++ b/jacoco/coverage_tool/DiffScanner.java
@@ -0,0 +1,361 @@
+//package diff;
+
+import java.io.*;
+import java.security.InvalidParameterException;
+import java.util.Scanner;
+import java.util.NoSuchElementException;
+import java.util.InputMismatchException;
+import java.util.regex.Pattern;
+import java.util.regex.Matcher;
+import java.util.regex.MatchResult;
+import java.util.List;
+import java.util.ArrayList;
+import java.util.Comparator;
+import java.util.Collections;
+import java.util.Iterator;
+
+/**
+ * Wrapper class for Java's Scanner. It is designed to parse the output of a diff provided by git.
+ **/
+public class DiffScanner {
+    private static final Pattern _BLOCK_HEADER_REGEX =
+            Pattern.compile("^@@\\s-(\\d+),(\\d+)\\s\\+(\\d+),(\\d+)\\s@@\\s.*$\\n", Pattern.MULTILINE);
+    private static final Pattern _BLOCK_REGEX =
+            Pattern.compile("(?=^[+\\-\\s])((?:^\\s.*$\\n)*)((?:^-.*$\\n)*)((?:^\\+.*$\\n)*)", Pattern.MULTILINE);
+    private static final Pattern _FILE_REGEX =
+            Pattern.compile("^diff\\s.*$\\n^index\\s\\w+\\.\\.\\w+\\s\\w+$\\n^---\\s(?:[abciow]/)?(.*)$\\n^\\+\\+\\+\\s(?:[abciow]/)?(.*)$\\n", Pattern.MULTILINE);
+
+    private Scanner _sc;
+    private MatchResult _next_match = null;
+    private NextType _next_type = NextType.NONE;
+
+    private enum NextType {
+        FILE, BLOCK_HEADER, BLOCK, NONE
+    }
+
+    public DiffScanner(InputStream source) {
+        _sc = new Scanner(source);
+    }
+
+    public DiffScanner(File source) throws FileNotFoundException {
+        _sc = new Scanner(source);
+    }
+
+    public boolean hasNextFile() {
+        if (_next_type == NextType.FILE) {
+            return true;
+        } else if (_next_type == NextType.NONE) {
+            try {
+                _sc.skip(_FILE_REGEX);
+            } catch (NoSuchElementException nsee) {
+                return false;
+            }
+            _next_match = _sc.match();
+            _next_type = NextType.FILE;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    public DiffFile nextFile() {
+        if (_next_type == NextType.NONE) hasNextFile();
+        if (_next_type == NextType.FILE) {
+            DiffFile df = new DiffFile(new File(_next_match.group(1)), new File(_next_match.group(2)));
+            resetNext();
+            return df;
+        } else {
+            throw new InputMismatchException();
+        }
+    }
+
+    public boolean hasNextBlockHeader() {
+        if (_next_type == NextType.BLOCK_HEADER) {
+            return true;
+        } else if (_next_type == NextType.NONE) {
+            try {
+                _sc.skip(_BLOCK_HEADER_REGEX);
+            } catch (NoSuchElementException nsee) {
+                return false;
+            }
+            _next_match = _sc.match();
+            _next_type = NextType.BLOCK_HEADER;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    public DiffBlockHeader nextBlockHeader() {
+        int r_start, r_length, i_start, i_length;
+
+        if (_next_type == NextType.NONE) hasNextBlockHeader();
+        if (_next_type == NextType.BLOCK_HEADER) {
+            r_start = Integer.parseInt(_next_match.group(1));
+            r_length = Integer.parseInt(_next_match.group(2));
+            i_start = Integer.parseInt(_next_match.group(3));
+            i_length = Integer.parseInt(_next_match.group(4));
+            if (r_start > 0) r_start -= 1;
+            if (i_start > 0) i_start -= 1;
+            DiffBlockHeader dbh = new DiffBlockHeader(r_start, r_length, i_start, i_length);
+            resetNext();
+            return dbh;
+        } else {
+            throw new InputMismatchException();
+        }
+    }
+
+    public boolean hasNextBlock() {
+        if (_next_type == NextType.BLOCK) {
+            return true;
+        } else if (_next_type == NextType.NONE) {
+            try {
+                _sc.skip(_BLOCK_REGEX);
+            } catch (NoSuchElementException nsee) {
+                return false;
+            }
+            _next_match = _sc.match();
+            _next_type = NextType.BLOCK;
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    public DiffBlock nextBlock() {
+        int blank_count, remove_count, insert_count;
+        if (_next_type == NextType.NONE) hasNextBlockHeader();
+        if (_next_type == NextType.BLOCK) {
+            blank_count = getLineCount(_next_match.group(1));
+            remove_count = getLineCount(_next_match.group(2));
+            insert_count = getLineCount(_next_match.group(3));
+            resetNext();
+            return new DiffBlock(blank_count - 1, remove_count, blank_count - 1, insert_count);
+        } else {
+            throw new InputMismatchException();
+        }
+    }
+
+    // Helper function to count the number of lines in a string
+    private int getLineCount(String s) {
+        if (s.isEmpty()) return 0;
+        Matcher m = Pattern.compile("\r\n|\r|\n").matcher(s);
+        int lines = 1;
+        while (m.find()) lines += 1;
+        return lines;
+    }
+
+    private void resetNext() {
+        _next_type = NextType.NONE;
+        _next_match = null;
+    }
+
+    public void close() {
+        _sc.close();
+    }
+
+
+    public static void main(String[] args) {
+        DiffScanner ds = null;
+        try {
+            ds = new DiffScanner(new File(args[0]));
+        } catch (FileNotFoundException fnfe) {
+            System.err.println("ERROR: Could not find file '" + args[0] + "'");
+            System.exit(1);
+        } catch (ArrayIndexOutOfBoundsException aioobe) {
+            System.out.println("ERROR: File name required");
+            System.exit(1);
+        }
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextFile());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextBlockHeader());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextBlock());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+        System.out.println(ds.nextBlock());
+        System.out.println(ds.hasNextBlockHeader());
+        System.out.println(ds.hasNextBlock());
+        System.out.println(ds.hasNextFile());
+    }
+}
+
+class DiffFile {
+    private final File _old_file;
+    private final File _new_file;
+    private Comparator<DiffBlock> _comp;
+    private List<DiffBlock> _diffs;
+
+    public DiffFile(File file) {
+        this(file, file);
+    }
+
+    public DiffFile(File file, List<DiffBlock> diffs) {
+        this(file, file, diffs);
+    }
+
+    public DiffFile(File old_file, File new_file) {
+        this(old_file, new_file, new ArrayList<DiffBlock>());
+    }
+
+    public DiffFile(File old_file, File new_file, List<DiffBlock> diffs) {
+        _old_file = old_file;
+        _new_file = new_file;
+        _diffs = diffs;
+        sortByRemove();
+    }
+
+    public boolean pushDiff(DiffBlock diff) {
+        return _diffs.add(diff);
+    }
+
+    public Iterator<DiffBlock> iterator() {
+        Collections.sort(_diffs, _comp);
+        return _diffs.iterator();
+    }
+
+    public void sortByInsert() {
+        _comp = new Comparator<DiffBlock>() {
+            public int compare(DiffBlock o1, DiffBlock o2) {
+                return o1.getInsertStart() - o2.getInsertStart();
+            }
+        };
+    }
+
+    public void sortByRemove() {
+        _comp = new Comparator<DiffBlock>() {
+            public int compare(DiffBlock o1, DiffBlock o2) {
+                return o1.getRemoveStart() - o2.getRemoveStart();
+            }
+        };
+    }
+
+    public String toString() {
+        Iterator<DiffBlock> i = iterator();
+        String out = "DiffFile: '" + _old_file.getName() + "' -> '" + _new_file.getName() + "'";
+        while (i.hasNext()) out += "\n\t" + i.next().toString();
+        return out;
+    }
+
+}
+
+class DiffBlockHeader {
+    private final int _remove_start; // Starts with 0, add 1 for actual line number
+    private final int _insert_start; // Starts with 0, add 1 for actual line number
+
+    private int _remove_length;
+    private int _insert_length;
+
+    public DiffBlockHeader(int remove_start, int remove_length, int insert_start, int insert_length) {
+        if (remove_start < 0 || remove_length < 0 || insert_start < 0 || insert_length < 0) {
+            throw new InvalidParameterException("Parameters must be non-negative");
+        } else {
+            _remove_start = remove_start;
+            _remove_length = remove_length;
+            _insert_start = insert_start;
+            _insert_length = remove_length;
+        }
+    }
+
+    public int getRemoveStart() {
+        return _remove_start;
+    }
+
+    public int getRemoveLength() {
+        return _remove_length;
+    }
+
+    public int getInsertStart() {
+        return _insert_start;
+    }
+
+    public int getInsertLength() {
+        return _insert_length;
+    }
+
+    public String toString() {
+        String out = String.format("DiffBlockHeader: (%d, %d) -> (%d, %d)", _remove_start, _remove_length, _insert_start, _insert_length);
+        return out;
+    }
+}
+
+class DiffBlock {
+    private final int _remove_start; // Starts with 0, add 1 for actual line number
+    private final int _insert_start; // Starts with 0, add 1 for actual line number
+
+    private int _remove_length;
+    private int _insert_length;
+
+    public DiffBlock(int remove_start, int insert_start) {
+        this(remove_start, 0, insert_start, 0);
+    }
+
+    public DiffBlock(int remove_start, int remove_length, int insert_start, int insert_length) {
+        if (remove_start < 0 || remove_length < 0 || insert_start < 0 || insert_length < 0) {
+            throw new InvalidParameterException("Parameters must be non-negative");
+        } else {
+            _remove_start = remove_start;
+            _remove_length = remove_length;
+            _insert_start = insert_start;
+            _insert_length = remove_length;
+        }
+    }
+
+    public void pushRemove() {
+        _remove_length += 1;
+    }
+
+    public void pushRemove(int num) {
+        if (num > 0) {
+            _remove_length += num;
+        } else {
+            throw new InvalidParameterException("Parameter must be greater than zero");
+        }
+    }
+
+    public void pushInsert() {
+        _insert_length += 1;
+    }
+
+    public void pushInsert(int num) {
+        if (num > 0) {
+            _insert_length += num;
+        } else {
+            throw new InvalidParameterException("Parameter must be greater than zero");
+        }
+    }
+
+    public int getRemoveStart() {
+        return _remove_start;
+    }
+
+    public int getRemoveLength() {
+        return _remove_length;
+    }
+
+    public int getInsertStart() {
+        return _insert_start;
+    }
+
+    public int getInsertLength() {
+        return _insert_length;
+    }
+
+    public boolean isEmpty() {
+        return (_insert_length == 0 && _remove_length == 0);
+    }
+
+    public String toString() {
+        String out = String.format("DiffBlock: (%d, %d) -> (%d, %d)", _remove_start, _remove_length, _insert_start, _insert_length);
+        return out;
+    }
+
+}
